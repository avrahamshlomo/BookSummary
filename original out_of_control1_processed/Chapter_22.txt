PREDICTION MACHINERY 
Tell me about the future," I plead.
I'm sitting on a sofa in the guru's office. I've trekked to this high mountain 
outpost at one of the planet's power points, the national research labs at Los Alamos, New Mexico. The office of the guru is decorated in colorful posters of past hi-tech conferences that trace his almost mythical career: from a maverick physics student who formed an underground band of hippie hackers to break the bank at Las Vegas with a wearable computer, to a principal character in a renegade band of scientists who invented the accelerating science of chaos by studying a dripping faucet, to a founding father of the artificial life movement, to current head of a small lab investigating the new science of complexity in an office kitty-corner to the museum of atomic weapons at Los Alamos.
The guru, Doyne Farmer, looks like Ichabod Crane in a bolo tie. Tall, bony, 
looking thirty-something, Doyne (pronounced Doan) was embarking on his next remarkable adventure. He was starting a company to beat the odds on Wall Street by predicting stock prices with computer simulations.
"I've been thinking about the future, and I have one question," I begin.
"You want to know if IBM is gonna be up or down!" Farmer suggests with a 
wry smile.
"No. I want to know why the future is so hard to predict."
"Oh, that's simple."
I was asking about predicting because a prediction is a form of control. It is 
a type of control particularly suited to distributed systems. By anticipating the future, a vivisystem can shift its stance to preadapt to it, and in this way control its destiny. John Holland says, "Anticipation is what complex adaptive systems do."
Farmer likes to use a favorite example when explaining the anatomy of a 
prediction. "Here catch this!" he says tossing you a ball. You grab it. "You know how you caught that?" he asks. "By prediction." Farmer contends you have a model in your head of how baseballs fly. You 
could predict the trajectory of a high-fly using Newton's classic equation of f=ma, but your brain doesn't stock up on elementary physics equations. Rather, it builds a model directly from experiential data. A baseball player watches a thousand baseballs come off a bat, and a thousand times lifts his gloved hand, and a thousand times adjusts his guess with his mitt. Without knowing how, his brain gradually compiles a model of where the ball lands-a model almost as good as f=ma, but not as generalized. It's based entirely on a series of hand-eye data from past catches. In the field of logic such a process is known as induction, in contradistinction to the deduction process that leads to f=ma. 
In the early days of astronomy before the advent of Newton's f=ma, 
planetary events were predicted on Ptolemy's model of nested circular orbits-wheels within wheels. Because the central premise upon which Ptolemy's theory was founded (that all heavenly bodies orbited the Earth) was wrong, his model needed mending every time new astronomical observations delivered more exact data for a planet's motions. But wheels-within-wheels was a model amazingly robust to amendments. Each time better data arrived, another layer of wheels inside wheels inside wheels was added to adjust the model. For all its serious faults, this baroque simulation worked and "learned." Ptolemy's simple-minded scheme served well enough to regulate the calendar and make practical celestial predictions for 1400 years!
An outfielder's empirically based "theory" of missiles is reminiscent of the 
latter stages of Ptolemic epicyclic models. If we parsed an outfielder's "theory" we would find it to be incoherent, ad-hoc, convoluted, and approximate. But it would also be evolvable. It's a rat's-nest of a theory, but it works and improves. If humans had to wait until each of our minds figured out f=ma (and half of f=ma is worse than nothing), no one would ever catch anything. Even knowing the equation now doesn't help. "You can do the flying baseball problem with f=ma, but you can't do it in the outfield in real-time," says Farmer. 
"Now catch this!" Farmer says as he releases an inflated balloon. It ricochets 
around the room in a wild, drunken zoom. No one ever catches it. It's a classic illustration of chaos-a system with sensitive dependence on initial conditions. Imperceptible changes in the launch can amplify into enormous changes in flight direction. Although the f=ma law still holds sway over the balloon, other forces such as propulsion and airlift push and pull, generate an unpredictable trajectory. In its chaotic dance, the careening balloon mirrors the unpredictable waltz of sunspot cycles, Ice Age's temperatures, epidemics, the flow of water down a tube, and, more to the point, the flux of the stock market. 
But is the balloon really unpredictable? If you tried to solve the equations for 
the balloon's crazy flitter, its path would be nonlinear, therefore almost unsolvable, and therefore unforeseeable. Yet, a teenager reared on Nintendo 
could learn how to catch the balloon. Not infallibly, but better than chance. After a couple dozen tries, the teenage brain begins to mold a theory-an intuition, an induction-based on the data. After a thousand balloon takeoffs, his brain has modeled some aspect of the rubber's flight. It cannot predict precisely where the balloon will land, but it detects a direction the missile favors, say, to the rear of the launch or following a certain pattern of loops. Perhaps over time, the balloon-catcher hits 10 percent more than chance would dictate. For balloon catching, what more do you need? In some games, one doesn't require much information to make a prediction that is useful. While running from lions, or investing in stocks, the tiniest edge over raw luck is significant.
Almost by definition, vivisystems-lions, stock markets, evolutionary 
populations, intelligences-are unpredictable. Their messy, recursive field of causality, of every part being both cause and effect, makes it difficult for any part of the system to make routine linear extrapolations into the future. But the whole system can serve as a distributed apparatus to make approximate guesses about the future. 
Farmer was into extracting the dynamics of financial markets so that he 
could crack the stock market. "The nice thing about markets is that you don't really have to predict very much to do an awful lot," says Farmer. 
Plotted on the gray, end-pages of a newspaper, the graphed journey of the 
stock market as it rises and falls has just two dimensions: time and price. For as long as there has been a stock market, investors have scrutinized that wavering two-dimensional black line in the hopes of discerning some pattern that might predict its course. Even the vaguest, if reliable, hint in direction would lead to a pot of gold. Pricey financial newsletters promoting this or that method for forecasting the chart's future are a perennial fixture in the stock market world. Practitioners are known as chartists. 
In the 1970s and 1980s chartists had modest success in predicting currency 
markets because, one theory says, the strong role of central banks and treasuries in currency markets constrained the variables so that they could be described in relatively simple linear equations. (In a linear equation, a solution can be expressed in a graph as a straight line.) As more and more chartists exploited the easy linear equations and successfully spotted trends, the market became less profitable. Naturally, forecasters began to look at the wild and woolly places where only chaotic nonlinear equations ruled. In nonlinear systems, the outcome is not proportional to the input. Most complexity in the world-including all markets-are nonlinear.
With the advent of cheap, industrial-strength computers, forecasters have 
been able to understand certain aspects of nonlinearity. Money, big money, is made by extracting reliable patterns out of the nonlinearity behind the two-dimensional plot of financial prices. Forecasters can extrapolate the graph's future and then bet on the prediction. On Wall Street the computer nerds who decipher these and other esoteric methods are called "rocket 
scientists." These geeks in suits, working in the basements of trading companies, are the hackers of the '90s. Doyne Farmer, former mathematical physicist, and colleagues from his earlier mathematical adventures, set up in a small, four-room house which serves as an office in adobe-baked Santa Fe-as far from Wall Street as one can get in America-are currently some of Wall Street's hottest rocket scientists.
In reality, the two-dimensional chart of stocks does not hinge on several 
factors but on thousands of them. The stock's thousands of vectors are whited-out when plotted as a line, leaving only its price visible. The same goes for charts of sunspot activity and seasonal temperature. You can plot, say, solar activity as a simple thin line over time, but the factors responsible for that level are mind-bogglingly complicated, multiple, intertwined, and recursive. Behind the facade of a two-dimensional line seethes a chaotic mixture of forces driving the line. A true graph of a stock, sunspot, or climate would include an axis for every influence, and would become an unpicturable thousand-armed monster. 
Mathematicians struggle with ways to tame these monsters, which they call 
"high dimensional" systems. Any living creature, complex robot, ecosystem, or autonomous world is a high-dimensional system. The Library of form is the architecture of a high-dimensional system. A mere 100 variables create a humongous swarm of possibilities. Because each behavior impinges upon the 99 others, it is impossible to examine one parameter without examining the whole interacting swarm at once. Even a simple three-variable model of weather, say, touches back upon itself in strange loops, breeding chaos, and making any kind of linear prediction unlikely. (The failure to predict weather led to the discovery of chaos theory in the first place.)

Pop wisdom says that chaos theory proves that these high-dimensional 
complex systems-such as the weather, the economy, army ants, and, of 
course, stock prices-are intrinsically no-way-around-it-unpredictable. So ironclad is the assumption, that in common perception any design for predicting the outcome of a complex system is considered naive or mad. 
But chaos theory is vastly misunderstood. It has another face. Doyne 
Farmer, a boomer born in 1952, illustrates this with a metaphor from the age when music came on vinyl: 
Chaos is like a hit record with two sides, he suggests. 
●     The lyrics to the hit side go: By the laws of chaos, initial order can 
unravel into raw unpredictability. You can't predict far.
●     But the flip side goes: By the laws of chaos, things that look completely disordered may be predictable over the short term. You can predict short.
In other words, the character of chaos carries both good news and bad news. The bad news is that very little, if anything, is predictable far into the future. The good news-the flip side of chaos-is that in the short term, more may be more predictable than it first seems. Both the long-term, unpredictable nature of the high dimensional systems, and the short-term, predictable nature of low-dimensional systems, derive from the fact that "chaos" is not the same thing as "randomness." "There is order in chaos," Farmer says.
Farmer should know. He was an original pioneer into the dark frontier of 
chaos before it gelled into a scientific theory and faddish field of study. In the hip California town of Santa Cruz of the 1970s, Doyne Farmer and friend Norm Packard cofounded a commune of nerd hippies who practiced collective science. They shared a house, meals, cooking, and credit on scientific papers. As the "Chaos Cabal," the band investigated the weird physics of dripping faucets and other seemingly random generating devices. Farmer in particular was obsessed with the roulette wheel. He was convinced that there must be hidden order in the apparently random spinning of the 
wheel. If one could discern secret order among the spinning chaos, then...why, one could get rich...very rich.
In 1977, long before the birth of commercial microcomputers such as the Apple, the Santa Cruz Chaos Cabal built a set of handcrafted programmable tiny microcomputers into the bottoms of three ordinary leather shoes. The computers were keyboarded with toes; their function was to predict the toss of a roulette ball. The home-brew computers ran code devised by Farmer based on the group's study of a purchased second-hand 
Las Vegas roulette wheel set up in one of the commune's crowded bedrooms. Farmer's computer algorithm was based not on the mathematics of roulette but on the physics of the wheel. In essence, the Cabal's code simulated the entire rotating roulette wheel and bouncing ball inside the chip in the shoe. And it did this in a miniscule 4K of memory, in an era when computers were behemoths demanding 24-hour air-conditioning and an attendant priesthood.
On more than one occasion the science commune played out the flip side of 
chaos in the scene like this: Wired-up at the casino, one person (usually Farmer) wore a pair of magic shoes to calibrate the roulette operator's flick of the wheel, the speed of the bouncing ball, and the tilt of the wheel's wobble. Nearby, a Cabal cohort wore the third magic shoe linked by radio signals, and placed the actual bet on the table. Earlier, using his toes, Farmer had tuned his algorithm to the idiosyncrasies of a particular wheel in the casino. Now, in the mere 15 seconds or so between the drop of the ball and its decisive stop, his shoe-computer simulated the full chaotic run of the ball. About a million times faster than it took the real ball to land in a numbered cup, Farmer's prediction machinery buzzed out the ball's future destination on his right big toe. Typing with his left big toe, Farmer transmitted that information to his partner, who "heard" it on the bottom of his feet, and then, with a poker face, pushed the chips onto the predetermined squares before the ball stopped. 
When everything worked, the chips won. The system never predicted the 
exact winning number; the Cabal were realists. Their prediction machinery forecasted a small neighborhood of numbers-one octave section of the wheel-as the bettable destination of the ball. The gambling partner spread the bets over this neighborhood as the ball finished spinning. Out of the bunch, one won. While the companion bets lost, the neighborhood as a whole would win often enough to beat the odds. And make money. 
The group sold the system to other gamblers because of unreliability in the 
hardware. But Farmer learned three important things about predicting the future from this adventure:
●     First, you can milk underlying patterns inherent in chaotic systems to make good predictions.●     Second, you don't need to look very far ahead to make a useful 
prediction.
●     And third, even a little bit of information about the future can be valuable.

With these lessons firmly in mind, Farmer together with five other 
physicists (one of them a former Chaos Cabal member) engineered a start-
up company to crack every gambler's dream: Wall Street. They would use high-powered computers. They would stuff them with experimental nonlinear dynamics and other esoteric rocket-scientist tricks. They would think laterally and let the technology do as much as possible without their control. They would create a thing, an organism if you will, that would on its own gamble millions of dollars. They would make it...(drum roll, please)... predict the future. With a bit of bravado, the old gang hung out their new shingle: the Prediction Company.
The guys in the Prediction Company figure that looking ahead a few days into the financial market future is all that is needed to make big bucks. Indeed, recent research done at the Santa Fe Institute, where Farmer and colleagues hang out, makes it clear that "seeing further is not seeing better." When immersed in real world complexity, where few choices are clear cut and every decision is clouded by incomplete information, evaluating choices too far ahead becomes counterproductive. Although this conclusion seems intuitive for humans, it has not been clear why it should pertain to computers and model worlds. The human brain is easily distracted. But let's say you have unlimited computing power specifically dedicated to the task of seeing ahead. Why wouldn't deeper, farther be better?
The short answer is that tiny errors (caused by limited information) compound into grievous errors when extended very far into the future. And the cost of dealing with exponentially increasing numbers of error-tainted possibilities just isn't worth the immense trouble, even if computation is free (which it never is). Santa Fe Institute investigators, Yale economist John Geanakoplos and Minnesota professor Larry Gray, used chess-
playing computer programs as the test-bed for their forecasting work. (The best computer chess programs, such as the top-ranked Deep Thought, can beat all human players except for the very best grandmasters.)
Contrary to the expectations of computer scientists, neither Deep Thought 
nor human grandmasters need to look very far ahead to play excellent games. This limited look-ahead is called "positive myopia." Generally 
grandmasters survey the chess board and forecast the pieces only one move ahead. Then they select the most plausible play or two and investigate its consequences deeper. At every move ahead the number of choices to consider explodes exponentially, yet great human players will concentrate only on a few of the most probable countermoves at each rehearsed turn. Occasionally they search far ahead when they spot familiar situations they know from experience to be valuable or dangerous. But in general, grandmasters (and now Deep Thought) work from rules of thumb. For instance: Favor moves that increase options; shy from moves that end well but require cutting off choices; work from strong positions that have many adjoining strong positions. Balance looking ahead to really paying attention to what's happening now on the whole board.
Every day we confront similar tradeoffs. We must anticipate what lies around 
the corner in business, politics, technology, or life. However, we never have sufficient information to make a fully informed decision. We operate in the dark. To compensate we use rules of thumb or rough guidelines. Chess rules of thumb are actually pretty good rules to live by. (Notes to my daughters: Favor moves that increase options; shy away from moves that end well but require cutting off choices; work from strong positions that have many adjoining strong positions. Balance looking ahead to really paying attention to what's happening now on the whole board.)
Common sense embodies a "positive myopia." Rather then spend years 
developing a company employee manual that anticipates every situation that might arise-yet be out of date the moment it is printed-how much better to adopt positive myopia and not look so far ahead. Devise some general guidelines for the events that seem sure to arise "on the next move" and treat extreme cases if and when they come up. To navigate through rush-hour traffic in an unfamiliar city we can either plan detailed routes through the town on a map-thinking far ahead-or adopt a heuristic such as "Go west until we hit the river road, then turn left." Usually, we do a bit of both. We refrain from looking too far ahead, but we do look immediately in front. We meander west, or uphill, or downtown, while using the map to evaluate the next immediate turn ahead, wherever we are. We employ limited look-ahead guided by rules of thumb.
Prediction machinery need not see like a prophet to be of use. It needs only 
to detect limited patterns-almost any pattern-out of a background camouflage of randomness and complexity.

According to Farmer, there are two kinds of complexity: inherent and 
apparent. Inherent complexity is the "true" complexity of chaotic systems. It 
leads to dark unpredictability. The other kind of complexity is the flip side of chaos-apparent complexity obscuring exploitable order. 
Farmer draws a square in the air. Going up the square increases apparent 
complexity; going across the square increases inherent complexity. "Physics normally works down here," Farmer says, pointing to the bottom corner of low complexity for both sorts, home of the easy problems. "Out there," pointing to the opposite upper corner, "it's all hard. But we are now sliding up to here, where it gets interesting-where the apparent complexity is high, but the true complexity is still low. Up here complex problems have something in them you can predict. And those are exactly the ones we are looking for in the stock market."
With crude computer tools that take advantage of the flip side of chaos, the 
Prediction Company hopes to knock off the easy problems in financial markets. 
"We are using every method we can find," says partner Norman Packard, a 
former Chaos Cabalist. The idea is to throw proven pattern-finding strategies of any stripe at the data and "keep pounding on them" to optimize the algorithms. Find the merest hint of a pattern, and then exploit the daylights out of it. The mindset here is that of a gambler's: any advantage is an advantage.
Farmer and Packard's motivating faith that chaos possesses a flip side firm 
enough to bank on is based on their own experience. Nothing overcomes doubts like the tangible money they won from their Las Vegas roulette wheel experiments. It seems dumb not to take advantage of these patterns. As the chronicler of their high-rolling adventure exclaims in the book The Eudaemonic Pie, "Why would anyone play roulette without wearing a computer in his shoe?" 
In addition to experience, Farmer and Packard place a lot of faith in the well-
respected theories they invented during their years in chaos research. Now they are testing their wildest, most controversial theory yet. They believe, against the unbelief of most economists, that certain regions of otherwise complicated phenomenon can be predicted accurately. Packard calls these 
areas "pockets of predictability" or "local predictability." In other words, the distribution of unpredictability is not uniform throughout systems. Most of the time, most of a complex system may not be forecastable, but some small part of it may be for short times. In hindsight, Packard believes local predictability is what allowed the Santa Cruz Cabal to make money forecasting the approximate path of a roulette ball. 
If there are pockets of predictability, they will surely be buried under a 
haystack of gross unpredictability. The signal of local predictability can be masked by a swirling mess of noise from a thousand other variables. The Prediction Company's six rocket scientists use a mixture of old and new, hi-tech and low-tech search techniques to scan this combinatorial haystack. Their software examines the mathematically high-dimensional space of financial data and searches for local regions-any local region-that might match low-dimensional patterns they can predict. They search the financial cosmos for hints of order, any order. 
They do this in real time, or what might be called hyperreal time. Just as the 
simulated bouncing roulette ball in the shoe-computer comes to rest before the real ball does, the Prediction Company's simulated financial patterns are played out faster than they happen on Wall Street. They reenact a simplified portion of the stock market in a computer. When they detect the beginnings of a wave of unfolding local order, they simulate it faster than real life and then bet on where they think the wave will approximately end.
David Berreby, writing in the March 1993 Discover, puts the search for 
pockets of predictability in terms of a lovely metaphor: "Looking at market chaos is like looking at a raging white-water river filled with wildly tossing waves and unpredictably swirling eddies. But suddenly, in one part of the river, you spot a familiar swirl of current, and for the next five or ten seconds you know the direction the water will move in that section of the river."
Sure, you can't predict where the water will go a half-mile downstream, but 
for five seconds-or five hours on Wall Street-you can predict the unfolding show. That's all you really need to be useful (or rich). Find any pattern and exploit it. The Prediction Company's algorithms grab a fleeting bit of order and exploit this ephemeral archetype to make money. Farmer and Packard emphasize that while economists are obliged by their profession to unearth the cause of such patterns, gamblers are not bound so. The exact reason why a pattern forms is not important for the Prediction Company's purposes. In inductive models-the kind the Prediction Company constructs-the abstracted causes of events are not needed, just as they aren't needed for an outfielder's internalized ballistic notions, or for a dog to catch a tossed stick. 
Rather than worry about the dim relationships between causes and effects in 
these massively swarmy systems crowded with circular causality, Farmer says, "The key question to ask in beating the stock market is, what patterns 
should you pay attention to?" Which ones disguise order? Learning to recognize order, not causes, is the key.
Before a model is used to bet with, Farmer and Packard test it with 
backcasting. In backcasting techniques (commonly used by professional futurists) a model is built withholding the most recent data from the human managing the model. Once the system finds order in past data, say from the 1980s, it is fed the record of the last several years. If it can accurately predict the 1993 outcome, based on what it found in the 1980s, then the pattern seeker has won its wings. Farmer: "The system makes twenty models. We run them each through a sieve of diagnostic statistics. Then the six of us will get together to select the one to run live." Each round of model-building may take days on the Company's computers. But once local order is detected, a prediction based on it can be spun in milliseconds. 
For the final step-running it live with bundles of real money in its fists-one of 
the Ph.D.'s still has to hit the "enter" button. This act thrusts the algorithm into the big-league world of very fast, mind-boggling big bucks. Cut loose from theory, running on automatic, the fleshed out algorithm can only hear the murmurs of its creators: "Trade, sucker, trade!"
"If we can earn 5 percent better than what the market does, then our 
investors will make money," Packard says. Packard clarifies that number by explaining that they can predict 55 percent of market moves, that is, 5 percent more than by random guessing, but that when they do guess right their result can be 200 percent better. The fat-cat Wall Street financial backers who invest in the Prediction Company (currently O'Connor & Associates) get exclusive use of the algorithms in exchange for payments according to the performance of the predictions. "We have competitors," Packard states with a smile. "I know of four other companies with the same thing in mind"-capturing patterns in chaos with nonlinear dynamics and predicting from them. "Two of them are up and going. Some involve friends." 
One competitor trading real money is Citibank. Since 1990, British 
mathematician Andrew Colin has been evolving trading algorithms. His forecasting program randomly generates several hundred hypotheses of which parameters influence currency data, and then tests the hundred against the last five years of data. The most likely influences are sent to a computer neural net which juggles the weight of each influence to better fit the data, rewarding the best combinations in order to produce better guesses. The neural net system keeps feeding the results back in so that the system can hone its guess in a type of learning. When a model fits the past data, it is sent out into the future. In 1992 the Economist said, "After two years of experiments, Dr. Colin reckons his computer can make returns of 25 percent a year on its notional dealing capital....That is several times more than most human traders hope to make." Midland Bank in London has eight rocket scientists working on prediction machinery. In their scheme, computers breed algorithms. However, just as at the Prediction Company, 
humans evaluate them before "hitting the return button." They were trading real money by late 1993.
A question investors like to ask Farmer is how can he prove you can make 
money in markets with the advantage of only a small bit of information. As an "existence proof" Farmer points to the people such as George Soros earning millions year after year trading currencies and whatnot on Wall Street. Successful traders, sniffs Farmer "are pooh-poohed by the academics as being extremely lucky-but the evidence goes the other way." Human traders unconsciously learn how to spot patterns of local predictability streaking through the ocean of random data. The traders make millions of dollars because they detect patterns (which they cannot articulate), then make an internal model (which they are unconscious of), in order to make predictions (which they are rewarded or punished for, sharpening the feedback loop). They have no more idea of what their model or theory is than of how they catch fly balls. They just do. Yet both kinds of models were empirically constructed in the same inductive Ptolemaic way. And that's how the Prediction Company employs computers to build models of high-flying stocks-from the data up. 
Says Farmer, "If we are successful on a broad basis in what we are doing, it 
will demonstrate that machines are better forecasters than people, and that algorithms are better economists than Milton Friedman. Already, traders are hesitant about this stuff. They feel threatened by it."
The hard part is keeping it simple. Says Farmer, "The more complex the 
problem is, the simpler the models that you end up having to use. It's easy to fit the data perfectly, but if you do that you invariably end up just fitting to the flukes. The key is to generalize."
Prediction machinery is ultimately theory-making machinery-devices for 
generating abstractions and generalizations. Prediction machinery chews on the mess of seemingly random chicken-scratched data produced by complex and living things. If there is a sufficiently large stream of data over time, the device can discern a small bit of pattern. Slowly the technology shapes an internal ad-hoc model of how the data might be produced. The apparatus shuns "overfitting" the pattern on specific data and leans to the fuzzy fit of a somewhat imprecise generalization. Once it has a general fit-a theory-it can make a prediction. In fact prediction is the whole point of theories. "Prediction is the most useful, the most tangible and, in many respects, the most important consequence of having a scientific theory," Farmer declares. Manufacturing a theory is a creative act that human minds excel in, although, ironically we have no theory of how we do it. Farmer calls this mysterious general-pattern-finding ability "intuition." It's the exact technology "lucky" Wall Street traders use.
Prediction machinery is found in biology, too. As David Liddle, the director of 
a hi-tech think tank called Interval, says, "Dogs don't do math," yet dogs can be trained to predictively calculate the path of a Frisbee and catch it 
precisely. Intelligence and smartness in general is fundamentally prediction machinery. In the same way, all adaptation and evolution are milder and more thinly spread apparatus for anticipation and prediction. 
Farmer confessed to a private gathering of business CEOs, "Predicting 
markets is not my long-term goal. Frankly, I'm the kind of guy who has a hard time opening to the financial page of the Wall Street Journal." For an unrepentant ex-hippie, that's no surprise. Farmer sees himself working for five years on the problem of predicting the stock market, scoring big time, and then moving on to more interesting problems-such as real artificial life, artificial evolution, and artificial intelligence. Financial forecasting, like roulette, is just another hard problem. "We are interested in this because our dream is to produce prediction machinery that will allow us to predict lots of different things"-weather, global climate, epidemics-"anything generating a lot of data we don't understand well." 
"Ultimately," says Farmer, "we hope to imbue computers with a crude form 
of intuition."
By late 1993, Farmer and Company publicly reported success in predicting 
markets with "computerized intuition" while trading real money. Their agreement with their investors prohibits them from talking about specific performance, as much as Farmer is dying to. He did say, though, that in a few years they should have enough data to prove "by scientific standards" that their trading success is not a statistical fluke: "We really have found statistically significant patterns in financial data. There really are pockets of predictability out there."

While researching prediction and simulation machinery, I had a chance 
to visit the Jet Propulsion Lab in Pasadena, California, where a state-of-the-
art battle simulation was under development. I came to JPL at the invitation of a computer science professor from UCLA who had been pushing the edge of computer power. Like many researchers pinched for support, this professor had to rely on military funding for his avant-garde theoretical experiments. He paid for his end of the bargain by picking a practical military problem to test his theories on. 
His test-bed was to see how decentralized, massively parallel computing-
what I'm calling "swarm computing"-could speed up a computer simulation of a tank battle, an application which only remotely interested him. On the other hand, I was earnestly interested to see a state-of-the art war game.
At the busy front desk of JPL, security clearance was straightforward. 
Considering that I visited the national research center while American troops were on red-alert along the Iraq border, the bouncers were fairly cordial. I signed some forms swearing my allegiance and citizenship, got a substantial badge to clip on, and was escorted with the professor to his cubbyhole office on an upper floor. In a small gray conference room, I met a long-haired graduate student who used the battle simulation mathematics as an excuse to pursue some far out notions on computational theories of the universe. Then I met the JPL honcho. He was nervously uncomfortable with my presence as a journalist.
Why? my professor friend asked him. The simulation system was not 
classified; the results were published in the open literature. The JPL honcho replied in so many words: "Well, umm, you see, there is this war going on, and quite inadvertently the generic scenario we have been dry-running for the last year or so-a game we chose quite by accident, with no thought of prediction-is being played out now for real. When we first tested this computer algorithm we had to pick some scenario, any scenario, to try out the simulation with. So we picked a simulated desert war with...Iraq and Kuwait. Now we are fighting this simulation. We are a bit on the spot here. It's a little sensitive. I'm sorry."
I did not get to see that war simulation. But about a year after the Gulf 
War's end, I discovered that JPL was not the only place that serendipitously preenacted that war. The U.S. Military Central Command in Florida ran a 
second and more useful simulation of a desert battle prior to the war. Cynics interpret the fact that the U.S. government had simulated the Kuwait war twice beforehand as a mark of its imperialist and conspiratorial desire to have that war. I find the predictive scenarios spooky, strange, and instructional rather than diabolical. I use this example to portray the potential power of prediction machinery. 
There are about two dozen centers around the world that are playing war 
games where the U.S. is Blue-the protagonist. Most of these places are small departments at military schools and training centers, such as the Wargaming Center at Maxwell Air Force Base in Alabama, the legendary Global Game room at the Naval War College in Newport, Rhode Island, or the classic "sand box" table set-ups at the Army's Combat Concepts Agency in Leavenworth, Kansas. Providing them technical support and know-how are academics and savants holed up in the numerous para-military think tanks peppering the beltway of Washington, D.C., or research alleys nested in the corridors of national laboratories like JPL and Lawrence Livermore Labs in California. The toy war simulators, of course, carry acronyms; TACWAR, JESS, RSAC, SAGA. A recent catalog of military software listed four hundred varieties of war games or other military models for sale right off the shelf.
The nerve center for any U.S. military operations is headquartered at Central 
Command, based in Florida. For its entire existence, Central Command, as an organ of the Pentagon, had been hawking one major scenario to Congress and the American people: Blue vs. Red-the superpower game where the only worthy opponent was the Soviet Union. When General Norman Schwarzkopf came on the scene in the 1980s, he didn't buy this story. Schwarzkopf-a thinking man's general-put out a new perspective, worded in a way that's been quoted up and down the ranks: "The Soviet dog is not going to hunt." Schwarzkopf refocused his planners' attention on alternative scenarios. High on the list was a Mid-East desert war along the border of Iraq.
In early 1989, Gary Ware, an officer at Central Command, began modeling a 
war based on Schwarzkopf's hunches. Ware worked with a small cell of military futurists in compiling data to create a simulated desert war. The simulation was code-named Operation Internal Look. 
Any simulation is only as good as the data it is based on, and Ware wanted 
Operation Internal Look based on reality as much as possible. That meant collecting a hundred thousand details about current forces in the Mid-East. Most of the work was horribly dull. The war simulation needed to know the number of vehicles in the Mid-East, stockpile strengths of food and fuel, killing power of weapons, climate conditions, and so on. Most of this minutiae was not readily available, even to the military. All bits were constantly in flux.Once Ware's team worked out a formulation of an army's organization, the 
war gamers compiled optical laser disc maps of the entire Gulf area. The foundation of the simulated desert war-the territory itself-was transferred from the latest satellite digitized photos. When they finished, the war gamers had the countries of Kuwait and Saudi Arabia compressed onto a CD. They were now ready to feed all this data into TACWAR, the main computerized war-gaming simulator. 
In early 1990 Ware began running a desert war on the virtual battlefield of 
Kuwait and Saudi Arabia. In July, in a conference room in north Florida, Gary Ware summarized the results of Operation Internal Look for his superiors. They reviewed a scenario based on Iraq invading Saudi Arabia, and the U.S./Saudi Arabia striking back. Ware's simulation forecast a fairly brief thirty-day war if anything this unlikely should occur.
Two weeks later, Saddam Hussein suddenly invaded Kuwait. At first, the 
upper echelons of the Pentagon had no idea they already owned a fully operational, data-saturated simulation of the war. Turn the key and it would run endless what-ifs of possible battles in that zone. When word of the prescient simulation surfaced, Ware came out smelling like roses. He admitted that "If we had to start from scratch at the time of the invasion we would have never caught up." In the future, standard army-issue preparedness may demand having a parallel universe of possible wars spinning in a box at the command center, ready to go. 
Immediately after Saddam's initial invasion, the war gamers shifted Internal 
Look to running endless variations of the "real" scenario. They focused on a group of possibilities revolving around the variant: "What if Saddam keeps on coming right away?" It took Ware's computers about 15 minutes to run each iteration of the forecasted thirty-day war. By running those simulations in many directions the team quickly learned that airpower would be the decisive key in this war. Further refined iterations clearly showed the war gamers that if airpower was successful, the U.S. war would be successful. 
Further, according to Ware's prediction machinery, if airpower could actually 
inflict the results assigned to it, U.S. ground forces would not sustain heavy losses. The top brass took this to mean that precise upfront airpower was the linchpin to low U.S. casualities. Gary Ware says, "Schwarzkopf was so adamant on maintaining the absolute minimum casualties of our forces that low casualities became the benchmark upon which all our analysis was done."
Predictive simulations, then, gave the command team the confidence that 
the U.S. could achieve success with minimum losses. This confidence led to the heavy air campaign. Says Ware, "The simulations definitely had an impact on our thinking [at Central Command]. Not that Schwarzkopf didn't have prior strong feelings, but the model gave us confidence that we could carry through the concepts."As a prediction, Operation Internal Look got good marks. Despite some 
shifts in the initial balance of forces, the 30-day simulated air and ground campaign was pretty close to the real sequence, although the percentage of air and ground action was slightly different. The ground battle pretty much unfolded as forecasted. Like everyone outside the field, the simulators were surprised by how fast Schwarzkopf's end run around the front lines went. Says Ware, "I have to tell you, though, that we did not expect to get so far [on the battlefield] as we did in a hundred hours. As I recall, we forecasted a six-day land battle instead of a hundred-hour [four day] battle. The ground commanders had told us that they envisioned moving faster than the simulation indicated they would. So they moved exactly as fast as they predicted."
The war game prediction machinery figured greater resistance from the 
Iraqis than the Iraqis actually gave. That's because every combat simulation assumes that the enemy will employ all of its available systems. But Iraq never pushed hard at all. The war gamers cheekily joked that no model reflects the white flag as a weapons system.
The war moved so fast the simulationists never got around to the obvious 
next step in simulations: daily modeled forecasts of the battle in progress. Although the planners recorded every day's events as best as they could, and they could project out into the future from any moment, they felt "it didn't take a genius to figure what was going on after about the first 12 hours."

If silicon chips are enough of a crystal ball to help steer a superarmy war, 
and algorithms coursing through small computers are enough predictive 
technology to outguess the stock market, then why not reconfigure a supercomputer to predict the rest of the world? If human society is just a large distributed system of agents and machines, why not construct an apparatus to forecast its future?
Even a cursory study of past predictions shows why not. On the whole, 
cultural predictions historically have been worse than random guesses. Old books are a graveyard of prophesied futures that never came to pass. A few prophecies hit the bullseye, but there is no way to discern beforehand the rare right one from the plentiful wrong ones. Since predictions are so often wrong, and since believing erroneous predictions is so tempting and so misleading, some professional futurists avoid predictions altogether on principle. To emphasize the corrupting unreliability of trying to prophesy, these futurists prefer to state their prejudice in deliberate exaggeration: "All predictions are wrong."
They have a point. So few long-term predictions prove correct that 
statistically they are all wrong. Yet, by the same statistical measure, so many short term predictions are right, that all short-term predictions are right. 
There is nothing more certain about a complex system than to say it will be 
just like it is now a moment later. This observation is nearly a truism. Systems are things that keep persisting; so it is only tautological that from one moment to the next a system-even a living thing-doesn't change much. An oak tree, the post office, and my Macintosh hardly change at all from one day to the next. I offer an easily guaranteed short-term prediction for complex things anywhere: tomorrow will be mostly like today.
Equally true is the cliché that things occasionally do change from one day to 
the next. But can these immediate alterations be predicted? And if they can, could you stack up a series of predictable short-term changes into a probable medium-range trend?
Yes. While long-range predictions will remain essentially unpredictable, short 
range predictions for complex systems are not only possible, they are essential. Furthermore, some types of mid-range predictions are quite 
feasible, and becoming more so. For reasons I will explain below, the human ability to forecast aspects of our society, economy, and technology will steadily increase despite the Alice-in-Wonderland strangeness that dependable predictions will have upon present actions.
We have the technology now to forecast many social phenomena, if we can 
catch them at the right moment. I follow the work of Theodore Modis, whose 1992 book, Predictions, nicely sums up the case for utility and believability of predictions. Modis addresses three types of found order in the greater web of human interactions. Each variety forms a pocket of predictability at certain times. He applies his research to the domain of economics, social infrastructure, and technology, but I believe his findings apply to organic systems as well. The three pockets of Modis: Invariants, Growth Curves, Cyclic Waves.
Invariants. The natural and unconscious tendency for all organisms to 
optimize their behavior instills in that behavior "invariants" that change very 
little over time. Humans in particular are certified optimizers. Twenty-four hours of time per day is an absolute invariant, so over decades people, on average, tend to spend a remarkably constant amount of time on such chores as cooking, traveling, cleaning-although the distance or what they accomplish during that time might change. If new activities (say airplane flight instead of walking) are reformulated into elemental dimensions for analysis (how much time is spent in daily moving), the new behaviors often exhibit a continuous pattern with the old that can be extrapolated (and predicted) into the future. Instead of walking a half hour to work, you now drive a half hour to work. In the future, you may fly a half hour to work. Marketplace pressures for efficiency are so relentless and unforgiving that they inevitably push human-made systems in a single (predictable) direction toward optimization. Tracing an invariant optimization point can often alert us to a clean pocket of predictability. For instance, improvement in mechanical efficiency is very slow. No system is yet over 50 percent efficient. A projected system operating on 45 percent efficiency is possible, but one that requires 55 percent is not. Therefore one can safely make a short-term prediction about fuel efficiency.
Growth Curves. The larger, more layered, more decentralized a system is, 
the more it takes on aspects of organic growth. Growing things share 
several universal characteristics. Among them are a lifespan that can be plotted as an S-shaped curve: slow birth, steep growth, slow decline. The worldwide production of cars per year or the lifetime production of symphonies composed by Mozart both fit an S-curve with great precision. "The predictive power of S-curves is neither magical nor worthless," writes Modis. "What is hidden under the graceful shape of the S-curve is that fact that natural growth obeys a strict law." This law says that the shape of the ending is symmetrical to the shape of the beginning. The law is based on empirical observations of thousands of biological and institutional life histories. The law is closely related to the natural distribution of complex things as expressed in a bell curve. Growth is extremely sensitive to initial 
conditions; the first data points on a growth curve are almost meaningless. But once a phenomenon is on a roll, a numerical snapshot of its history can be taken and flipped over to predict the phenomenon's eventual limits and demise. One can extract from the curve a cross-over point with a competing system, or a "ceiling" and a date when the ceiling essentially flattens out. Not every system exhibits a smooth S-curve lifespan; but a remarkable variety and number do. Modis believes that more things adhere to the laws of growth then we suspect. If such growing systems are examined at the right time (midway in their history), then the presence of local order-summed up by the S-curve law-affords yet another pocket of predictability.
Cyclic Waves. The apparent complex behavior of a system is partly a 
reflection of the complex structure of the system's environment. This was 
pointed out over 30 years ago by Herbert Simon, who used the journey of an ant over the ground as an illustration. The ant's jig-jagging path across the soil reflected not the ant's complex locomotion but the complex structure of its environment. According to Modis, cyclic phenomenon in nature can infuse a cyclic flavor to systems running within it. Modis is intrigued by the 56-year economic cycles discovered by economist N. D. Kondratieff. In addition to Kondratieff's economic waves, Modis adds similar 56-year cycles in scientific advances described by himself, and 56-year cycles in infrastructure replacement studied by Arnulf Grubler. The causes of these apparent waves have been hypothesized by various other authors as coming from 56-year lunar cycles, or every fifth 11-year sunspot cycle, or even from the every-other cycle of human generations-as each 28-year generational cohort swings away from the work of its parental cohort. Modis argues that primary environmental cycles trigger many secondary and tertiary internal cycles in their wake. Seekers who uncover any fragments of these cycles can use them to predict pockets of behavior.
Together, these three modes of prediction suggest that at certain moments 
of heightened visibility, the invisible pattern of order becomes clear to those paying attention. Like the next beat of a drum, its future can almost be heard. A moment later, the pattern is gone, muddied and overwritten by noise. Pockets of prediction won't keep away big surprises. But local predictability does point to methods that can be improved, deepened, and lengthened into bigger things.
The long odds against successful big predictions haven't discouraged hordes 
of amateur and full-time financial chartists attempting to extract longwave patterns from past stock market prices. Any external cyclic behavior is fair game for a chartist: the length of women's hemlines, the age of presidents, the price of eggs. Chartists are forever chasing the mythical "leading indicator" that will predict the destiny of stock prices as a number they can bet on. For many years chartists were ridiculed for their vaguely numerological approach. But in recent years academics such as Richard J. Sweeney and Blake LeBaron have shown that chartist methods often do work. A chartist's technical rule can be stunningly simple: "If the market has been going up for a while, bet that it will continue to go up. If it's on a 
downward trend, bet it will continue downward." Such a rule reduces the high dimensionality of a complex market into to the low dimensionality of this simple two-part rule. In general, this kind of pattern-seeking works. The "up-up, down-down" pattern performs better than random chance, and thus better than the average investor. Since stasis is the most predictable thing about a system this pattern of order should not come as a surprise, even though it does.
In opposition to chartism, other financial forecasters rely on the 
"fundamentals" of the market in an effort to predict it. Fundamentalists, as they are called, attempt to understand the driving forces, the underlying dynamics, and the fundamental conditions of a complex phenomenon. In short they seek a theory: f=ma.
Chartists, on the other hand, seek a pattern from the data without concern 
for whether they understand why the pattern is there. If there is order in the universe, then somewhere, somehow, all complexity will disclose-at least momentarily-order that reveals its future path. One merely needs to learn what signals to disregard as noise. Chartism is organized induction in Doyne Farmer's mode. Farmer admits that he and his fellows at the Prediction Company are "statistically rigorous chartists." 
In another fifty years, computerized induction, algorithmic chartism, and 
pocket predictionism will be respectable human endeavors. Forecasting stock markets will remain an oddball case because, more than other systems, stock markets are built out of expectations. In an expectation game, accurate predictions offer no opportunity for money-making if everyone shares the prediction. All the Prediction Company can really own is lead time. As soon as Farmer's group makes much money exploiting a pocket of predictability, others will rush in, somewhat clouding the pattern, but mostly leveling the opportunity to make any money. In a stock market, success stirs up strong self-canceling feedback currents. In other systems, such as a growing network, or an expanding corporation, anticipatory feedback is not self- canceling. Ordinarily, feedback is self-governing.

The original cyberneticist, Norbert Wiener, struggled to explain the 
immense power of feedback control. Wiener had in mind simple toilet- 
flusher type feedback. He noticed that delivering a constant weak trickle of information about what the system had just accomplished ("the water level is still down") into the system in some way directed the whole system. Wiener concluded that this power was a function of time-shifting. He wrote in 1954: "Feedback is a method of controlling a system by reinserting into it the results of its past performance." 
There's no puzzle in a sensor sensing the present. What more does one need 
to know about the present other than it is here and now? It obviously pays for a system to mind the present since it has little other choice. But why expend resources on what is gone and cannot be changed? Why raid the past for present control?
A system-organism, corporate firm, computer program-spends energy 
feeding the past back into the present because this is an economical way for the system to deal with the future. To see into the future one must see into the past. A constant pulse of the past along feedback loops informs and controls the future.
But there is another avenue for a system to time-shift into the future. Sense 
organs in a body that pick up sound and light waves miles away act as meters of the present and more as gauges of the future. Events geographically distant are, for practical purposes, events that hail from the future. An image of an approaching predator becomes information about the future now. A distant roar may soon be an animal up close; a whiff of salt signals a soon-to-be change in tide. Thus an animal's eye "feed-forwards" information from a distant time/space into its here/now body. 
Some philosophers say it is no coincidence that life arose on a planet bathed 
in two mediums-air and water-amazingly transparent in most spectrums. A cleanly transparent environment permits organs to receive data-rich signals from "distant" (future) events and process them in anticipation of a response from the organism. Eyes, ears, and noses are thus prediction machinery to peer into time. 
Completely opaque water or air, according to this notion, might have squelched the development of anticipation machinery by preventing 
information about distant events from reaching the present. Organisms in an opaque world would be cramped in both space and time; they would lack the room to develop adaptive responses. Adaptation-at its core-requires a sense of the future. In a changing environment, either opaque or clear, systems that anticipate the future are more likely to persist. Michael Conrad writes, "At bottom adaptability is the use of information to handle environmental uncertainty." Gregory Bateson put it telegraphically when he said, "Adaptation is change in the service of nonchange." A system (nonchange by definition) adapts (changes) in order to persist (nonchange). A flamingo adapts in order to persist.
Thus, systems stuck solely in the present will more often be surprised by 
change, and die. Therefore, a transparent environment rewards the evolution of predictive machinery, because prediction machinery confers survivability upon complexity. Complex systems survive because they anticipate, and a transparent medium helps them anticipate. Opaqueness, on the other hand, would hinder anticipation, adaptation, and evolution of complex vivisystems altogether.

Postmodern humans swim in a third transparent medium now 
materializing. Every fact that can be digitized, is. Every measurement of 
collective human activity that can be ported over a network, is. Every trace of an individual's life that can be transmuted into a number and sent over a wire, is. This wired planet becomes a torrent of bits circulating in a clear shell of glass fibers, databases, and input devices.
Once moving, data creates transparency. Once wired, a society can see 
itself. The reason the rocket scientists at the Prediction Company can fare better than the chartists of old is that they work in a more transparent medium. The billion computerized bits sloughed off by networked financial institutions clot into a transparent air through which the Company can detect unfolding patterns. The cloud of data flowing through their workstations forms a clear digital globe for them to peer into. In certain patches of the new air they can see ahead. 
At the same time, industrial factories mass-produce video cameras, tape 
recorders, hard disks, text scanners, spreadsheets, modems, and satellite dishes. Each of these is an eye, an ear, or a neuron. Connected together they form a billion-lobed sense organ floating in the clear medium of whizzing digits. This tissue serves to feed-forward information from distant limbs into the body electric. The U.S. Command Center wargamers can use the digitized land-terrain of Kuwait, just-in-time satellite images, and the relayed reports of hand-held transmitters anchored by global positioning information (accurate to within 50 feet anywhere on Earth) to anticipate-to see in the collective mind's eye-the course of an approaching battle.
Telling the future, when it comes right down to it, is not solely a human 
yearning. It is the fundamental nature of any organism, and perhaps any complex system. Telling the future is what organisms are for.
My working definition of a complex system is a "thing which talks to itself." 
One might ask, then: What is the story that complex systems tell themselves? The answer is that they tell themselves stories of the future. Stories of what might come next-whether next is reckoned in nanoseconds or years. continue... 
  
 
Out of Control
In the 1970s, after thousands of years of telling tales about the Earth's 
past and creation, the inhabitants of planet Earth began to tell their first 
story of what might happen to the planet in the future. Rapid communications of the day gave them their first comprehensive real-time view of their home. The portrait from space was enchanting-a cloudy blue marble hanging delicately in the black deep. But down on the ground the emerging tale wasn't so pretty. Reports from every quadrant of the globe said the Earth was unraveling.
Tiny cameras in space brought back photographs of the whole Earth that 
were awesome in the old-fashioned sense of the word: at once inspiring and frightening. The cameras, together with reams of ground data pouring in from every country, formed a distributed mirror reflecting a picture of the whole system. The entire biosphere was becoming more transparent. The global system began to look ahead-as systems do-wanting to know what might come next, say, in the next 20 years.
The first impression arising from the data-collecting membrane around the 
world was that the planet was wounded. No static world map could verify (or refute) this picture. No globe could chart the ups and downs of pollution and population over time, or decipher the interconnecting influence of one factor upon another. No movie from space could play out the question, what if this continues? What was needed was a planetary prediction machine, a global what-if spreadsheet.
In the computer labs of MIT, an unpretentious engineer cobbled together the 
first global spreadsheet. Jay Forrester had been dabbling in feedback loops since 1939, perfecting machinery-steering servomechanisms. Together with Norbert Wiener, his colleague at MIT, Forrester followed the logical path of servomechanisms right into the birth of computers. As he helped invent digital computers, Forrester applied the first computing machines to an area outside of typical engineering concerns. He created computer models to assist the management of industrial firms and manufacturing processes. The usefulness of these company models inspired Forrester to tackle a simulation of a city, which he modeled with the help of a former mayor of Boston. He intuitively, and quite correctly, felt that cascading feedback loops-impossible to track with paper and pencil, but child's play for a computer-were the only way to approach the web of influences between wealth, population, and resources. Why couldn't the whole world be modeled?
Sitting on an airplane on the way home from a conference on "The 
Predicament of Mankind" held in Switzerland in 1970, Forrester began to sketch out the first equations that would form a model he called "World Dynamics." 
It was rough. A thumbnail sketch. Forrester's crude model mirrored the 
obvious loops and forces he intuitively felt governed large economies. For data, he grabbed whatever was handy as a quick estimate. The Club of Rome, the group that had sponsored the conference, came to MIT to evaluate the prototype Forrester had tinkered up. They were encouraged by what they saw. They secured funding from the Volkswagen Foundation to hire Forrester's associate, Dennis Meadows, to develop the model to the next stage. For the rest of 1970, Forrester and Meadows improved the World Dynamics model, designing more sophisticated process loops and scouring the world for current data.
Dennis Meadows, together with his wife Dana and two other coauthors, 
published the souped-up model, now filled with real data, as the "Limits to Growth." The simulation was wildly successful as the first global spreadsheet. For the first time, the planetary system of life, earthly resources, and human culture were abstracted, embodied into a simulation, and set free to roam into the future. The Limits to Growth also succeeded as a global air raid siren, alerting the world to the conclusions of the authors: that almost every extension of humankind's current path led to civilization's collapse. 
The result of the Limits to Growth model ignited thousands of editorials, 
policy debates, and newspaper articles around the world for many years following its release. "A Computer Looks Ahead and Shudders" screamed one headline. The gist of the model's discovery was this: "If the present growth trends in world population, industrialization, pollution, food production, and resource depletion continue unchanged, the limits to growth on this planet will be reached sometime within the next 100 years." The modelers ran the simulation hundreds of times in hundreds of slightly different scenarios. But no matter how they made tradeoffs, almost all the simulations predicted population and living standards either withering away or bubbling up quickly to burst shortly thereafter. 
Primarily because the policy implications were stark, clear, and unwelcome, 
the model was highly controversial and heavily scrutinized. But it forever raised the discussion of resources and human activity to the necessary planetary scale.
The Limits to Growth model was less successful in spawning better predictive 
models, which the authors had hoped to spark with their pioneer efforts. Instead, in the intervening 20 years, world models came to be mistrusted, in large part because of the controversy of Limits to Growth. Ironically, the 
only world model visible in the public eye now (two decades later) is the Limits to Growth. The authors have reissued it on its 20th anniversary, with only slight changes. 
As currently implemented, the Limits to Growth model runs on a software 
program called Stella. Stella takes the dynamic systems approach worked out by Jay Forrester on mainframe computers and ports it over to the visual interface of a Macintosh. The Limits to Growth model is woven out of an impressive web of "stocks" and "flows." Stocks (money, oil, food, capital, etc.) flow into certain nodes (representing general processes such as farming), where they trigger outflows of other stocks. For instance money, land, fertilizer, and labor flow into farms to trigger an outflow of raw food. Food, oil, and other stocks flow into factories to produce fertilizer, to complete one feedback loop. A spaghetti maze of loops, subloops, and cross-loops constitute the entire world. The leverage each loop has upon the others is adjustable and determined by ratios found in real-world data: how much food is produced per hectare per kilo of fertilizer and water, generating how much pollution and waste. As is true in all complex systems, the impact of a single adjustment cannot be calculated beforehand; it must be played out in the whole system to be measured. 
Vivisystems must anticipate to survive. Yet the complexity of the prediction 
apparatus must not overwhelm the vivisystem itself. As an example of the difficulties inherent in prediction machinery, we can examine the Limits to Growth model in detail. There are four reasons to choose this particular model. The first is that its reissue demands that it be (re)considered as a reliable anticipatory apparatus for human endeavor. Second, the model provides a handy 20-year period over which to evaluate it. Did the patterns it detected 20 years ago still prevail? Third, one of the virtues of the Limits to Growth model is that it is critiqueable. It generates quantifiable results rather than vague descriptions. It can be tested. Fourth, nothing could be more ambitious than to model the future of human life on Earth. The success or failure of this prominent attempt can teach much about using models to predict extremely complex adaptive systems. Indeed one has to ask: Can such a seemingly unpredictable process as the world be simulated or anticipated with any confidence at all? Can feedback-driven models be reliable predictors of complex phenomenon?
The Limits to Growth model has many things going for it. Among them: It is 
not overly complex; it is pumped by feedback loops; it runs scenarios. But among the weaknesses I see in the model are the following:
Narrow overall scenarios. Rather than explore possible futures of any real 
diversity, Limits to Growth plays out a multitude of minor variations upon one fairly narrow set of assumptions. Mostly the "possible futures" it explores are those that seem plausible to the authors. Twenty years ago they ignored scenarios not based on what they felt were reasonable assumptions of expiring finite resources. But resources (such as rare metals, oil, and fertilizer) didn't diminish. Any genuinely predictive model must be 
equipped with the capability to generate "unthinkable" scenarios. It is important that a system have sufficient elbowroom in the space of possibilities to wander in places we don't expect. There is an art to this, because a model with too many degrees of freedom becomes unmanageable, while one too constrained becomes unreliable.
Wrong assumptions. Even the best model can be sidetracked by false 
premises. The original key assumption of the model was that the world contains only a 250-year supply of nonrenewable resources, and that the demands on that supply are exponential. Twenty years later we know both those assumptions are wrong. Reserves of oil and minerals have grown; their prices have not increased; and demand for materials like copper are not exponential. In the 1992 reissue of the model, these assumptions were adjusted. Now the foundational assumption is that pollution must rise with growth. I can imagine that premise needing to be adjusted in the next 20 years, if the last 20 are a guide. "Adjustments" of this basic nature have to be made because the Limits to Growth model has...
No room for learning. A group of early critics of the model once joked that 
they ran the Limits to Growth simulation from the year 1800 and by 1900 found a "20-foot level of horse manure on the streets." At the rate horse transportation was increasing then, this would have been a logical extrapolation. The half-jesting critics felt that the model made no provisions for learning technologies, increasing efficiencies, or the ability of people to alter their behavior or invent solutions.
There is a type of adaptation wired into the model. As crises arise (such as 
increase in pollution), capital assets are shifted to cover it (so the coefficient of pollution generated is lowered). But this learning is neither decentralized nor open-ended. In truth, there's no easy way to model either. Much of the research reported elsewhere in this book is about the pioneering attempts to achieve distributed learning and open-ended growth in manufactured settings, or to enhance the same in natural settings. Without decentralized open-ended learning, the real world will overtake the model in a matter of days. 
In real life, the populations of India, Africa, China, and South America don't 
change their actions based upon the hypothetical projections of the Limits to Growth model. They adapt because of their own immediate learning cycle. For instance, the Limits to Growth model was caught off-guard (like most other forecasts) by global birth rates that dropped faster than anyone predicted. Was this due to the influence of doomsday projections like Limits to Growth? The more plausible mechanism is that educated women have less children and are more prosperous, and that prosperous people are imitated. They don't know about, or care about, global limits to growth. Government incentives assist local dynamics already present. People anywhere act (and learn) out of immediate self-interest. This holds true for other functions such as crop productivity, arable land, transportation, and so on. The assumptions for these fluctuating values are fixed in Limits to 
Growth model, but in reality the assumptions themselves have coevolutionary mechanisms that flux over time. The point is that the learning must be modeled as an internal loop residing within the model. In addition to the values, the very structure of the assumptions in the simulation-or in any simulation that hopes to anticipate a vivisystem-must be adaptable.
World averages. The Limits to Growth model treats the world as uniformly 
polluted, uniformly populated, and uniformly endowed with resources. This homogenization simplifies and uncomplicates the world enough to model it sanely. But in the end it undermines the purpose of the model because the locality and regionalism of the planet are some of its most striking and important features. Furthermore, the hierarchy of dynamics that arise out of differing local dynamics provides some of the key phenomena of Earth. The Limits to Growth modelers recognize the power of subloops-which is, in fact, the chief virtue of Forrester's system dynamics underpinning the software. But the model entirely ignores the paramount subloop of a world: geography. A planetary model without geography is...not the world. Not only must learning be distributed throughout a simulation; all functions must be. It is the failure to mirror the distributed nature-the swarm nature-of life on Earth that is this model's greatest failure.
The inability to model open-ended growth of any kind. When I asked Dana 
Meadows what happened when they ran the model from 1600, or even 1800, she replied that they never tried it. I found that astonishing since backcasting is a standard reality test for forecasting models. In this case, the modelers suspected that the simulation would not cohere. That should be a warning. Since 1600 the world has experienced long-term growth. If a world model is reliable, it should be able to simulate four centuries of growth-at least as history. Ultimately, if we are to believe Limits to Growth has anything to say about future growth, the simulation must, in principle, be capable of generating long-term growth through several periods of transitions. As it is, all that Limits to Growth can prove is that it can simulate one century of collapse. 
"Our model is astonishingly 'robust,' " Meadows told me. "You have to do all 
kinds of things to keep it from collapsing....Always the same behavior and basic dynamic emerges: overshoot and collapse." This is a pretty dangerous model to rely on for predictions of society's future. All the initial parameters of the system quickly converge upon termination, when history tells us human society is a system that displays marvelous continuing expansion. 
Two years ago I spent an evening talking to programmer Ken Karakotsios 
who was building a tiny world of ecology and evolution. His world (which eventually became the game of SimLife) provides tools to god-players who can then create up to 32 virtual species of animals and 32 species of plants. The artificial animals and plants interact, compete, prey upon each other and evolve. "What's the longest you've had your world running?" I asked him. "Oh," he moans, "only a day. You know it's really hard to keep one of these 
complex worlds going. They do like to collapse." 
The scenarios in Limits to Growth collapse because that's what the Limits to 
Growth simulation is good at. Nearly every initial condition in the model leads to either apocalypse or (very rarely) to stability-but never to a new structure-because the model is inherently incapable of generating open-ended growth. The Limits to Growth cannot mimic the emergence of the industrial evolution from the agrarian age. "Nor," admits Meadows, "can it take the world from the Industrial Revolution to whatever follows next beyond that." She explains, "What the model shows is that the logic of the industrial revolution runs into an inevitable wall of limits. The model does two things, either it begins to collapse, or we intervene as modelers and make changes to save it."
ME: "Wouldn't a better world model possess the dynamics to transform itself 
to the next level on its own?"
DANA MEADOWS: "It strikes me as a little bit fatalistic to think that this is 
designed in the system to happen and we just lean back and watch it. Instead we modeled ourselves into it. Human intelligence comes in, perceives the whole situation, and makes changes in the human societal structure. So this reflects our mental picture of how the system transcends to the next stage-with intelligence that reaches in and restructures the system."
That's Save-The-World mode, as well as inadequate modeling of how an 
ever complexifying world works. Meadows is right that intelligence reaches in to human culture and restructures it. But that isn't done just by modelers, and it doesn't happen only at cultural thresholds. This restructuring happens in six billion minds around the world, every day, in every era. Human culture is a decentralized evolutionary system if there ever was one. Any predictive model that fails to incorporate this distributed ongoing daily billion-headed microrevolution is doomed to collapse, as civilization itself would without it.
Twenty years later, the Limits to Growth simulation needs not a mere 
update, but a total redo. The best use for it is to stand as a challenge and a departure point to make a better model. A real predictive model of a planetary society would:
1) spin significantly varied scenarios,2) start with more flexible and informed assumptions, 3) incorporate distributed learning, 4) contain local and regional variation, and 5) if possible, demonstrate increasing complexification. 
I do not focus on the Limits to Growth world model because I want to pick on its potent political implications (the first version did, after all, inspire a generation of antigrowth activists). Rather, the model's inadequacies 
precisely parallel several core points I hope to make in this book. In bravely attempting to simulate an extremely complex adapting system (the human infrastructure of living on Earth), in order to feed-forward a scenario of this system into the future, the Forrester/Meadows model highlights not the limits to growth but the limits of certain simulations.
The dream of Meadows is the same as that of Forrester, the U.S. Command 
Central wargamers, Farmer and the Prediction Company, and myself, for that matter: to create a system (a machine) that sufficiently mirrors the real evolving world so that this miniature can run faster than real life and thus project its results into the future. We'd like prediction machinery not for a sense of predestiny but for guidance. And ideally it must be a Kauffman or von Neumann machine that can create things more complex that itself.
To do that, the model must possess a "requisite complexity." This is a term 
coined in the 1950s by the cybernetician Ross Ashby who built some of the first electronically adaptive models. Every model must distill a myriad of fine details about the real into a compressed representation; one of the most important traits it must condense is reality's complexity. Ashby concluded from his own experiments in making minimal models out of vacuum tubes that if a model simplifies the complexity too steeply, it misses the mark. A simulation's complexity has to be within the ballpark of the complexity of the modeled; otherwise the model can't keep up with the zig and zags of the thing modeled. Another cybernetician, Gerald Weinberg, supplies a fine metaphor for requisite complexity in his book On the Design of Stable Systems. Imagine, Weinberg suggests, a guided missile aimed at an enemy jet. The missile does not have to be a jet itself, but it must embody a requisite degree of complex flight behavior to parallel the behavior of the jet. If the missile is not at least as fast and aerodynamically nimble as the targeted jetfighter, then it cannot hit its target.

Stella-based models such as Limits to Growth possess a remarkable 
surfeit of feedback circuits. As Norbert Wiener showed in 1952, feedback 
circuits, in all their combinatorial variety, are the fountainhead of control and self-governance. But in the forty years since that initial flush of excitement about feedback, we now know that feedback loops alone are insufficient to breed the behaviors of the vivisystems we find most interesting. There are two additional types of complexity (there may be others) the researchers in this book have found necessary in order to birth the full spectrum of vivisystem character: distributed being and open-ended evolution. 
The key insight uncovered by the study of complex systems in recent years 
is this: the only way for a system to evolve into something new is to have a flexible structure. A tiny tadpole can change into a frog, but a 747 Jumbo Jet can't add six inches to its length without crippling itself. This is why distributed being is so important to learning and evolving systems. A decentralized, redundant organization can flex without distorting its function, and thus it can adapt. It can manage change. We call that growth. 
Direct feedback models such as Limits to Growth can achieve stabilization-
one attribute of living systems-but they can't learn, grow, or diversify-three essential complexities for a model of changing culture or life. Without these abilities, a world model will fall far behind the moving reality. A learning-less model can be used to anticipate the near-future where evolutionary change is minimal; but to predict an evolutionary system-if it can ever be predicted in pockets-will require the requisite complexity of a simulated, artificial evolutionary model. 
But we cannot import evolution and learning without exporting control. 
When Dana Meadows speaks of a collective human intelligence which steps back to perceive global problems and then "reaches in and restructures the system" of human endeavor, she is pointing to the greatest fault of the Limit to Growth model: its linear, mechanical, and unworkable notion of control. 
There is no control outside a self-making system. Vivisystems, such as 
economies, ecologies, and human culture, can hardly be controlled from any position. They can be prodded, perturbed, cajoled, herded, and at best, coordinated from within. On Earth, there is no outside platform from which to send an intelligent hand into the vivisystem, and no point inside where a control dial waits to be turned. The direction of large swarmlike systems 
such as human society is controlled by a messy multitude of interconnecting, self-contradictory agents who have only the dimmest awareness of where the whole is at any one moment. Furthermore, many active members of this swarmy system are not individual human intelligences; they are corporate entities, groups, institutions, technological systems, and even the nonbiological systems of the Earth itself. 
The song goes: No one is in charge. We can't predict the future. 
Now hear the flip side of the album: We are all steering. And we can learn to 
anticipate what is immediately ahead. To learn is to live.