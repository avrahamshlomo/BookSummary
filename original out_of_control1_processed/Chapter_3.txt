MACHINES WITH AN ATTITUDE 
When Mark Pauline offers you his hand in greeting, you get to shake his 
toes. Years ago Pauline blew off his fingers messing around with homemade 
rockets. The surgeons reconstituted a hand of sorts from his feet parts, but Pauline's lame hand still slows him down. 
Pauline builds machines that chew up other machines. His devices are 
intricate and often huge. His smallest robot is bigger than a man; the largest is two-stories high when it stretches its neck. Outfitted with piston-driven jaws and steam-shovel arms, his machines exude biological vibes. 
Pauline's maimed hand often has trouble threading a bolt to keep his monsters together. To quicken repairs he installed a top-of-the-line industrial lathe outside his bedroom door and stocked his kitchen area full of welding equipment. It only takes him a minute or two to braze the broken pneumatic limbs of his iron beasts. But his own hand is a hassle. He wants to replace it with a hand from a robot.
Pauline lives in a warehouse at the far end of a San Francisco street that 
dead-ends under a highway overpass. His pad is flanked by a bunch of grungy galvanized iron huts decorated with signs advertising car-body repair. A junkyard just outside Pauline's warehouse is piled as high as the chainlink fence with rusty skeletons of dead machines; one hunk is a jet engine. The yard is usually eerily vacant. When the postman hops out of his jeep to deliver Pauline's mail, the guy turns off his motor and locks the jeep door. 
Pauline started out as a self-described juvenile delinquent, later graduating 
to a young adult doing "creative vandalism." Everyone agrees that Mark Pauline's pranks are above average, even for an individualist's town like San Francisco. As a 10-year-old kid Pauline used a stolen acetylene torch to decapitate the globe of a gumball machine. As a young adult he got into the art of "repurposing" outdoor billboards: late at night he altered their lettering into political messages with creative applications of spray paint. He made news recently when his ex-girlfriend reported to the police that while she was away for a weekend he covered her car with epoxy and then feathered it, windshields and all. The devices Pauline builds are at once the most mechanical and the most 
biological of machines. Take the Rotary Mouth Machine: two hoops studded with sharklike teeth madly rotate in intersecting orbits, each at an angle to the other, so that their "bite" circles round and round. The spinning jaw can chew up a two-by-four in a second. Usually it nibbles the dangling arm of another machine. Or take the Inchworm, a modified farm implement powered by an automobile engine mounted on one end that cranks around six pairs of oversized tines to inch it along. It creeps in the most inefficient yet biological way. Or, the Walk-and-Peck machine. It uses its onboard canister of pressurized carbon dioxide to pneumatically chip though the asphalt by hammering its steel head into the ground, as if it were a demented 500 pound "roadpecker." "Most of my machines are the only machines of their type on Earth. No one else in their right mind would make them because there is no practical reason for humans to make them," Pauline claims, without a hint of a smile. 
A couple of times a year, Pauline stages a performance for his machines. His debut in 1979 was called "Machine Sex." During the show his eccentric machines ran into each other, consumed each other, and melded into broken heaps. A few years later he staged a spectacle called 
"Useless Mechanical Activity," continuing his work of liberating machines into their own world. He's put on about 40 shows since, usually in Europe where, he says, "I can't be sued." But Europe's system of national support for the arts (Pauline calls it the Art Mafia) also supports these in-your-face performances. 
In 1991 Pauline staged a machine circus in downtown San Francisco. On this 
night, several thousand fans dressed in punk black leather convened, entirely by word of mouth, at an abandoned parking lot squeezed under a freeway overpass ramp. In the makeshift arena, under the industrial glare of spotlights, ten or so mechanical animals and autonomous iron gladiators waited to demolish each other with flames and brute force. 
The scale and spirit of the iron creatures on display brought to mind one 
image: mechanical dinosaurs without skin. The dinos poised in the skeletal power of hydraulic hoses, chained gears, and cabled levers. Pauline called them "organic machines." 
These dinosaurs were not suffocating in a museum. Pauline had borrowed 
and stolen their parts from other machines, their power from automobiles, and had given them a meager kind of life to perform under the beams of searchlights stinking of hot ozone. Crash, rear up, jump, collide, live! 
The unseated audience that night churned in the titanium glare. 
Loudspeakers (chosen for their gritty static) played an endless stream of recorded industrial noise. The grating broadcasts sometimes switched to tapes of radio call-in shows and other background sounds of an electronic civilization. The screeching was upstaged by a shrieking siren; the signal to start. The machines moved. 
The next hour was pandemonium. A two-foot-long drill bit tipped the end of 
a brontosaurus-like creature's long neck. This nightmare of a dentist's drill was tapered like a bee's stinger. It went on a rampage and mercilessly drilled another robot. Wheeeezzz. The sound triggered toothaches. Another mad creature, the Screw Throwbot, comically zipped around, tearing up the pavement with an enormous racket. It was a ten-foot, one-ton steel sled carried by two steel corkscrew treads, each madly spinning auger 11/2 feet in diameter. It screwed across asphalt, skittering in various directions at 30 miles per hour. It was actually cute. Mounted on top was a mechanical catapult capable of hurling 50-lb. exploding firebombs. So while the Drill was stinging the Screw, the Screw was hurling explosives at a tower of pianos. 
"It's barely controlled anarchy here," Pauline joked at one point to his all-
volunteer crew. He calls his "company" the Survival Research Labs (SRL), a deliberately misleading corporate-sounding name. SRL likes to stage performances without official permits, without notification of the city's fire department, without insurance, and without advance publicity. They let the audience sit way too close. It looked dangerous. And it was. 
A converted commercial lawn sprinkler-the kind that normally creeps across grass blessing it with life-giving water-diabolically blessed the place with a shower of flames. Its rotating arms pumped fiery orange clouds of ignited kerosene fuel over a wide circle. The acrid, half-burnt smoke, trapped by the overhead freeway structure, choked the spectators. Then the Screw accidentally tipped over its fuel can, and the Sprinkler from Hell went out of commission. So the Flamethrower lit up to take up the slack. The Flamethrower was a steerable giant blower-of the type used to air-condition a mid-town skyscraper-bolted to a Mack truck engine. The truck motor twirled the huge cage-fan and pumped diesel fuel from a 55-gallon drum into the airstream. A carbon-arc spark ignited the air/fuel mixture and spewed it into a tongue of vicious yellow flame 50 feet long. It roasted the pile of 20 pianos. 
Pauline could aim the dragon with a radio-control joystick from a model 
airplane. He turned Flamethrower's snout toward the audience, who ducked reflexively. The heat, even from 50 feet away, slapped the skin. "You know how it is," Pauline said later. "Ecosystems without predators become unstable. Well, these spectators have no predators in their lives. So that's what these machines are, that's their role. To interject predators into civilization." 
SLR's machines are quite sophisticated, and getting more so. Pauline is 
always busy breeding new machines so that the ecology of the circus keeps evolving. Often he upgrades old models with new appendages. He may give the Screw Machine a pair of lobsterlike pincers instead of a buzz saw, or he welds a flamethrower to one arm of 25-foot-tall Big Totem. Sometimes he cross-fertilizes, swapping parts between two creatures. Other times he 
midwifes wholly new beings. At a recent show he unveiled four new pets: a portable lightning machine that spits 9-foot bolts of crackling blue lightning at nearby machines; a 120-decibel whistle driven by a jet engine; a military rail gun that uses magnetic propulsion to fire a burning comet of molten iron at 200 miles per hour, which upon impact explodes into a fine drizzle of burning droplets; and an advanced tele-presence cannon, a human/machine symbiont that lets a goggled operator aim the gun by turning his head to gaze at the target. It fires beer cans stuffed with concrete and dynamite detonators. 
The shows are "art," and so are constantly underfunded; the admission 
barely pays for the sundry costs of a show-for fuel, food for the workers, spare parts. Pauline candidly admits that some of the ancestors he cannibalized to procreate these monsters were stolen. One SRL crew member says that they like to put shows on in Europe because there is a lot of "Obtainium" there. What's Obtainium?: "Something that is easily obtained, easily liberated, or gotten for free." That which isn't made out of Obtainium is built from military surplus parts that Pauline buys by the truckload for $65 per pound from friendly downsizing military bases. He also scrounges the military for machine tools, submarine parts, fancy motors, rare electronics, $100,000-spare parts, and raw steel. "Ten years ago this stuff was valuable, important for national security and all that. Then suddenly it became worthless junk. Now I'm converting machines, improving them really, from things which once did 'useful' destruction into things that can now do useless destruction." 
Several years ago, Pauline made a crablike robot that would scurry across 
the floor. It was piloted by a freaked-out guinea pig locked inside a tiny switch-laden cockpit. The robot was not intended to be cruel. Rather the idea was to explore the convergence of the organic and the machine. SRL inventions commonly marry hi-speed heavy metal and soft biological architecture. When turned on, the guinea pig robot teetered on the edge of chaos. In the controlled anarchy of the show, it was hardly noticed. Pauline: "These machines barely have enough control to be useful, but that's all the control that we need." 
At the ground-breaking ceremony for the new San Francisco Museum of 
Modern Art, Pauline was invited to gather his machines on the empty downtown lot in order to "create a hallucination in broad daylight for a few minutes." His Shockwave Cannon wheeled about and exploded raw air. You could actually see the shockwave zip out of the muzzle. The Cannon halted rush-hour traffic as it rattled the windows of every car and skyscraper for blocks around. Pauline then introduced his Swarmers. These were waist-high cylindrical mobile robots that skittered around in a flock. Where the flock would go was anyone's guess; no one Swarmer directed the others; no one steered it. It was hardware heaven: machines out of control. 
The ultimate aim of SRL is to make machines autonomous. "Getting some autonomous action, though, is really difficult," Pauline told me. Yet he is 
ahead of many heavily funded university labs in attempting to transfer control from humans to machines. His several-hundred-dollar swarming creatures-decked out with recycled infrared sensors and junked stepped motors-beat out the MIT robot lab in an informal race to construct the first autonomous swarming robots. 
In the conflict many people see between nature-born and machine-made, 
Mark Pauline is on the side of the made. Pauline: "Machines have something to say to us. When I start designing an SRL show, I ask myself, what do these machines want to do? You know, I see this old backhoe that some red-neck is running everyday, maybe digging ditches out in the sun for the phone company. That backhoe is bored. It's ailing and dirty. We're coming along and asking it what it wants to do. Maybe it wants to be in our show. We go around and rescue machines that have been abandoned, or even dismembered. So we have to ask ourselves, what do these machines really want to do, what do they want to wear? So we think about color coordination and lighting. Our shows are not for humans, they are for machines. We don't ask how machines are going to entertain us. We ask, how can we entertain them? That's what our shows are, entertainment for machines." 
Machines are something that need entertainment. They have their own 
complexity and their own agenda. By building more complex machines we are giving them their own autonomous behavior and thus inevitably their own purpose. "These machines are totally at ease in the world we have built for them," Pauline told me. "They act completely natural." 
I asked Pauline, "If machines are natural, do they have natural rights?" "Big 
machines have a lot of rights," Pauline said. "I have learned respect for them. When one of them is coming toward you, they keep right on going. You need to get out of their way. That's how I respect them." 
The problem with our robots today is that we don't respect them. They are stuck in factories without windows, doing jobs that humans don't want to do. We take machines as slaves, but they are not that. That's what Marvin Minsky, the mathematician who pioneered artificial intelligence, tells anyone who will listen. Minsky goes all the way as an advocate for downloading human intelligence into a computer. Doug Englebart, on the other hand, is the legendary guy who invented word processing, the mouse, and hypermedia, and who is an advocate for computers-for-the-people. When the two gurus met at MIT in the 1950s, they are reputed to have had the following conversation: 
MINSKY: We're going to make machines intelligent. We are going to make them conscious! ENGLEBART: You're going to do all that for the machines? What 
are you going to do for the people? 
This story is usually told by engineers working to make computers more friendly, more humane, more people centered. But I'm squarely on Minsky's side-on the side of the made. People will survive. We'll train our machines to serve us. But what are we going to do for the machines? 
The total population of industrial robots working in the world today is close 
to a million. Nobody, except a crazy bad-boy artist in San Francisco, asks what the robots want; that's considered a silly, retrograde, or even sacrilegious sentiment. 
It's true that 99 percent of these million "bots" are little more than glorified 
arms. Smart arms, as far as arms go. And tireless. But as the robots we hoped for, they are dumb, blind, and still nursing the wall plug. 
Except for a few out-of-control robots of Mark Pauline, most muscle-bound 
bots of today are overweight, sluggish, and on the dole-addicted to continuous handouts of electricity and brain power. It is a chore to imagine them as the predecessor of anything interesting. Add another arm, some legs, and a head, and you have a sleepy behemoth. 
What we want is Robbie the Robot, the archetypal being of science fiction 
stories: a real free-ranging, self-navigating, auto-powered robot who can surprise. 
Recently, researchers in a few labs have realized that the most expedient 
path to Robbie the Robot was to cut off the electrical plug of a stationary robot. Make "mobots"-mobile robots. "Staybots" are okay, as long as the power and brains are fully contained in the arm. Any robot is better if it follows these two rules: move on your own; survive on your own. Despite his punk attitude and artistic sensibility, Pauline continues to build robots that often beat what the best universities of the world are doing. He uses discarded lab equipment from the very universities he's beating. A deep familiarity with the limits and freedoms of metal makes up for his lack of degrees. He doesn't use blueprints to build his organic machines. Just to humor an insistent reporter, Pauline scoured his workshop once to dig up ''plans" for a running machine he was creating. After twenty minutes of pawing around ("I know it was here last month"), he located a paper under an old 1984 phone book in the lower drawer of a beat-up metal desk. It was a pencil outline of the machine, a sketch really, with no technical specifications. 
"I can see it in my head. I lay out the lines on a hunk of metal and just 
starting cutting," Pauline told me as he held an elegantly machined piece of aluminum about two inches thick, roughly in the shape of a Tyrannosaurus arm bone. Two others identical to it lay on the workbench. He was working on the fourth. Each would become one part of the four legs of a running 
machine, about the size of a mule. 
Pauline's completed running machine doesn't really run. It walks fairly fast, lurching occasionally with surprising speed. No one has yet made a real running machine. A few years ago Pauline built a complicated four-legged giant walking machine. Twelve feet high, cube in shape, not very 
smart or nimble, but it did shuffle along slowly. Four square posts, as massive as tree trunks, became legs when energized by a clutter of hydraulic lines working in tandem with a humongous transmission. Like other SRL inventions, this ungainly beast was sort-of-steered by a radio-control unit designed for model cars. In other words the beast was a 2,000-pound dinosaur with a pea brain. 
Despite millions of dollars in research funding, no hacker has been able to 
coax a machine to walk across a room under its own intellect. A few robots cross in the unreal time of days, or they bump into furniture, or conk out after three-quarters of the way. In December 1990, after a decade of effort, graduate students at Carnegie Mellon University's Field Robotics Center wired together a robot that slowly walked all the way across a courtyard. Maybe 100 feet in all. They named him Ambler. 
Ambler was even bigger than Pauline's shuffling giant and was funded to 
explore distant planets. But CMU's mammoth prototype cost several million dollars of tax money to construct, while Pauline's cost several hundred dollars to make, of which 2/3 went for beer and pizza. The 19-foot-tall iron Ambler weighed 2 tons, not counting its brain which was so heavy it sat on the ground off to the side. This huge machine toddled in a courtyard, deliberating at each step. It did nothing else. Walking without tripping was enough after such a long wait. Ambler's parents applauded happily at its first steps. 
Moving its six crablike legs was the easiest part for Ambler. The giant had a 
harder time trying to figure out where it was. Simply representing the terrain so that it could calculate how to traverse it turned out to be Ambler's curse. Ambler spends its time, not walking, but worrying about getting the layout of the yard right. "This must be a yard," it says to itself. "Here are possible paths I could take. I'll compare them to my mental map of the yard and throw away all but the best one." Ambler works from a representation of its environment that it creates in its mind and then navigates from that symbolic chart, which is updated after each step. A thousand-line software program in the central computer manages Ambler's laser vision, sensors, pneumatic legs, gears, and motors. Despite its two-ton, two-story-high hulk, this poor robot is living in its head. And a head that is only connected to its body by a long cable. 
Contrast that to a tiny, real ant just under one of Ambler's big padded feet. 
It crosses the courtyard twice during Ambler's single trip. An ant weighs, brain and body, 1/100th of a gram-a pinpoint. It has no image of the 
courtyard and very little idea of where it is. Yet it zips across the yard without incident, without even thinking in one sense. 
Ambler was built huge and rugged in order to withstand the extreme cold 
and grit conditions on Mars, where it would not be so heavy. But ironically Ambler will never make it to Mars because of its bulk, while robots built like ants may. 
The ant approach to mobots is Rodney Brooks's idea. Rather than waste his 
time making one incapacitated genius, Brooks, an MIT professor, wants to make an army of useful idiots. He figures we would learn more from sending a flock of mechanical can-do cockroaches to a planet, instead of relying on the remote chance of sending a solitary overweight dinosaur with pretensions of intelligence. 
In a widely cited 1989 paper entitled "Fast, Cheap and Out of Control: A 
Robot Invasion of the Solar System," Brooks claimed that "within a few years it will be possible at modest cost to invade a planet with millions of tiny robots." He proposed to invade the moon with a fleet of shoe-box-size, solar-powered bulldozers that can be launched from throwaway rockets. Send an army of dispensable, limited agents coordinated on a task, and set them loose. Some will die, most will work, something will get done. The mobots can be built out of off-the-shelf parts in two years and launched completely assembled in the cheapest one-shot, lunar-orbit rocket. In the time it takes to argue about one big sucker, Brooks can have his invasion built and delivered. 
There was a good reason why some NASA folks listened to Brooks's bold 
ideas. Control from Earth didn't work very well. The minute-long delay in signals between an Earth station and a faraway robot teetering on the edge of a crevice demand that the robot be autonomous. A robot cannot have a remotely linked head, as Ambler did. It has to have an onboard brain operating entirely by internal logic and guidance without much communication from Earth. But the brains don't have to be very smart. For instance, to clear a landing pad on Mars an army of bots can dumbly spend twelve hours a day scraping away soil in the general area. Push, push, push, keep it level. One of them wouldn't do a very even job, but a hundred working as a colony could clear a building site. When an expedition of human visitors lands later, the astronauts can turn off any mobots still alive and give them a pat. 
Most of the mobots will die, though. Within several months of landing, the 
daily shock of frigid cold and oven heat will crack the brain chips into uselessness. But like ants, individual mobots are dispensable. Compared to Ambler, they are cheaper to launch into space by a factor of 1000; thus, sending hundreds of mobots is a fraction of the cost of one large robot. Brooks's original crackpot idea has now evolved into an official NASA 
program. Engineers at the Jet Propulsion Laboratory are creating a microrover. The project began as a scale model for a "real" planet rover, but as the virtues of small, distributed effort began to dawn on everyone, microrovers became real things in themselves. NASA's prototype tiny bot looks like a very flashy six-wheeled, radio-controlled dune buggy for kids. It is, but it is also solar-powered and self-guiding. A flock of these microrovers will probably end up as the centerpiece of the Mars Environmental Survey scheduled to land in 1997. 
Microbots are fast to build from off-the-shelf parts. They are cheap to launch. And once released as a group, they are out of control, without the need for constant (and probably misleading) supervision. This rough-and-ready reasoning is upside-down to the slow, thorough, in-control approach most industrial designers bring to complex machinery. Such radical engineering philosophy was reduced to a slogan: Fast, cheap, and out of control. Engineers envisioned fast, cheap, and out-of-control robots ideal for: (1) Planet exploration; (2) Collection, mining, harvesting; and (3) Remote construction.

"Fast, cheap, and out of control" began appearing on buttons of 
engineers at conferences and eventually made it to the title of Rodney 
Brooks's provocative paper. The new logic offered a completely different view of machines. There is no center of control among the mobots. Their identity was spread over time and space, the way a nation is spread over history and land. Make lots of them; don't treat them so precious. 
Rodney Brooks grew up in Australia, where like a lot of boys round the world, he read science fiction books and built toy robots. He developed a Downunder perspective on things, wanting to turn views on their heads. Brooks followed up on his robot fantasies by hopscotching around the prime robot labs in the U.S., before landing a permanent job as director of mobile robots at MIT. 
There, Brooks began an ambitious graduate program to build a robot that would be more insect than dinosaur. "Allen" was the first robot Brooks built. It kept its brains on a nearby desktop, because that's what all robot makers did at the time in order to have a brain worth keeping. The 
multiple cables leading to the brain box from Allen's bodily senses of video, sonar, and tactile were a neverending source of frustration for Brooks and crew. There was so much electronic background interference generated on the cables that Brooks burnt out a long string of undergraduate engineering students attempting to clear the problem. They checked every known communication media, including ham radio, police walkie-talkies and cellular phones, as alternatives, but all failed to find a static-free connection for such diverse signals. Eventually the undergraduates and Brooks vowed that on their next project they would incorporate the brains inside a robot-where no significant wiring would be needed-no matter how tiny the brains might have to be. 
They were thus forced to use very primitive logic steps, and very short and 
primitive connections in "Tom" and "Jerry," the next two robots they built. But to their amazement they found that the dumb way their onboard neural circuit was organized worked far better than a brain in getting simple things done. When Brooks reexamined the abandoned Allen in light of their modest success with dumb neurons, he recalled that "it turned out that in Allen's brain, there really was not much happening." 
The success of this profitable downsizing sent Brooks on a quest to see how 
dumb he could make a robot and still have it do something useful. He ended up with a type of reflex-based intelligence, and robots as dumb as ants. But they were as interesting as ants, too. 
Brooks's ideas gelled in a cockroachlike contraption the size of a football called "Genghis." Brooks had pushed his downsizing to an extreme. Genghis had six legs but no "brain" at all. All of its 12 motors and 21 sensors were distributed in a decomposable network without a centralized controller. Yet the interaction of these 12 muscles and 21 sensors yielded an amazingly complex and lifelike behavior. 
Each of Genghis's six tiny legs worked on its own, independent of the others. 
Each leg had its own ganglion of neural cells-a tiny microprocessor-that controlled the leg's actions. Each leg thought for itself! Walking for Genghis then became a group project with at least six small minds at work. Other small semiminds within its body coordinated communication between the legs. Entomologists say this is how ants and real cockroaches cope-they have neurons in their legs that do the leg's thinking. 
In the mobot Genghis, walking emerges out of the collective behavior of the 
12 motors. Two motors at each leg lift, or not, depending on what the other legs around them are doing. If they activate in the right sequence-Okay, hup! One, three, six, two, five, four!-walking "happens." 
No one place in the contraption governs walking. Without a smart central 
controller, control can trickle up from the bottom. Brooks called it "bottom-up control." Bottom-up walking. Bottom-up smartness. If you snip off one leg of a cockroach, it will shift gaits with the other five without losing a stride. The shift is not learned; it is an immediate self-reorganization. If you disable one leg of Genghis, the other legs organize walking around the five that work. They find a new gait as easily as the cockroach. 
In one of his papers, Rod Brooks first laid out his instructions on how to 
make a creature walk without knowing how: 
There is no central controller which directs the body where to put each foot or how high to lift a leg should there be an obstacle ahead. Instead, each leg is granted a few simple behaviors and each independently knows what to do under various circumstances. For instance, two basic behaviors can be thought of as "If I'm a leg and I'm up, put myself down, " or "If I'm a leg and I'm forward, put the other five legs back a little." These processes exist independently, run at all times, and fire whenever the sensory preconditions are true. To create walking then, there just needs to be a sequencing of lifting legs 
(this is the only instance where any central control is evident). As soon as a leg is raised it automatically swings itself forward, and also down. But the act of swinging forward triggers all the other legs to move back a little. Since those legs happen to be touching the ground, the body moves forward. 
Once the beast can walk on a flat smooth floor without tripping, other behaviors can be added to improve the walk. For Genghis to get up and over a mound of phone books on the floor, it needs a pair of sensing whiskers to send information from the floor to the first set of legs. A signal from a whisker can suppress a motor's action. The rule might be, "If you feel something, I'll stop; if you don't, I'll keep going." 
While Genghis learns to climb over an obstacle, the foundational walking 
routine is never fiddled with. This is a universal biological principle that Brooks helped illuminate-a law of god: When something works, don't mess with it; build on top of it. In natural systems, improvements are "pasted" over an existing debugged system. The original layer continues to operate without even being (or needing to be) aware that it has another layer above it. 
When friends give you directions on how to get to their house, they don't tell 
you to "avoid hitting other cars" even though you must absolutely follow this instruction. They don't need to communicate the goals of lower operating levels because that work is done smoothly by a well-practiced steering skill. Instead, the directions to their house all pertain to high-level activities like navigating through a town. 
Animals learn (in evolutionary time) in a similar manner. As do Brooks's 
mobots. His machines learn to move through a complicated world by building up a hierarchy of behaviors, somewhat in this order: 
Avoid contact with objects 
Wander aimlessly Explore the world 
Build an internal map 
Notice changes in the environment Formulate travel plans 
Anticipate and modify plans accordingly The Wander-Aimlessly Department doesn't give a hoot about obstacles, 
since the Avoidance Department takes such good care of that. 
The grad students in Brooks's mobot lab built what they cheerfully called 
"The Collection Machine"-a mobot scavenger that collected empty soda cans in their lab offices at night. The Wander-Aimlessly Department of the Collection Machine kept the mobot wandering drunkenly through all the rooms; the Avoidance Department kept it from colliding with the furniture while it wandered aimlessly. 
The Collection Machine roamed all night long until its video camera spotted 
the shape of a soda can on a desk. This signal triggered the wheels of the mobot and propelled it to right in front of the can. Rather than wait for a message from a central brain (which the mobot did not have), the arm of the robot "learned" where it was from the environment. The arm was wired so that it would "look" at its wheels. If it said, "Gee, my wheels aren't turning," then it knew, "I must be in front of a soda can." Then the arm reached out to pick up the can. If the can was heavier than an empty can, it left it on the desk; if it was light, it took it. With a can in hand the scavenger wandered aimlessly (not bumping into furniture or walls because of the avoidance department) until it ran across the recycle station. Then it would stop its wheels in front of it. The dumb arm would "look" at its hand to see if it was holding a can; if it was it would drop it. If it wasn't, it would begin randomly wandering again through offices until it spotted another can. 
That crazy hit-or-miss system based on random chance encounters was one 
heck of an inefficient way to run a recycling program. But night after night when little else was going on, this very stupid but very reliable system amassed a great collection of aluminum. 
The lab could grow the Collection Machine into something more complex by 
adding new behaviors over the old ones that worked. In this way complexity can be accrued by incremental additions, rather than basic revisions. The lowest levels of activities are not messed with. Once the wander-aimlessly module was debugged and working flawlessly, it was never altered. Even if wander-aimlessly should get in the way of some new higher behavior, the proven rule was suppressed, rather than deleted. Code was never altered, just ignored. How bureaucratic! How biological! 
Furthermore, all parts (departments, agencies, rules, behaviors) worked-and 
worked flawlessly-as stand-alones. Avoidance worked whether or not Reach-For-Can was on. Reach-For-Can worked whether or not Avoidance was on. The frog's legs jumped even when removed from the circuits of its head. 
The distributed control layout for robots that Brooks devised came to be 
known as "subsumption architecture" because the higher level of behaviors subsumed the roles of lower levels of behaviors when they wished to take control. If a nation were a machine, here's how you could build it using subsumption 
architecture: 
You start with towns. You get a town's logistics ironed out: basic stuff like 
streets, plumbing, lights, and law. Once you have a bunch of towns working reliably, you make a county. You keep the towns going while adding a layer of complexity that will take care of courts, jails, and schools in a whole district of towns. If the county apparatus were to disappear, the towns would still continue. Take a bunch of counties and add the layer of states. States collect taxes and subsume many of the responsibilities of governing from the county. Without states, the towns would continue, although perhaps not as effectively or as complexly. Once you have a bunch of states, you can add a federal government. The federal layer subsumes some of the activities of the states, by setting their limits, and organizing work above the state level. If the feds went away the thousands of local towns would still continue to do their local jobs-streets, plumbing and lights. But the work of towns subsumed by states and finally subsumed by a nation is made more powerful. That is, towns organized by this subsumption architecture can build, educate, rule, and prosper far more than they could individually. The federal structure of the U.S. government is therefore a subsumption architecture. 

A brain and body are made the same way. From the bottom up. Instead of 
towns, you begin with simple behaviors-instincts and reflexes. You make a 
little circuit that does a simple job, and you get a lot of them going. Then you overlay a secondary level of complex behavior that can emerge out of that bunch of working reflexes. The original layer keeps working whether the second layer works or not. But when the second layer manages to produce a more complex behavior, it subsumes the action of the layer below it. 
Here is the generic recipe for distributed control that Brooks's mobot lab 
developed. It can be applied to most creations: 
1) Do simple things first. 
2) Learn to do them flawlessly. 
3) Add new layers of activity over the results of the 
simple tasks. 
4) Don't change the simple things. 5) Make the new layer work as flawlessly as the simple. 
6) Repeat, ad infinitum. 
This script could also be called a recipe for managing complexity of any type, 
for that is what it is. 
What you don't want is to organize the work of a nation by a centralized 
brain. Can you imagine the string of nightmares you'd stir up if you wanted the sewer pipe in front of your house repaired and you had to call the Federal Sewer Pipe Repair Department in Washington, D.C., to make an appointment? 
The most obvious way to do something complex, such as govern 100 million 
people or walk on two skinny legs, is to come up with a list of all the tasks that need to be done, in the order they are to be done, and then direct their completion from a central command, or brain. The former Soviet Union's 
economy was wired in this logical but immensely impractical way. Its inherent instability of organization was evident long before it collapsed. 
Central-command bodies don't work any better than central-command 
economies. Yet a centralized command blueprint has been the main approach to making robots, artificial creatures, and artificial intelligences. It is no surprise to Brooks that braincentric folks haven't even been able to raise a creature complex enough to collapse. 
Brooks has been trying to breed systems without central brains so that they 
would have enough complexity worth a collapse. In one paper he called this kind of intelligence without centrality "intelligence without reason," a delicious yet subtle pun. For not only would this type of intelligence-one constructed layer by layer from the bottom up-not have the architecture of "reasoning," it would also emerge from the structure for no apparent reason at all. 
The USSR didn't collapse because its economy was strangled by a central 
command model. Rather it collapsed because any central-controlled complexity is unstable and inflexible. Institutions, corporations, factories, organisms, economies, and robots will all fail to thrive if designed around a central command. 
Yes, I hear you say, but don't I as a human have a centralized brain? 
Humans have a brain, but it is not centralized, nor does the brain have a 
center. "The idea that the brain has a center is just wrong. Not only that, it is radically wrong," claims Daniel Dennett. Dennett is a Tufts University professor of philosophy who has long advocated a "functional" view of the mind: that the functions of the mind, such as thinking, come from non-thinking parts. The semimind of a insectlike mobot is a good example of both animal and human minds. According to Dennett, there is no place that controls behavior, no place that creates "walking," no place where the soul of being resides. Dennett: "The thing about brains is that when you look in them, you discover that there's nobody home." 
Dennett is slowly persuading many psychologists that consciousness is an 
emergent phenomenon arising from the distributed network of many feeble, unconscious circuits. Dennett told me, "The old model says there is this central place, an inner sanctum, a theater somewhere in the brain where consciousness comes together. That is, everything must feed into a privileged representation in order for the brain to be conscious. When you make a conscious decision, it is done in the summit of the brain. And reflexes are just tunnels through the mountain that avoid the summit of consciousness." 
From this logic (very much the orthodox dogma in brain science) it follows, says Dennett, that "when you talk, what you've got in your brain is a 
language output box. Words are composed by some speech carpenters and put in the box. The speech carpenters get directions from a sub-system called the 'conceptualizer' which gives them a preverbal message. Of course the conceptualizer has to gets its message from some source, so it all goes on to an infinite regress of control." 
Dennett calls this view the "Central Meanor." Meaning descends from some 
central authority in the brain. He describes this perspective applied to language-making as the "idea that there is this sort of four-star general that tells the troops, 'Okay, here's your task. I want to insult this guy. Make up an English insult on the appropriate topic and deliver it.' That's a hopeless view of how speech happens." 
Much more likely, says Dennett, is that "meaning emerges from distributed 
interaction of lots of little things, no one of which can mean a damn thing." A whole bunch of decentralized modules produce raw and often contradictory parts-a possible word here, a speculative word there. "But out of the mess, not entirely coordinated, in fact largely competitive, what emerges is a speech act." 
We think of speech in literary fashion as a stream of consciousness pouring 
forth like radio broadcasts from a News Desk in our mind. Dennett says, "There isn't a stream of consciousness. There are multiple drafts of consciousness; lots of different streams, no one of which will be singled out as the stream." In 1874, pioneer psychologist William James wrote, "...the mind is at every stage a theatre of simultaneous possibilities. Consciousness consists in the comparisons of these with each other, the selection of some, and the suppression of the rest...." 
The idea of a cacophony of alternative wits combining to form what we think 
of as a unified intelligence is what Marvin Minsky calls "society of mind." Minsky says simply "You can build a mind from many little parts, each mindless by itself." Imagine, he suggests, a simple brain composed of separate specialists each concerned with some important goal (or instinct) such as securing food, drink, shelter, reproduction, or defense. Singly, each is a moron; but together, organized in many different arrangements in a tangled hierarchy of control, they can create thinking. Minsky emphatically states, "You can't have intelligence without a society of mind. We can only get smart things from stupid things." 
The society of mind doesn't sound very much different from a bureaucracy 
of mind. In fact, without evolutionary and learning pressures, the society of mind in a brain would turn into a bureaucracy. However, as Dennett, Minsky, and Brooks envision it, the dumb agents in a complex organization are always both competing and cooperating for resources and recognition. There is a very lax coordination among the vying parts. Minsky sees intelligence as generated by "a loosely-knitted league of almost separate agencies with almost independent goals." Those agencies that succeed are preserved, and those that don't vanish over time. In that sense, the brain is no monopoly, 
but a ruthless cutthroat ecology, where competition breeds an emergent cooperation. 
The slightly chaotic character of mind goes even deeper, to a degree our 
egos may find uncomfortable. It is very likely that intelligence, at bottom, is a probabilistic or statistical phenomenon-on par with the law of averages. The distributed mass of ricocheting impulses which form the foundation of intelligence forbid deterministic results for a given starting point. Instead of repeatable results, outcomes are merely probabilistic. Arriving at a particular thought, then, entails a bit of luck. 
Dennett admits to me, "The thing I like about this theory is that when 
people first hear about it they laugh. But then when they think about it, they conclude maybe it is right! Then the more they think about it, they realize, no, not maybe right, some version of it has to be right!" 
As Dennett and others have noted, the odd occurrence of Multiple 
Personalities Syndrome (MPS) in humans depends at some level on the decentralized, distributed nature of human minds. Each personality-Billy vs. Sally-uses the same pool of personality agents, the same community of actors and behavior modules to generate visibly different personas. Humans with MPS present a fragmented facet (one grouping) of their personality as a whole being. Outsiders are never sure who they are talking to. The patient seems to lack an "I." 
But isn't this what we all do? At different times of our life, and in different 
moods, we too shift our character. "You are not the person I used to know," screams the person we hurt by manifesting a different cut on our inner society. The "I" is a gross extrapolation that we use as an identity for ourselves and others. If there wasn't an "I" or "Me" in every person then each would quickly invent one. And that, Minsky says, is exactly what we do. There is no "I" so we each invent one. 
There is no "I" for a person, for a beehive, for a corporation, for an animal, 
for a nation, for any living thing. The "I" of a vivisystem is a ghost, an ephemeral shroud. It is like the transient form of a whirlpool held upright by a million spinning atoms of water. It can be scattered with a fingertip. 
But a moment later, the shroud reappears, driven together by the churning 
of a deep distributed mob. Is the new whirlpool a different form, or the same? Are you different after a near-death experience, or only more mature? If the chapters in this book were arranged in a different order, would it be a different book or the same? When you can't answer that question, then you know you are talking about a distributed system. 
continue...   
 
Out of Control
Inside every solitary living creature is a swarm of non-creature things. 
Inside every solitary machine one day will be a swarm of non-mechanical 
things. Both types of swarms have an emergent being and their own agenda. 
Brooks writes: "In essence subsumption architecture is a parallel and 
distributed computation for connecting sensors to actuators in robots." An important aspect of this organization is that complexity is chunked into modular units arranged in a hierarchy. Many observers who are delighted with the social idea of decentralized control are upset to hear that hierarchies are paramount and essential in this new scheme. Doesn't distributed control mean the end of hierarchy? 
As Dante climbed through a hierarchy of heavens, he ascended a hierarchy 
of rank. In a rank hierarchy, information and authority travels one way: from top down. In a subsumption or web hierarchy, information and authority travel from the bottom up, and from side to side. No matter what level an agent or module works at, as Brooks points out, "all modules are created equal....Each module merely does its thing as best it can." 
In the human management of distributed control, hierarchies of a certain 
type will proliferate rather than diminish. That goes especially for distributed systems involving human nodes-such as huge global computer networks. Many computer activists preach a new era in the network economy, an era built around computer peer-to-peer networks, a time when rigid patriarchal networks will wither away. They are right and wrong. While authoritarian "top-down" hierarchies will retreat, no distributed system can survive long without nested hierarchies of lateral "bottom-up" control. As influence flows peer to peer, it coheres into a chunk-a whole organelle-which then becomes the bottom unit in a larger web of slower actions. Over time a multi-level organization forms around the percolating-up control: fast at the bottom, slow at the top. 
The second important aspect of generic distributed control is that the 
chunking of control must be done incrementally from the bottom. It is impossible to take a complex problem and rationally unravel the mess into logical interacting pieces. Such well-intentioned efforts inevitably fail. For example, large companies created ex nihilo, as in joint ventures, have a remarkable tendency to flop. Large agencies created to solve another 
department's problems become problem departments in themselves. 
Chunking from the top down doesn't work for the same reason why 
multiplication is easier than division in mathematics. To multiply several prime numbers into a larger product is easy; any elementary school kid can do it. But the world's supercomputers choke while trying to unravel a product into its simple primes. Top-down control is very much like trying to decompose a product into its factors, while the large product is very easy to assemble from its factors up. 
The law is concise: Distributed control has to be grown from simple local 
control. Complexity must be grown from simple systems that already work. 
As a test bed for bottom-up, distributed control, Brian Yamauchi, a 
University of Rochester graduate student, constructed a juggling seeing-eye robot arm. The arm's task was to repeatedly bounce a balloon on a paddle. Rather than have one big brain try to figure out where the balloon was and then move the paddle to the right spot under the balloon and then hit it with the right force, Yamauchi decentralized these tasks both in location and in power. The final balancing act was performed by a committee of dumb "agents." 
For instance, the extremely complex question of Where is the balloon? was 
dispersed among many tiny logic circuits by subdividing the problem into several standalone questions. One agent was concerned with the simple query: Is the balloon anywhere within reach?-an easier question to act on. The agent in charge of that question didn't have any idea of when to hit the balloon, or even where the balloon was. Its single job was to tell the arm to back up if the balloon was not within the arm's camera vision, and to keep moving until it was. A network, or society, of very simpleminded decision-making centers like these formed an organism that exhibited remarkable agility and adaptability. 
Yamauchi said, "There is no explicit communication between the behavior 
agents. All communication occurs through observing the effects of actions that other agents have on the external world." Keeping things local and direct like this allows the society to evolve new behavior while avoiding the debilitating explosion in complexity that occurs with hardwired communication processes. Contrary to popular business preaching, keeping everybody informed about everything is not how intelligence happens. 
"We take this idea even further," Brooks said, "and often actually use the 
world as the communication medium between distributed parts." Rather than being notified by another module of what it expects to happen, a reflex module senses what happened directly in the world. It then sends its message to the others by acting upon the world. "It is possible for messages to get lost-it actually happens quite often. But it doesn't matter because the agent keeps sending the message over and over again. It goes 'I see it. I 
see it. I see it' until the arm picks the message up, and does something in the world to alter the world, deactivating the agent."

Centralized communication is not the only problem with a central brain. 
Maintaining a central memory is equally debilitating. A shared memory has 
to be updated rigorously, timely, and accurately-a problem that many corporations can commiserate with. For a robot, central command's challenge is to compile and update a "world model," a theory, or representation, of what it perceives-where the walls are, how far away the door is, and, by the way, beware of the stairs over there. 
What does a brain center do with conflicting information from many sensors? 
The eye says something is coming, the ear says it is leaving. Which does the brain believe? The logical way is to try to sort them out. A central command reconciles arguments and recalibrates signals to be in sync. In presubsumption robots, most of the great computational resources of a centralized brain were spent in trying to make a coherent map of the world based on multiple-vision signals. Different parts of the system believed wildly inconsistent things about their world derived from different readings of the huge amount of data pouring in from cameras and infrared sensors. The brain never got anything done because it never got everything coordinated. 
So difficult was the task of coordinating a central world view that Brooks 
discovered it was far easier to use the real world as its own model: "This is a good idea as the world really is a rather good model of itself." With no centrally imposed model, no one has the job of reconciling disputed notions; they simply aren't reconciled. Instead, various signals generate various behaviors. The behaviors are sorted out (suppressed, delayed, activated) in the web hierarchy of subsumed control. 
In effect, there is no map of the world as the robot sees it (or as an insect 
sees it, Brooks might argue). There is no central memory, no central command, no central being. All is distributed. "Communication through the world circumvents the problem of calibrating the vision system with data from the arm," Brooks wrote. The world itself becomes the "central" controller; the unmapped environment becomes the map. That saves an immense amount of computation. "Within this kind of organization," Brooks said, "very small amounts of computation are needed to generate intelligent behaviors." With no central organization, the various agents must perform or die. One 
could think of Brooks's scheme as having, in his words, "multiple agents within one brain communicating through the world to compete for the resources of the robot's body." Only those that succeed in doing get the attention of other agents. 
Astute observers have noticed that Brooks's prescription is an exact 
description of a market economy: there is no communication between agents, except that which occurs through observing the effects of actions (and not the actions themselves) that other agents have on the common world. The price of eggs is a message communicated to me by hundreds of millions of agents I have never met. The message says (among many other things): "A dozen eggs is worth less to us than a pair of shoes, but more than a two-minute telephone call across the country." That price, together with other price messages, directs thousands of poultry farmers, shoemakers, and investment bankers in where to put their money and energy. 
Brooks's model, for all its radicalism in the field of artificial intelligence, is 
really a model of how complex organisms of any type work. We see a subsumption, web hierarchy in all kinds of vivisystems. He points out five lessons from building mobots. What you want is: 
     Incremental construction-grow complexity, don't install it 
     Tight coupling of sensors to actuators-reflexes, not thinking 
     Modular independent layers-the system decomposes into viable subunits 
     Decentralized control-no central planning 
     Sparse communication-watch results in the world, not wires 
When Brooks crammed a bulky, headstrong monster into a tiny, featherweight bug, he discovered something else in this miniaturization. Before, the "smarter" a robot was to be, the more computer components it needed, and the heavier it got. The heavier it got, the larger the motors needed to move it. The heavier the motors, the bigger the batteries needed to power it. The heavier the batteries, the heavier the structure needed to move the bigger batteries, and so on in an escalating vicious spiral. The spiral drove the ratio of thinking parts to body weight in the direction of ever more body. 
But the spiral worked in the other direction even nicer. The smaller the 
computer, the lighter the motors, the smaller the batteries, the smaller the structure, and the stronger the frame became relative to its size. This also drove the ratio of brains to body towards a mobot with a proportionally 
larger brain, small though its brain was. Most of Brooks's mobots weighed less than ten pounds. Genghis, assembled out of model car parts, weighed only 3.6 pounds. Within three years Brooks would like to have a 1-mm (pencil-tip-size) robot. "Fleabots" he calls them. 
Brooks calls for an infiltration of robots not just on Mars but on Earth as 
well. Rather than try to bring as much organic life into artificial life, Brooks says he's trying to bring as much artificial life into real life. He wants to flood the world (and beyond) with inexpensive, small, ubiquitous semi-thinking things. He gives the example of smart doors. For only about $10 extra you could put a chip brain in a door so that it would know you were about to go out, or it could hear from another smart door that you are coming, or it could notify the lights that you left, and so on. If you had a building full of these smart doors talking to each other, they could help control the climate, as well as help traffic flow. If you extend that invasion to all kinds of other apparatus we now think of as inert, putting fast, cheap, out-of-control intelligence into them, then we would have a colony of sentient entities, serving us, and learning how to serve us better. 
When prodded, Brooks predicts a future filled with artificial creatures living 
with us in mutual dependence-a new symbiosis. Most of these creatures will be hidden from our senses, and taken for granted, and engineered with an insect approach to problems-many hands make light work, small work done ceaselessly is big work, individual units are dispensable. Their numbers will outnumber us, as do insects. And in fact, his vision of robots is less that they will be R2D2s serving us beers, than that they will be an ecology of unnamed things just out of sight. 
One student in the Mobot Lab built a cheap, bunny-size robot that watches 
where you are in a room and calibrates your stereo so it is perfectly adjusted as you move around. Brooks has another small robot in mind that lives in the corner of your living room or under the sofa. It wanders around like the Collection Machine, vacuuming at random whenever you aren't home. The only noticeable evidence of its presence is how clean the floors are. A similar, but very tiny, insectlike robot lives in one corner of your TV screen and eats off the dust when the TV isn't on. 
Everybody wants programmable animals. "The biggest difference between 
horses and cars," says Keith Hensen, a popular techno-evangelist, "is that cars don't need attention every day, and horses do. I think there will be a demand for animals that can be switched on and off." 
"We are interested in building artificial beings," Brooks wrote in a manifesto 
in 1985. He defined an artificial being as a creation that can do useful work while surviving for weeks or months without human assistance in real environment. "Our mobots are Creatures in the sense that on power-up they exist in the world and interact with it, pursuing multiple goals. This is in contrast to other mobile robots that are given programs or plans to follow for a specific mission." Brooks was adamant that he would not build toy 
(easy, simple) environments for his beings, as most other robotists had done, saying "We insist on building complete systems that exist in the real world so that we won't trick ourselves into skipping hard problems." 
To date, one hard problem science has skipped is jump-starting a pure mind. If Brooks is right, it probably never will. Instead it will grow a mind from a dumb body. Almost every lesson from the Mobot Lab seems to teach that there is no mind without body in a real unforgiving world. "To think is to act, and to act is to think," said Heinz von Foerster, gadfly of the 1950s cybernetic movement. "There is no life without movement."

Ambler's dinosaur troubles began because we humans, with our 
attendant minds, think we are more like Ambler than ants. Since the vital 
physiological role of the brain has become clear to medicine, the vernacular sense of our center has migrated from the ancient heart to newfangled mind. 
We twentieth century humans live entirely in our heads. And so we build 
robots that live in their heads. Scientists-humans too-think of themselves as beings focused onto a spot just south of their forehead behind their eyeballs. There breathes us. In fact, in 1968, brain death became the deciding threshold for human life. No mind, no life. 
Powerful computers birthed the fantasy of a pure disembodied intelligence. 
We all know the formula: a mind inhabiting a brain submerged in a vat. If science would assist me, the contemporary human says, I could live as a brain without a body. And since computers are big brains, I could live in a computer. In the same spirit a computer mind could just as easily use my body. 
One of the tenets in the gospel of American pop culture is the widely held 
creed of transferability of mind. People declare that mind transfer is a swell idea, or an awful idea, but not that it is a wrong idea. In modern folk-belief, mind is liquid to be poured from one vessel to another. From that comes Terminator 2, Frankenstein, and a huge chunk of science fiction. 
For better or worse, in reality we are not centered in our head. We are not 
centered in our mind. Even if we were, our mind has no center, no "I." Our bodies have no centrality either. Bodies and minds blur across each others' supposed boundaries. Bodies and minds are not that different from one another. They are both composed of swarms of sublevel things. 
We know that eyes are more brain than camera. An eyeball has as much 
processing power as a supercomputer. Much of our visual perception happens in the thin retina where light first strikes us, long before the central brain gets to consider the scene. Our spinal cord is not merely a trunk line transmitting phone calls from the brain. It too thinks. We are a lot closer to the truth when we point to our heart and not our head as the center of behaviors. Our emotions swim in a soup of hormones and peptides that percolate through our whole body. Oxytocin discharges thoughts of love 
(and perhaps lovely thoughts) from our glands. These hormones too process information. Our immune system, by science's new reckoning, is an amazing parallel, decentralized perception machine, able to recognize and remember millions of different molecules. 
For Brooks, bodies clarify, simplify. Intelligences without bodies and beings 
without form are spectral ghosts guaranteed to mislead. Building real things in the real world is how you'll make complex systems like minds and life. Making robots that have to survive in real bodies, day to day on their own, is the only way to find artificial intelligence, or real intelligence. If you don't want a mind to emerge, then unhinge it from the body.

Tedium can unhinge a mind. 
Forty years ago, Canadian psychologist D. O. Hebbs was intrigued by the 
bizarre delusions reported by the ultrabored. Radar observers and long-distance truck drivers often reported blips that weren't there, and stopped for hitchhikers that didn't exist. During the Korean War, Hebbs was contacted by the Canadian Defense Research Board to investigate another troublesome product of monotony and boredom: confessions. Seems that captured UN soldiers were renouncing the West after being brainwashed (a new word) by the communists. Isolation tanks or something. 
So in 1954 Hebbs built a dark, soundproof cell at McGill University in 
Montreal. Volunteers entered the tiny cramped room, donned translucent goggles, padded their arms in cardboard, gloved their hands with cotton mittens, covered their ears with earphones playing a low noise, and laid in bed, immobile, for two to three days. They heard a steady hum, which soon melted into a steady silence. They felt nothing but a dull ache in their backs. They saw nothing but a dim grayness, or was it blackness? The amazonian flow of colors, signals, urgent messages that had been besieging their brains since birth evaporated. Slowly, each of their minds unhitched from its moorings in the body and spun. 
Half of the subjects reported visual sensations, some within the first hour: "a 
row of little men, a German helmet...animated integrated scenes of a cartoonlike character." In the innocent year of 1954 the Canadian scientists reported: "Among our early subjects there were several references, rather puzzling at first, to what one of them called 'having a dream while awake.' Then one of us, while serving as a subject, observed the phenomenon and realized its peculiarity and extent." By the second day of stillness the subjects might report "loss of contact with reality, changes in body image, speech difficulties, reminiscence and vivid memories, sexual preoccupation, inefficiencies of thought, complex dreams, and a higher incident of worry and fright." They didn't say "hallucinations" because that wasn't a word in their vocabulary. Yet. 
Hebb's experiments were taken up a few years later by Jack Vernon, who 
built a "black room" in the basement of the psychology hall at Princeton. He recruited graduate students who hoped to spend four days or so in the dark "getting some thinking done." One of the initial students to stay in the 
numbing room told the debriefing researchers later, "I guess I was in there about a day or so before you opened the observation window. I wondered why you waited so long to observe me." There was, of course, no observation window. 
In the silent coffin of disembodiment, few subjects could think of anything in 
particular after the second day. Concentration crumbled. The pseudobusyness of daydreaming took over. Worse were thoughts of an active mind that got stuck in an inactive loop. "One subject made up a game of listing, according to the alphabet, each chemical reaction that bore the name of the discoverer. At the letter n he was unable to think of an example. He tried to skip n and go on, but n kept doggedly coming up in his mind, demanding an answer. When this became tiresome, he tried to dismiss the game altogether, only to find that he could not. He endured the insistent demand of his game for a short time, and, finding that he was unable to control it, he pushed the panic button." 
The body is the anchor of the mind, and of life. Bodies are machines to 
prevent the mind from blowing away under a wind of its own making. The natural tendency of neural circuitry is to play games with itself. Left on its own, without a direct link to "outside," a brainy network takes its own machinations as reality. A mind cannot possibly consider anything beyond what it can measure or calculate; without a body it can only consider itself. Given its inherent curiosity, even the simplest mind will exhaust itself devising solutions to challenges it confronts. Yet if most of what it confronts is its own internal circuitry and logic, then it spends its days tinkering with its latest fantasy. 
The body-that is, any bundle of senses and activators-interrupts this natural 
mental preoccupation with an overload of urgent material that must be considered right now! A matter of survival! Should we duck?! The mind no longer needs to invent its reality-the reality is in its face, rapidly approaching dead-on. Duck! it decides by a new and wholly original insight it had never tried before, and would have never thought to try. 
Without senses, the mind mentally masturbates, engendering a mental 
blindness. Without the interruptions of hellos from the eye, ear, tongue, nose, and finger, the evolving mind huddles in the corner picking its navel. The eye is most important because being half brain itself (chock-full of neurons and biochips) it floods the mind with an impossibly rich feed of half-digested data, critical decisions, hints for future steps, clues of hidden things, evocative movements, and beauty. The mind grinds under the load, and behaves. Cut loose from its eyes suddenly, the mind will rear up, spin, retreat. 
The cataracts that afflict elderly men and women after a life of sight can be 
removed, but not without a brief journey into a blindness even darker than what cataracts bring. Doctors surgically remove the lens growths and then cover patients' eyes with a black patch to shield them from light and to 
prevent the eyeballs from moving, as they unconsciously do whenever they look. Since the eyes move in tandem, both are patched. To further reduce eye movement, patients lie in bed, quiet, for up to a week. At night, when the hospital bustle dies down, the stillness can match the blackness under the blindfold. In the early 1900s when this operation was first commonly performed, there was no machinery in hospitals, no TV or radio, few night shifts, no lights burning. Eyes wrapped in bandages in the cataract ward, the world as hushed and black as the deepest forever. 
The first day was dim but full of rest and still. The second day was darker. 
Numbing. Restless. The third day was black, black, black, silent, and filled with red bugs crawling on the walls. 
"During the third night following surgery [the 60-year-old woman] tore her 
hair and the bedclothes, tried to get out of bed, claimed that someone was trying to get her, and said that the house was on fire. She subsided when the bandage was removed from the unoperated eye," stated a hospital report in 1923. 
In the early 1950s, doctors at Mount Sinai Hospital in New York studied a 
sample of 21 consecutive admissions to the cataract ward. "Nine patients became increasingly restless, tore off the masks, or tried to climb over the siderails. Six patients had paranoid delusions, four had somatic complaints, four were elated [!!], three had visual hallucinations, and two had auditory hallucinations." 
"Black patch psychosis" is now something ophthalmologists watch for on the 
wards. I think universities should keep an eye out for it too. Every philosophy department should hang a pair of black eye patches in a red firealarm-like box that says, "In case of argument about mind/body, break glass, put on." 
In an age of virtual everything, the importance of bodies cannot be 
overemphasized. Mark Pauline and Rod Brooks have advanced further than most in creating personas for machines, because the creatures are fully embodied. They insist that their robots be situated in real environments. 
Pauline's automatons don't live very long. By the end of his shows, only a 
few iron beasts still move. But to be fair to Pauline, none of the other university robots have lived much longer than his. It is a rare mobile robot that has an "on" lifetime of more than dozens of hours. For the most part, automatons are improved while they are off. In essence, robotists are trying to evolve things while dead, a curious situation that hasn't escaped some researchers' notice. "You know, I'd like to build a robot that could run 24 hours a day for weeks. That's the way for a robot to learn," says Maja Mataric, one of Brooks's robot builders at MIT. When I visited the Mobot Lab at MIT, Genghis lay sprawled in disassembled 
pieces on a lab bench. New parts lay nearby. "He's learning," quipped Brooks. 
Genghis was learning, but not in any ultimately useful manner. He had to 
rely on the busy schedules of Brooks and his busy grad students. How much better to learn while alive. That is the next big step for machines. To learn over time, on their own. To not only adapt, but evolve. 
Evolution proceeds in steps. Genghis is an insect-equivalent. Its descendants 
someday will be rodents, and someday further, as smart and nimble as apes. 
But we need to be a little patient in our quest for machine evolution, Brooks 
cautions. From day one of Genesis, it took billions of years for life to reach plant stage, and another billion and a half before fish appeared. A hundred million years later insects made the scene. "Then things really started moving fast," says Brooks. Reptiles, dinosaurs, and mammals appeared within the next 100 million years. The great, brainy apes, including man, arrived in the last 20 million years. 
The relatively rapid complexification in most recent geological history 
suggests to Brooks "that problem solving behavior, language, expert knowledge and reason, are all pretty simple once the essence of being and reacting are available." Since it took evolution 3 billion years to get from single cells to insects, but only another half billion years from there to humans, "this indicates the nontrivial nature of insect level intelligence." 
So insect life-the problem Brooks is sweating over-is really the hard part. 
Get artificial insects down, and artificial apes will soon follow. This points to a second advantage to working with fast, cheap, and out-of-control mobots: the necessity of mass numbers for evolution. One Genghis can learn. But evolution requires a seething population of Genghises to get anything done. 
To evolve machines, we'll need huge flocks of them. Gnatbots might be 
perfect. Brooks ultimately dreams of engineering vivisystems full of machines that both learn (adjust to variations in environment ) and evolve (populations of critters undergoing "gazillions of trials"). 
When democracy was first proposed for (and by) humans, many reasonable 
people rightly feared it as worse than anarchy. They had a point. A democracy of autonomous, evolving machines will be similarly feared as Anarchy Plus. This fear too has some truth. 
Chris Langton, an advocate of autonomous machine life, once asked Mark 
Pauline, "When machines are both superintelligent and superefficient, what will be the niche for humans? I mean, do we want machines, or do we want us?" Pauline responded in words that I hope echo throughout this book: "I think 
humans will accumulate artificial and mechanical abilities, while machines will accumulate biological intelligence. This will make the confrontation between the two even less decisive and less morally clear than it is now." 
So indecisive that the confrontation may resemble a conspiracy: robots who 
think, viruses that live in silicon, people hotwired to TV sets, life engineered at the gene level to grow what we want, the whole world networked into a human/machine mind. If it all works, we'll have contraptions that help people live and be creative, and people who help the contraptions live and be creative. 
Consider the following letter published in the June 1984 IEEE Spectrum. 
Mr. Harmon Blis
Topnotch Professionals Inc.7777 Turing Blvd.Palo Alto, CA 94301
June 1, 2034Dear Mr. Bliss:I am pleased to support your consideration of a human for 
professional employment. As you know, humans historically have proved to be the providers of choice. There are many reasons why we still recommend them strongly. 
As their name would suggest, humans are humane. They can 
transmit a feeling of genuine concern to their clients that makes for a better, more productive relationship. 
Each human is unique. There are many situations that reward 
multiple viewpoints, and there is nothing like a team of individualistic humans to provide this variety. 
Humans are intuitive, which enables them to make decisions 
even when they can't justify why. 
Humans are flexible. Because clients often place highly varied, 
unpredictable demands on professionals, flexibility is crucial. 
In summary, humans have a lot going for them. They are not a 
panacea, but they are the right solution for a class of important and challenging employment problems. Consider this human carefully. Yours truly.
Frederick Hayes-Roth 
The greatest social consequence of the Darwinian revolution was the 
grudging acceptance by humans that humans were random descendants of monkeys, neither perfect nor engineered. The greatest social consequence of neo-biological civilization will be the grudging acceptance by humans that humans are the random ancestors of machines, and that as machines we can be engineered ourselves. 
I'd like to condense that further: Natural evolution insists that we are apes; 
artificial evolution insists that we are machines with an attitude. 
I believe that humans are more than the combination of ape and machine 
(we have a lot going for us!), but I also believe that we are far more ape and machine than we think. That leaves room for an unmeasured but discernible human difference, a difference that inspires great literature, art, and our lives as a whole. I appreciate and indulge in those sentiments. But what I have encountered in the rather mechanical process of evolution, and in the complex but knowable interconnections underpinning living systems, and in the reproducible progress in manufacturing reliable behaviors in robots, is a singular unity between simple life, machines, complex systems, and us. This unity can stir lofty inspirations the equal of any passion in the past. 
Machines are a dirty word now. This is because we have withheld from them 
the full elixir of life. But we are poised to remake them into something that one day may be taken as a compliment. 
As humans, we find spiritual refuge in knowing that we are a branch in the 
swaying tree of life spread upon this blue ball. Perhaps someday we will find spiritual wholesomeness in knowing we are a link in a complex machine layered on top of the green life. Perhaps we'll sing hymns rhapsodizing our role as an ornate node in a vast network of new life that is spawning on top of the old. 
When Pauline's monsters demolish fellow monsters, I see not useless 
destruction, but lions stalking zebras keeping wildlife on course. When the iron paw of Brooks's six-legged Genghis hunts for a place to grip, I see not workers relieved of robotic jobs, but joyful baby squirms of a new organism. We are of one nature in the end. Who will not feel a bit of holy awe on the day when machines talk back to us?
continue...   
 
Out of Control