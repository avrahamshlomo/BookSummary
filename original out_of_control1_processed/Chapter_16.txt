THE FUTURE OF CONTROL 
The absolutely neat thing about the dinosaurs in the movie Jurassic Park 
is that they possess enough artificial life so that they can be reused as 
cartoon dinos in a Flintstones movie.
They won't be completely the same of course. They'll be tamer, longer, 
rounder, and more obedient. But inside Dino will beat the digital heart of T. Rex and Velociraptor-different bodies but the same dinosaurness. Mark Dippe, the wizard at Industrial Light and Magic who invented the virtual dinosaurs, has merely to alter the settings in the creatures' digital genes to transform their shape into lovable pets, while maintaining their convincing screen presence.
Yet the Jurassic Park dinosaurs are zombies. They have magnificent 
simulated bodies, but they lack their own behavior, their own will, their own drive for survival. They are ghostly muppets guided by computer animators. Someday, though, the dinosaurs may become Pinocchios-puppets given their own life. 
Before the Jurassic dinosaurs were imported into the photo-realistic world of 
a movie, they dwelt in a empty world consisting solely of three dimensions. In this dreamland-let's think of it as that place where all the flying logos for TV stations live-there is volume, light, and space, but not much else. Wind, gravity, inertia, friction, stiffness, and all the subtle aspects of a material world are absent and have to be faked by imaginative animators. 
"In traditional animation all knowledge of physics has to come from the 
animator's head, " says Michael Kass, a computer graphics engineer at Apple Computer. For instance, when Walt Disney drew Mickey Mouse bouncing downstairs on his rear end, Disney played out on drawing paper his perception of how the law of gravity works. Mickey obeyed Disney's ideas of physics, whether they were realistic or not. They usually weren't, which has always been their charm. Many animators exaggerated, altered, or ignored the physical laws of the real world for a laugh. But in the current cinematic style, the goal is strict realism. Modern audiences want E.T.'s flying bicycle to behave like a "real" flying bicycle, not like a cartoon version. 
Kass is trying to imbue physics into simulated worlds. "We thought about the 
tradition of having the physics in the animator's head and decided that instead, the computer should have some knowledge of physics."
Say we start with flying logo dreamland. One of the problems with this 
simple world, Kass says, is that "things look like they don't weigh anything." To increase the realism of the world we could add mass and weight to objects and a gravity law to the environment, so that if a flying logo drops to the floor it falls at the same acceleration as would a solid logo falling to Earth. The equation for gravity is very simple, and implanting it in a small world is not difficult. We could add a bounce formula to the animated logo so that it rebounds from the floor "of its own accord" in a very regular manner. It obeys the rule of gravity and the rules of kinetic energy and friction which slow it down. And it can be given stiffness-say of plastic or metal-so that it reacts to an impact realistically. The final result has the feel of reality, as a chrome logo falls to the floor and bounces in diminishing hops until it clatters to a rest.
We might continue to apply additional formulas of physical rules, such as 
elasticity, surface tension, and spin effects, and code them into the environment. As we increase the complexity of these artificial environments, they become fertile ground for synthetic life. 
This is why the Jurassic dinosaurs were so lifelike. When they lifted their 
legs, they encountered the virtual weight of meat. Their muscles flexed and sagged. When the foot came down, gravity pulled it, and the impact of landing reverberated back up the leg.
The talking cat in Disney's summer of '93 movie Hocus Pocus was a virtual 
character similar to the dinosaurs, but in close-up. The animators built a digital cat form and then "texture-mapped" its fur from a photographed cat, which it perfectly resembled except for its remarkable talking. Its mouth behavior was mapped from a human. The thing was a virtual cat-human hybrid.
A movie audience watches autumn leaves blowing down the street. The 
audience does not realize the scene is computer-generated animation. The event looks real because the video is of something real: individual virtual leaves being blown by a virtual wind down a virtual street. As in Reynolds's flocks of virtual bats, there is a real shower of things really being pushed by a force in a place with physical laws. The virtual leaves have attributes such as weight and shape and surface area. When they are released into a virtual wind they obey a set of laws parallel to the real ones that real leaves obey. The relationship between all the parts is as real as a New England day, although the lack of details in the leaves wouldn't work in close-up. The blowing leaves are not so much drawn as let loose. 
Letting animations follow their own physics is the new recipe for realism. 
When Terminator 2 wells up from a molten pool of chrome, the effect is astoundingly convincing because the chrome is obeying physical constraints of liquids (such as surface tension) in a parallel universe. It is a liquid in 
simulation.
Kass and Apple colleague Gavin Miller came up with computer programs to 
render the subtle ways in which water trickles down a shallow stream, or falls as rain on a puddle. They transferred the laws of hydrology into a simulated universe by hooking up the formulas to an animating engine. Their video clips show a shallow wave sweeping over a dry sandy shore under a soft light, breaking in the irregular manner of real waves, then receding, leaving wet sand behind. In reality it's all just equations.
To make these digital worlds really work in the future, everything in creation 
will have to be reduced to equations. Not just the dinosaurs and water, but eventually the trees the dinos munched on, the jeeps (which were digital in some scenes of Jurassic Park), buildings, clothes, breakfast tables, and the weather. If this all had to happen just for the movies, it wouldn't. But every manufactured item in the near future will be designed and produced using CAD (computer-assisted design) programs. Already today, automobile parts are simulated on computer screens first, and their equations later transmitted directly to the factory lathes and welders to give the numbers actual form. A new industry called automatic fabrication takes the data from a CAD and instantly generates a 3-D prototype from powered metal or liquid plastic. First an object is just lines on a screen; then it's a solid thing you can hold in your hand or walk around. Instead of printing a picture of a gear, automatic fabrication technology "prints" the actual gear itself. Emergency spare parts for factory machines are now printed out in hi-impact plastic on the factory floor; they'll hold out until the authentic spare part arrives. Someday soon, the printed object will be the authentic part. John Walker, founder of the world's premier CAD program, AutoCAD, told a reporter, "CAD is about building models of real-world objects inside the computer. I believe in the fullness of time, every object in the world, manufactured or not, will be modeled inside a computer. This is a very, very big market. This is everything." 
Biology included. Flowers can already be modeled in computers. Przemyslaw 
Prusinkiewicz, a computer scientist at the University of Calgary, Canada, uses a mathematical model of botanical growth to create 3-D virtual flowers. A few simple laws apparently govern most plant growth. Flowering signals can get complicated. The blossom sequence on a stalk may be determined by several interacting messages. But these interacting signals can be coded into a program quite simply.
The mathematics of growing plants was worked out in 1968 by the 
theoretical biologist Aristid Lindenmeyer. His equations articulated the distinction between a carnation and a rose; the difference can be reduced to a set of variables in a numerical seed. An entire plant may only take a few kilobytes on a hard disk-a seed. When the seed is decompressed by the computer program, a graphical flower grows on the screen. First a green sprout shoots up, leaves unfurl, a bud takes shape, and then, at the right moment, a flower blossoms. Prusinkiewicz and his students have scoured 
the botanical literature to discover how multiple heads of flowers bloom, or how a daisy forms, and how an elm or oak fork their distinctive branches. They have also compiled algorithmic laws of growth for hundreds of seashells and butterflies. The graphical results are entirely convincing. A still frame of one of Prusinkiewicz's computer-grown lilac sprays with its myriad florets could pass for a photograph in a seed catalog. 
At first this was a fun academic exercise, but Prusinkiewicz is now besieged 
with calls from horticulturists wanting his software. They'll pay a lot of money if they can get a program that will show their clients what their landscape designs will look like in ten years or even next spring.
The best way to fake a living creature, Prusinkiewicz found, is to grow it. The 
laws of growth he has extracted from biology and then put into a virtual world are used to grow cinematic trees and flowers. They make a wonderfully apt environment for dinosaurs or other digital characters.
Brøderbund software, a venerable publisher of educational software for 
personal computers, sells a program that models physical forces as a way of teaching physics. When you boot-up the Physics program on your Macintosh you launch a toy planet that orbits the sun on the computer screen. The virtual planet obeys the forces of gravity, motion, and friction written into the toy universe. By fiddling with the forces of momentum and gravity, a student can get a feel for how the physics of the solar system works. 
How far can we press this? If we kept adding other forces that the toy planet 
had to obey, such as electrostatic attraction, magnetism, friction, thermodynamics, volume, if we kept adding every feature we saw in the real world to this program, what kind of solar system would we eventually have in the computer? If a computer is used to model a bridge-all its forces of steel, wind, and gravity-could we ever get to the point that we could say we have a bridge inside the computer? And can we do this with life?
As fast as physics is encroaching into digital worlds, life is invading faster. To 
see how far distributed life has infiltrated computational cinema, and to what consequences, I took a tour of the state-of-the-art animation labs.

Mickey Mouse is one of the ancestors of artificial life. Mickey, now 66 years 
old, will soon have to face the digital era. In one of the permanent 
"temporary" buildings on the backlot of Disney's Glendale studios, his trustees were cautiously planning ways to automate animated characters and backgrounds. I spoke to Bob Lambert, director of new technologies for the Disney animators.
The first thing Bob Lambert made clear to me was that Disney was in no 
hurry to completely automate animation. Animation was a handcraft, an art. Disney Inc.'s great fortune was sealed in this craft, and their crown jewels-Mickey Mouse and pals-were perceived by their customers as exemplary works of art. If computer animation meant anything like the wooden robots kids see on Saturday morning cartoons then Disney wanted no part of it. Lambert: "We don't need people saying, 'Oh damn, there goes another handcrafted art down the computer hole.'"
Then there was the problem of the artists themselves. Said Lambert, "Look, 
we have 400 ladies in white smocks who have been painting Mickey for 30 years. We can't change suddenly." 
The second thing Lambert wanted to make clear was that Disney had 
already been using some automated animation in their legendary films since 1990. Gradually they were digitizing their worlds. Their animators had gotten the message that those who didn't transfer their artists' intelligence from their heads into an almost living simulation would soon be dinosaurs of another kind. "To be honest," said Lambert, "by 1992 our animators were clamoring to use computers."
The giant clockwork in Disney's The Great Mouse Detective was a computer-
generated model of a clock that hand-drawn characters ran over. In Rescuers Down Under, Oliver the Albatross dove down through a virtual New York City, a completely computer-generated environment grown from a large database of New York buildings compiled by a large contractor for commercial reasons. And in The Little Mermaid, Ariel swam through clusters of fish whose schooling was simulated, seaweed that swayed autonomously, and bubbles that percolated with physics. However, with a nod to the 400 ladies in white, each frame of these computer-generated background scenes was printed out on fine painting paper and hand-colored to match the rest of the movie.
Beauty and the Beast was Disney's first movie to use "paperless animation," 
at least in one scene. The ballroom dance at the end of the film was composed and rendered digitally, except for the hand-drawn characters of the Beast and Belle. The shift in the movie between the real cartoon and the faked cartoon was just slightly noticeable to my eye. The discontinuity protruded not because it was less graceful than the hand animation, but because it was better-because it looked more photographic than the cartoon.
The first Disney character to be completely paperless was the flying 
(walking, pointing, jumping) carpet in Aladdin. To make it, the form of a Persian carpet was rendered on a computer screen. The animator bent it into its poses by moving a cursor, and then the computer filled out the "between" frames. The digitized carpet action was then added into the digitized version of the rest of the hand-drawn movie. Lion King, Disney's latest animation, has several animals that are computer-generated in the manner of the Jurassic dinosaurs, including some animals with semi-autonomous herding and flocking behaviors. Disney is now working on their first completely digital animation, to be released in late 1994. It will feature the work of an ex-Disney animator, John Lassiter. Almost the entire computer animation will be done at Pixar, a small innovative studio located in a remodeled business park in Richmond Point, California.
I stopped by Pixar to see what kind of artificial life they were hatching. Pixar 
has made four award-winning short computer animations done by Lassiter. Lassiter likes to animate normally inanimate objects-a bicycle, a toy, a lamp, or knick-knacks on a shelf. Although Pixar films are considered state-of-the-art computer animations in computer graphic circles, the animation part is mostly handcrafted. Instead of drawing with a pencil, Lassiter uses a cursor to modify his computer-rendered 3-D objects. If he wants his toy soldier character to be depressed he goes into his figure's happy face on the computer screen and drags the toon's mouth into a droop. After testing the expression he may decide the toy soldier's eyebrows really shouldn't droop so fast, or maybe its eyes bat too slowly. So by cursor-dragging he alters the computer form. "I don't know how else to tell it what to do, such as making its mouth like this," says Lassiter, forming an O with his mouth in mock surprise, "that would be any faster or better than doing it myself."
I hear more of this communication problem from Ralph Guggenheim, 
production director at Pixar: "Most hand animators believe that what Pixar does is feed scripts into a computer and out comes a film. That's why we were once barred from animation festivals. But if we were to really do that, we could not create great stories....The chief day-to-day problem we have at Pixar is that computer animation reverses the animation process. It asks animators to describe before they animate what it is they want to animate!"
Animators, true artists, are like writers in that they don't know what they want to say until they hear themselves say it. Guggenheim reiterates, 
"Animators can't know a character until they animate it. They will tell you that it is very slow going in the beginning of a story because they are becoming familiar with their character. Then it starts speeding up as they become more intimate with it. As they get to the halfway point of the film, now they know the character well and they are screaming through the frames." 
In the short animation Tin Toy, a plume on the toy soldier's hat shakes 
naturally when he bobs his head. That effect was achieved with virtual physics, or what the animators call "lag, drag, and wiggle." When the base of the plume moved, the rest of the feather acted as if it were a spring pendulum-a fairly standard physics equation. The exact way the plume quivered was unpredicted and quite realistic because the plume was obeying the physics of shaking. But the face of the toy soldier was still manipulated entirely by an experienced human animator. The animator is a surrogate actor. He acts out a character by drawing it. Every animator's desk has a mirror on it that the animator uses to draw his own exaggerated facial expressions.
I asked the artists at Pixar if they can at least imagine an autonomous 
computer character-you feed in a rough script and out comes a digital Daffy Duck doing his mischief. There was uniform grave denial and shaking of heads. "If animating a believable character was as easy as feeding a script into a computer, then there would be no bad actors in the world," said Guggenheim. "But we know that not all actors are great. You see tons of Elvis or Marilyn Monroe impersonators all the time. Why aren't we fooled? Because the impersonator has a complex job knowing when to twitch the right side of his mouth or how to hold a microphone. If a human actor has difficulty doing that, how will a computer script do it?"
The question they are asking is one of control. It turns out that the special 
effects and animation business is an industry of control freaks. They feel that the subtleties of acting are so minute that only a human overseer can channel the choices of a digital or drawn character. They are right.
But tomorrow, they won't be. If computer power continues to increase as it 
has, within five years we'll see a character created by releasing synthetic behavior into a synthetic body star in a film. 
The Jurassic Park dinos made it very clear how nearly perfect synthetic body 
representations are today. The flesh of the dinos was visually indistinguishable from what we'd expect a filmed dinosaur to be. A number of digital effects laboratories are compiling the components of a believable digital human actor right now. One lab specializes in creating perfect digital human hair, another concentrates on getting the hands right, and another on generating facial expressions. Already, digital characters are inserted into Hollywood films (without anyone noticing) when a synthetic scene demands people moving in the distance. Realistic clothing that drapes and folds naturally is still a challenge; done imperfectly it gives the virtual person a 
clunky feel. But at the start, digital characters will be used for dangerous stunts, or worked into composite scenes-but only in long shots, or in crowds, rather than in the full attention of a close-up. An entirely convincing virtual human form is tricky, but close at hand. 
What is not very close at hand is simulating convincing human action. 
Especially out of reach is convincing facial behavior. The final frontier, the graphics experts say, is the human expression. A quest for control of a human face is now a minor crusade.

At Colossal Picture Studios in the industrial outskirts of San Francisco, 
Brad de Graf works on faking human behavior. Colossal is the little-known 
special effects studio behind some of the most famous animated commercials on TV such as the Pillsbury Doughboy. Colossal also did the avant garde animation series for MTV called Liquid TV, starring animated stick figures, low-life muppets on motorbikes, animated paper cutouts, and the bad boys Beavis and Butt-head.
De Graf works in a cramped studio in a redecorated warehouse. In several 
large rooms under dimmed lights about two dozen large computer monitors glow. This is an animation studio of the '90s. The computers-heavy-duty graphic workstations from Silicon Graphics-are lit with projects in various stages, including a completely computerized bust of rock star Peter Gabriel. Gabriel's head shape and face were scanned, digitized, and reassembled into a virtual Gabriel that can substitute for his live body in his music videos. Why waste time dancing in front of cameras when you could be in a recording studio or in the pool? I watched an animator fiddle with the virtual star. She was trying to close Gabriel's mouth by dragging a cursor to lift his jaw. "Ooops" she said, as she went too far and Gabriel's lower lip sailed up and penetrated his nose, making a disgusting grimace.
I was at de Graf's workshop to see Moxy, the first completely computer-
animated character. On the screen Moxy looks like cartoon dog. He's got a big nose, a chewed ear, two white gloves for hands, and "rubber hose" arms. He's also got a great comic voice. His actions are not drawn. They are lifted from a human actor. There's a homemade virtual reality "waldo" in one corner of the room. A waldo (named from a character in an old science-fiction story) is a device that lets a person drive a puppet from a distance. The first waldo-driven computer animation was an experimental Kermit the Frog animated by a hand-size muppet waldo. Moxy is a full-bodied virtual character, a virtual puppet.
When an animator wants to have Moxy dance, the animator puts on a yellow 
hardhat with a stick taped to the peak. At the end of the stick is a location sensor. The animator straps on shoulder and hip sensors, and then picks up two foam-board pieces cut out in the shape of very large cartoon hand-gloves. He waves these around-they also have location sensors on them-as he dances. On the screen Moxy the cartoon dog in his funky toon room dances in unison.
Moxy's best trick is that he can lip sync automatically. A recorded human 
voice pours into an algorithm which figures out how Moxy's lips should move, and then moves them. The studio hackers like to have Moxy saying all kinds of outrageous things in other people's voices. In fact, Moxy can be moved in many ways. He can be moved by twirling dials, typing commands, moving a cursor, or even by autonomous behavior generated by algorithms.
That's the next step for de Graf and other animators: to imbue characters 
like Moxy with elementary moves-standing up, bending over, lifting a heavy object-which can be recombined into smooth believable action. And then to apply that to a complex human figure.
To calculate the move of a human figure is marginally possible for today's 
computers given enough time. But done on the fly, as your body does in a real life, in a world that shifts while you are figuring where to put your foot, this calculation becomes nearly impossible to simulate well. The human figure has about 200 moving joints. The total number of possible positions a human figure can assume from 200 moving parts is astronomical. To simply pick your nose in real time demands more computational power than we have in large computers.
But the complexity doesn't stop there because each pose of the body can be 
reached by a multitude of pathways. When I raise my foot to slip into a pair of shoes, I steer my leg through that exact pose by hundreds of combinations of thigh, leg, foot, and toe actions. In fact, the sequences that my limbs take while walking are so complex that there is enough room for a million differences in doing so. Others can identify me-often from a hundred feet away and not seeing my face-entirely by my unconscious choice of which feet muscles I engage when I walk. Faking someone else's combination is hard.
Researchers who try to simulate human movement in artificial figures 
quickly discover what animators of Bugs Bunny and Porky Pig have known all along: that some linkage sequences are more "natural" than others. When Bugs reaches for a carrot, some arm routes to the vegetable appear more human than other routes. (Bugs's behavior, of course, does not simulate a rabbit but a person.) And much depends on the sequential timing of parts. An animated figure following a legitimate sequence of human movements can still appear robotic if the relative speeds of, say, swinging upper arm to striding leg are off. The human brain detects such counterfeits easily. Timing, therefore, is yet another complexifying aspect of motion.
Early attempts to create artificial movement forced engineers far afield into 
the study of animal behavior. To construct legged vehicles that could roam Mars, researchers studied insects, not to learn how to build legs, but to figure out how insects coordinated six legs in real time.At the corporate labs of Apple Computer, I watched a computer graphic 
specialist endlessly replay a video of a walking cat to deconstruct its movements. The video tape, together with a pile of scientific papers on the reflexes of cat limbs, were helping him extract the architecture of cat walking. Eventually he planned to transplant that architecture into a computerized virtual cat. Ultimately he hoped to extract a generic four-footed locomotion pattern that could be adjusted for a dog, cheetah, lion, or whatever. He was not concerned at all with the look of the animal; his model was a stick figure. He was concerned with organization of the complicated leg, ankle, and foot actions.
In David Zeltzer's lab at MIT's Media Lab, graduate students developed 
simple stick figures which could walk across an uneven landscape "on their own." The animals were nothing more than four legs on a stick backbone, each leg hinged in the middle. The students would aim the "animat" in a certain direction, then it would move its legs upon figuring out where the low or high spots were, adjusting its stride to compensate. The effect was a remarkably convincing portrait of a critter walking across rugged terrain. But unlike an ordinary Road Runner animation, no human decided where each leg had to go at every moment of the picture. The character itself, in a sense, decided. Zeltzer's group eventually populated their world with autonomous six-legged animats, and even got a two-legged thing to ramble down a valley and back.
Zeltzer's students put together Lemonhead, a cartoony figure that could 
walk on his own. His walking was more realistic and more complicated than the sticks because he relied on more body parts and joints. He could skirt around obstacles such as fallen tree trunks with realistic motion. Lemonhead inspired Steve Strassman, another student in Zeltzer's lab, to see how far he could get in devising a library of behavior. The idea was to make a generic character like Lemonhead and give him access to a "clip book" of behaviors and gestures. Need a sneeze? Here's a disk-full. 
Strassman wanted to instruct a character in plain English. You simply tell it 
what to do, and the figure retrieves the appropriate behaviors from the "four food groups of behavior" and combines them in the right sequence for sensible action. If you tell it to stand up, it knows it has to move its feet from under the chair first. "Look," Strassman warns me before his demo begins, "this guy won't compose any sonatas, but he will sit in a chair."
Strassman fired up two characters, John and Mary. Everything happened in 
a simple room viewed from an oblique angle above the ceiling-a sort of god's-eye view. "Desktop theater," Strassman called it. The setting, he said, was that the couple occasionally had arguments. Strassman worked on their goodbye scene. He typed: "In this scene, John gets angry. He offers the book to Mary rudely, but she refuses it. He slams it down on the table. Mary rises while John glares." Then he hits the PLAY key.The computer thinks about it for a second, and then the characters on the 
screen act out the play. John frowns; his actions with the book are curt; he clenches his fists. Mary stands up suddenly. The end. There's no grace, nothing very human about their movements. And it's hard to catch the fleeting gestures because they don't call attention to their motions. One does not feel involved, but there, in that tiny artificial room, are characters interacting according to a god's script.
"I'm a couch-potato director," Strassman says. "If I don't like the way the 
scene went I'll have them redo it." So he types in an alternative: "In this scene, John gets sad. He's holding the book in his left hand. He offers it to Mary kindly, but she refuses it politely." Again, the characters play out the scene.
Subtlety is the difficult part. "We pick up a phone differently than a dead 
rat," Strassman said. "I can stock up on different hand motions, but the tricky thing is what manages them? Where does the bureaucracy that controls these choices get invented?"
Taking what they learned from the stick figures and Lemonhead, Zeltzer and 
colleague Michael McKenna fleshed out the skeleton of one six-legged animat into a villainous chrome cockroach and made the insect a star in one of the strangest computer animations ever made. Facetiously entitled "Grinning Evil Death," the token plot of the five-minute video was the story of how a giant metallic bug from outer space invaded Earth and destroyed a city. While the story was a yawner, the star, a six-legged menace, was the first animat-an internally driven artificial animal. 
When the humongous chrome cockroach crawled down the street, its 
behavior was "free." The programmers told it, "walk over those buildings," and the virtual cockroach in the computer figured out how its legs should go and what angle its torso should be and then it painted a plausible video portrait of itself wriggling up and over five-story brick buildings. The programmers aimed its movements rather than dictated them. Coming down off the buildings, an artificial gravity pulled the giant robotic cockroach to the ground. As it fell, the simulated gravity and simulated surface friction made its legs bounce and slip realistically. The cockroach acted out the scene without its directors being drowned in the minutiae of its foot movements.
The next step toward birthing an autonomous virtual character is now in 
trial: Take the bottom-up behavioral engine of the giant cockroach and surround it with the glamorous carcass of a Jurassic dino to get a digital film actor. Wind the actor up, feed it lots of computer cycles, and then direct it as you would a real actor. Give it general instructions-"Go find food"-and it will, on its own, figure out how to coordinate its limbs to do so.
Building the dream, of course, is not that easy. Locomotion is merely one facet of action. Simulated creatures must not only move, they must 
navigate, express emotion, react. In order to invent a creature that could do more than walk, animators (and roboticists) need some way to cultivate indigenous behaviors of all types.

In the 1940's, a trio of legendary animal watchers in Europe-Konrad 
Lorenz, Karl von Frisch, and Niko Tinbergen-began describing the logical 
underpinnings of animal behavior. Lorenz shared his house with geese, von Frisch lived among honeybee hives, and Tinbergen spent his days with stickleback perch and sea gulls. By rigorous and clever experiments the three ethologists refined the lore of animal antics into a respectable science called ethology (roughly, the study of character). In 1973, they shared a Nobel prize for their pioneering achievements. When cartoonists, engineers, and computer scientists later delved into the literature of ethology, they found, much to their surprise, a remarkable behavioral framework already worked out by the three ethologists, ready to be ported over to computers.
At the core of ethological architecture dwells the crucial idea of 
decentralization. As formalized in 1951 by Tinbergen in his book The Study of Instinct, the behavior of an animal is a decentralized coordination of independent action (drive) centers which are combined like behavioral building blocks. Some behavioral modules consist of a reflex; they invoke a simple function, such as: pull away when hot, or blink when touched. The reflex knows nothing of where it is, what else is going on, or even of the current goal of its host body. It can be triggered anytime the right stimulus appears. 
A male trout instinctually responds to the following stimuli: a female trout 
ripe for copulation, a nearby worm, a predator approaching from behind. But when all three stimuli are presented simultaneously, the predator module always wins out by suppressing feeding or mating instincts. Sometimes, when there is a conflict between action modules, or several simultaneous stimuli, management modules are triggered to decide. For instance, you are in the kitchen with messy hands when the phone rings at the same time someone knocks on the front door. The conflicting drives-jump to the phone! no, wipe hands first! no, dash to the door!-could lead to paralysis unless arbitrated by a third module of learned behavior, perhaps one that invokes the holler, "Please wait!"
A less passive way to view a Tinbergen drive center is as an "agent." An 
agent (whatever physical form it takes) detects a stimuli, then reacts. Its reaction, or "output" in computer talk, may be considered input by other modules, drive centers, or agents. Output from one agent may enable other modules (cocking a gun's hammer) or it may activate other modules already 
enabled (pulling the trigger). Or the signal may disable (uncock) a neighboring module. Rubbing your tummy and patting your head at the same time is tricky because, for some unknown reason, one action suppresses the other. Commonly an output may both enable some centers and suppress others. This is, of course, the layout of a network swamped with circular causality and primed to loop into self-creation.
Outward behavior thus emerges from the thicket of these blind reflexes. 
Because of behavior's distributed origin, very simple agents at the bottom can produce unexpectedly complex behavior at the top. No central module in the cat decides whether the cat should scratch its ear or lick its paw. Instead, the cat's conduct is determined by a tangled web of independent "behavioral agents"-cat reflexes-cross-activating each other, forming a gross pattern (called licking or scratching) that wells up from the distributed net. 
This sounds a lot like Brooks's subsumption architecture because it is. 
Animals are robots that work. The decentralized, distributed control that governs animals is also what works in robots and what works for digital creatures. 
Web-strewn diagrams of interlinked behavior modules in ethology textbooks 
appear to computer scientists as computer logic flow charts. The message is: Behavior is computerizable. By arranging a circuit of subbehaviors, any kind of personality can be programmed. It is theoretically feasible to generate in a computer any mood, any sophisticated emotional response that an animal has. Film creatures will be driven by the same bottom-up governance of behavior running Robbie the Robot-and the very same scheme borrowed from living songbirds and stickleback fish. But instead of causing pneumatic hoses to pressurize, or fishtails to flick, the distributed system pumps bits of data which move a leg on a computer screen. In this way, autonomous animated characters in film behave according to the same general organizational rules as real animals. Their behavior, although synthetic, is real behavior (or at least hyperreal behavior). Thus, toons are simply robots without hard bodies. 
More than just movement can be programmed. Character-in the old-
fashioned sense of the word-can be encapsulated into bit code. Depression, elation, and rage will all be add-on modules for a creature's operating system. Some software companies will sell better versions of the fear emotion than others. Maybe they'll sell "relational fear"-fear that not only registers on a creature's body but trickles into successive emotion modules and only gradually dissipates over time.
continue...   
 
Out of Control
Behavior wants to be free, but to be of any use to humans, artificially 
generated behavior needs to be supervised or controlled. We want Robbie 
the Robot, or Bugs Bunny, to accomplish things on his own without our oversight. At the same time, not everything Robbie or Bugs could do would be productive. How can we give a robot, or a robot without a hard body, or any artificial life, the license to behave, while still directing them to be useful to us?
Some answers are unexpectedly being uncovered in a research project on 
interactive literature begun at Carnegie Mellon University. There researcher Joseph Bates fabricated a world called "Oz," somewhat similar to the tiny room of John and Mary that Steve Strassman created. In Oz there are characters, a physical environment, and a narrative-the same trio of ingredients for classical drama. In traditional drama, the narrative dictates both characters and environment. In Oz, however, the control is inverted somewhat; characters and environment influence the narrative.
Oz is made for humans to enjoy. It is a fanciful virtual world populated with 
automatons as well as human-directed characters. The goal is to create an environment, a narrative structure, and automatons in such a way that a human can participate in the story without either crashing the story line, or feeling left out as a mere observer in the audience. David Zeltzer, who lent some ideas to the project, gives a wonderful example: "If we provided you with a digitized version of Moby Dick, there's no reason why you couldn't have your own cabin on the Pequod. You could talk to Starbuck as he went after the White Whale. There is enough room in the narrative for you to be involved, without changing the plot."
There are three frontiers of control research involved in Oz: 
●     How do you organize a narrative to allow deviations yet keep it 
centered on its intended destination? 
●     How do you construct an environment that can generate surprise events? 
●     How do you create creatures that have autonomy, but not too much?From Strassman's "desktop theater" we go to Joseph Bates's "computational 
drama." Bates envisions a drama of distributed control. A story becomes a type of coevolution, with perhaps only its outer boundaries predestined. You could be in an episode of Star Trek attempting to influence alternative storylines, or you could be on a journey with a synthetic Don Quixote confronting new fantasies. Bates, who is chiefly concerned about the experience of the human user of Oz, puts his quest this way. "The question I'm working on is: How do you impose a destiny upon a user without removing their freedom?" 
In my search for the future of control from the perspective of the created 
rather than creator, I will rephrase his question as: How do you impose a destiny upon a character of artificial life without removing its freedom?
Brad de Graf believes this shift in control is shifting the goal of authors. "It's 
a different medium we are making. Instead of creating a story, I'm creating a world. Instead of creating a character's dialogue and action, I'm creating a personality." 
When I had a chance to play with some artificial characters Bates developed, 
I got a sense of how much fun such personality petlike creatures could be. Bates calls his pets "woggles." Woggles come in three varieties: a blue blob, a red blob, a yellow blob. The blobs are stretchy spheres with two eyes. They hop around in a simple world of stepping-stones and some caves. Each color of woggle is coded with a different suite of behaviors. One is shy, one is aggressive, one is a follower. When a woggle frightens another woggle, the aggressive one stretches tall to scare away the threat. The shy one shrinks and flees.
Ordinarily the woggles hop around doing their woggly thing among 
themselves. But when a human enters their world by inserting a cursor into their space, they interact with the visitor. They may follow you around, or avoid you, or wait until you aren't around to harass another woggle. You are in the picture, but you are not controlling the show.
I got a better sense of the future of pet control from a prototype world that 
is somewhat an extension of Bates's woggle world. A virtual reality (VR) group at Fujitsu Laboratories in Japan took wogglelike characters and fleshed them out in virtual three dimensions. I watched a guy wearing a clunky VR helmet on his head and data gloves on his hands give a demonstration. 
He was in a fantasy underwater world. A faint impression of a submerged 
castle shimmered in the distant background. A few old Greek columns and chest-high seaweed furnished the immediate play area. Three "jellyfish" hopped around, and one small sharkish fish circled the area. The jellyfish, in the shape of mushrooms and about the size of dogs, changed color depending on their mood or behavior state. Playing by themselves the three were blue. They would hop around on their fat monopod tirelessly. If the VR-
guy beckoned them to come, by waving with his hand, they would excitedly bounce over, turn orange, and jump up and down like friendly dogs waiting to chase a stick. When he showed them attention their eyes would close in a happy expression. The guy could call in the less friendly fish by emitting a blue laser line from his forefinger and touch the fish from afar. This would change the fish's color and interest in humans, so it circled in much closer, and swam nearby-but like a cat, not too close-as long as it was occasionally touched by the blue line.
Even watching from the outside, it was evident that artificial characters with 
the mildest autonomous behavior and some three-dimensional form in a shared three-dimensional space had a distinct presence of their own. I could imagine having an adventure with them. I could imagine them as Jurassic dinosaurs and me really being scared. Even the Fujitsu guy ducked once when the virtual fish swam too close to his head. "Virtual reality," says de Graf, "is not going to be interesting unless it is populated with interesting characters."
Pattie Maes, an artificial life researcher at the MIT Media Lab, abhors goggle-and-gloves virtual reality. She finds such clothing "too artificial" and confining. She and colleague Sandy Pentland came up with an alternative way to interact with virtual creatures. Her system, called ALIVE, lets a human play with animated creatures via a computer screen and video camera. The camera points back at the human participant, inserting the observer into the virtual world that he or she is watching on the screen.
This neat trick gives a real sense of intimacy. By moving my arms I can interact with little "hamsters" on the screen. The hamsters look like tiny toasters on wheels, but they are autonomous goal-seeking animats that contain a rich repertoire of motivations, sensors, and responses. The hamsters roam the enclosed pen looking for "food" when they haven't eaten in a while. They seek each other's company; sometimes they chase 
each other. They run from my hand if I move it too fast. If I move it slowly, they try to follow it out of curiosity. A hamster will sit up and beg for food. When they get tired, they fall over and sleep. They are halfway between robots and animated animals, and only several steps away from authentic virtual characters.
Pattie Maes is trying to teach creatures "how to do the right thing." She 
wants her creatures to learn from their experiences in the environment, without much human supervision. The Jurassic dinosaurs won't be real characters until they can learn. It will be hardly worth creating a humanist virtual actor unless he or she could learn. Following the subsumption architecture model, Maes is structuring a hierarchy of algorithms that let her creatures not only adapt, but also bootstrap themselves to increasing complex behaviors and-as an essential part of the package-also let their own goals emerge from those behaviors
The animators at Disney and Pixar nearly croak at the thought, but someday 
Mickey Mouse will have his own agenda.

It's the winter of 2001, in a corner of the Disney studio lot; a trailer is set 
up as a top-secret research lab. Reels of old Disney cartoons, stacks of 
gigabyte computer hard drives, and three 24-year-old-computer graphic artists hole up inside. In about three months they deconstruct Mickey Mouse. He is reanimated as a potentially 3-D being who only appears in two dimensions. He knows how to walk, leap, dance, show surprise and wave goodbye on his own. He can lip sync but can't talk. The entire overhauled Mickey fits onto one Syquest 2-gig portable disk.
The disk is walked over to the old animation studio, past its rows of empty 
and dusty animation stands, to the cubicles where the Silicon Graphics workstations are glowing. Mickey is popped into a computer. The animators have already created a fully detailed artificial world for the Mouse. He's cued up to the scene and the tape turned on. Roll! When Mickey trips on the stairs of his house, gravity hauls him down. The simulated physics of his rubbery rear end bouncing against the wooden stairs generates realistic hops. His cap is blown away by a virtual wind from the open front door, and when the carpet slides out from under him as he attempts to run after his hat, it bunches up in accordance with the physics of fabric, just as Mickey collapses under his own simulated weight. The only instruction Mickey got was to enter the room and be sure to chase his hat. The rest came naturally.
After 1997, nobody ever draws Mickey again. There's no need to. Oh, 
sometimes the animators butt in and touch up a critical facial expression here or there-mere make-up artists the handlers call them-but by and large Mickey is given a script and he obeys. And he-or one of his clones-works all year round on more than one film at once. Never complains, of course. 
The graphic jocks aren't satisfied. They hook up a Maes learning module into 
Mickey's code. With this on, Mickey matures as an actor. He responds to the emotions and actions of the other great actors in his scenes-Donald Duck and Goofy. Every time a scene is rerun, he remembers what he did on the keeper take and that gesture is emphasized next time. He evolves from the outside as well. The programmers tune up his code, give him improved smoothness, increase the range of his expressions, and beef up the depth of his emotions. He can play the "sensitive guy" now if needed.
But, over five years of learning, Mickey begins to get his own ideas. He somehow reacts hostilely to Donald, and becomes furious when he gets 
clunked on the head with a mallet. And when he is angry, he becomes obstinate. He balks when the director instructs him to walk off the edge of a cliff, having learned over the years to avoid obstacles and edges. Mickey's programmers complain that they can't code around these idiosyncrasies without disrupting all the other finely tuned traits and skills Mickey has acquired. "It's like an ecology," they say. "You can't remove one thing without disturbing them all." One graphic jock puts it best: "Actually, it's like a psychology. The Mouse has a real personality. You can't separate it. You've just got to work around it."
So by 2007, Mickey Mouse is quite an actor. He is a hot "property" as the 
agents say. He can speak. He can handle any kind of slapstick situation you can imagine. Does his own stunts. He has a great sense of humor, and the fabulous timing of a comedian. The only problem is that he is an SOB to work with. He'll suddenly fly off the handle and go berserk. Directors hate him. But they put up with him-they've seen worse-because, well, because he's Mickey Mouse.
Best of all, he'll never die, never age. 
Disney foreshadowed this liberation of toons in its own film Roger Rabbit. 
Toons in this movie have their own independent life and dreams, but they have to stay in Toon Town, their own virtual world, except when we need them to work in our films. On the set, toons may or may not be cooperative and pleasant. They have the same whims and tantrums that human actors have. Roger Rabbit is just fiction, but someday Disney will have to deal with an autonomous out-of-control Roger Rabbit.
Control is the issue. In his first film, Steamboat Willie, Mickey was under the 
full control of Walt Disney. Disney and the Mouse were one. As more lifelike behaviors are implanted into Mickey, he is less at one with his creators and more out of their control. This is old news to anyone with kids or pets. But it is new news to anyone with a cartoon character, or machines that get smarter. Of course, neither kids nor pets are completely out of our control. There is the direct authority we have in their obedience, and the larger indirect control we have in their training and formation.
The fairest way to state this is that control is a spectrum. At one end there is 
the total domination of "as one" control. At the other is "out of control." In between are varieties of control we don't have words for. 
Until recently, all our artifacts, all our own handmade creations have been 
under our authority. But as we cultivate synthetic life in our artifacts, we cultivate the loss of our command. "Out of control," to be honest, is a great exaggeration of the state that our enlivened machines will take. They will remain indirectly under our influence and guidance but free of our domination.Though I have searched everywhere, I could not find the word that 
describes this type of clout. We simply have no name for the loose relationship between an influential creator and a creation with a mind of its own-a thing we shall see more of. The realm of parent and child should have such a word, but sadly doesn't. We do better with sheep where we have the notion of "shepherding." When we herd a flock of sheep, we know we are not in complete authority, yet neither are we without control. Perhaps we will shepherd artificial lives. 
We also "husband" plants, as we assist them in their natural goals, or deflect 
them slightly for our own. "Manage" is probably the closest in meaning to the general type of control we will need for artificial lives, such as a virtual Mickey Mouse. A women can "manage" her difficult child, or a barking dog, or the 300-strong sales department under her authority. Disney can manage Mickey in films.
"Manage" is close, but not perfect. Although we manage wilderness areas 
like the Everglades, we actually have little say in what goes on among the seaweed, snakes and marsh grass. Although we manage the national economy, it does what it wants. And although we manage a telephone network, we have no supervision on how a particular call is completed. The word "management" may imply more oversight then we really have in the examples above, and more than we will have in future very complex systems.

The word I'm looking for is more like "co-control." It's seen in some 
mechanical settings already. Keeping a 747 Jumbo Jet aloft and landing it in 
bad weather is a very complex task. Because of the hundreds of systems running simultaneously, the immediate reaction time required by the speed of the plane, and disorienting effects of sleepless long trips and hazardous weather, a computer can fly a jet better a human pilot. The sheer number of human lives at stake permits no room for errors or second best. Why not have a very smart machine control the jet?
So engineers wired together an autopilot, and it turns out be very capable. 
It flies and lands a Jumbo Jet oh so nicely. Flying-by-wire also fits very handily into the craving for order by the air traffic controllers-everything is under digital control. The original idea was that human pilots would monitor the computer in case anything went wrong. The only problem is that humans are terrible at passive monitoring. They get bored. They daydream. Then they start missing critical details. Then an emergency pops up which they have to tackle cold.
So instead of having the pilot watch the computer, the new idea was to 
invert the relationship and have the computer watch the pilot. This approach was taken in the European Airbus A320, one of the most highly automated planes built to date. Introduced in 1988, the onboard computer supervises the pilot. When he pushes the control stick to turn the plane, the computer figures out how far to bank left or right, but it won't let the plane bank more than 67 degrees or nose up or down more than 30 degrees. This means, in the words of Scientific American, "the software spins an electronic cocoon that stops the aircraft from exceeding its structural limitations." It also means, pilots complain, that the pilot surrenders control. In 1989 British Airways pilots flying 747s experienced six different incidents where they had to override a computer-initiated power reduction. Had they not been able to override the erroneous automatic pilot-which Boeing blamed on a software bug-the error could have been fatal. The Airbus A320, however, provides no override of its autosystem.
Human pilots felt they were fighting for control of the plane. Should the 
computer be a pilot or navigator? The pilots joked that the computer was like putting a dog into the cockpit. The dog's job was to bite the pilot if he tries to touch the controls; and the pilot's only job was to feed the dog. In fact, in the emerging lingo of automated flying, pilots are called "system 
managers."
I'm pretty sure the computer will end up as co-pilot. There will be much that 
it does completely out of the reach of the pilot. But the pilot will manage, or shepherd, the computer's behavior. And the two-machine and human-will be in a constant tussle, as are all autonomous things. Planes will fly by co-control.
A graphic jock at Apple, Peter Litwinowicz, fabricated a great hack. He 
extracted the body and facial movements from a live human actor and applied them to digital actors. He had a human performer ask, in a sort of theatrical way, for a dry martini. He took those gestures-the raised eyebrow, the smirk on the lips, the lilt of the head-to control the face of a cat. The cat delivered the line in exactly the same manner as the actor would. As an encore Litwinowicz then mapped the actor's expressions onto a cartoon, and then onto an inert classical mask, and finally, he animated a tree trunk with the actor's facial controls. Human actors will not be out of jobs. While some characters will be wholly autonomous, most will be of a cyborgian nature. An actor will animate a cat, while the artificial cat pushes back and helps the actor be a better cat. An actor can "ride" a cartoon, in the same type of cocontrol that a cowboy rides a horse, or a pilot rides a computer-steered airplane. The green figure of a digital Ninja Turtle may dart about the world on its own, but the human actor sharing control supplies the appropriate nuance every now and then in a smile, or finishes a just-perfect growl with a jeer. 
James Cameron, the director of Terminator 2, recently told an audience of 
computer graphic specialists, "Actors love masks. They're willing to sit in makeup chairs for eight hours to put them on. We must make them partners in synthetic character creation. They will be given new bodies and new faces with which to expand their art."
The future of control: Partnership, Co-control, Cyborgian control. What it all 
means is that the creator must share control, and his destiny, with his creations.