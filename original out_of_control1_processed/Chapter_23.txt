WHOLES, HOLES, AND SPACES 
Good morning, self-organizing systems!" 
The cheerful speaker smiled with a polished ease and adjusted his tie. "I am 
indeed very happy to find the Office of Naval Research joining with the Armour Research Foundation in organizing this conference on what I personally consider an exceedingly important topic, and at such a well-chosen time."
It was a spring day in early May, 1959. Four hundred men from an 
astoundingly diverse group of scientific backgrounds had gathered in Chicago for what promised to be an electrifying meeting. Almost every major branch of science was represented: psychology, linguistics, engineering, embryology, physics, information theory, mathematics, astronomy, and social sciences. No one could remember a conference before this where so many top scientists in different fields were about to spend two days talking about one thing. Certainly there had never been a large meeting about this particular one thing.
It was a topic that only a young country flush with success and confident of 
its role in the world would even think about: self-organizing systems-how organization bootstraps itself to life. Bootstrapping! It was the American dream put into an equation.
"The choice of time is particularly significant in my personal life, too," the 
speaker continued. "For the last nine months the Department of Defense of the United States of America has been in the throes of an organizational effort which shows reasonably clearly that we are still a long way from understanding what makes a self-organizing system."
Hearty chuckles from the early morning crowd just settling into their seats. 
At the podium Dr. Joachim Weyl, Research Director of the Office of Naval Research, beamed and continued. "There are three basic elements I'd like to call to your attention which can be studied best. From the area of computers we will, in the long run, draw our essential understanding of the element of memory that is absolutely and inevitably present in what you might call in the future 'self-organizing systems.' You might go so far, as I have done, as to say that a computer is nothing but a means for a memory to get from one state to another. "The second element biologists call differentiation. In any system that will 
evolve it is quite clearly necessary that you have what the geneticists have called mutations, essentially random events. Some initial triggering mechanism is needed to push one group in one direction, and another in another direction. In other words, environment containing noise has to be relied on to furnish the triggering mechanism on which the long-term selection rule will operate.
"The third basic element probably presents itself most purely and most 
accessibly when we are dealing with large social organizations. Let me call it, for the purpose here, subordination, or if you wish, the executive function."
There they were: signal noise, mutations, executive function, self-
organization. These words were spoken before the arrival of the DNA model, before digital technology, before departments of information management systems, and before complexity theory. It is difficult to imagine how alien and innovative these ideas were at the time.
And how right. In one fell swoop 35 years ago, Dr. Weyl outlined my whole 
1994 book on the breaking science of adaptive, distributed systems and the emergent phenomenon they engender.
While the prescience of the 1959 meeting is remarkable, I also see 
something remarkable on the other side: how little our knowledge of whole systems has advanced in 35 years. Despite the great progress made recently and reported in this book, many of the basic questions about self-organization, differentiation, and subordination of whole systems still remain mysterious.
The all-star lineup who presented papers at the 1959 conference was a 
public rendezvous of scientists who had been convening in smaller meetings since 1942. These intimate, invitation-only gatherings were organized by the Josiah Macy, Jr. Foundation, and became known as the Macy Conferences. In the spirit of wartime urgency, the small gatherings were interdisciplinary, elite, and emphasized thinking big. Among the several dozen visionaries invited over the nine years of the conference were Gregory Bateson, Norbert Wiener, Margaret Mead, Lawrence Frank, John von Neumann, Warren McCulloch, and Arturo Rosenblueth. This stellar congregation later became known as the cybernetic group for the perspective they pioneered-cybernetics, the art and science of control.
Some beginnings are inconspicuous; this one wasn't. From the very first 
Macy Conference, the participants could imagine the alien vista they were opening. Despite their veteran science background and natural skepticism, they saw immediately that this new view would change their life's work. Anthropologist Margaret Mead recalled she was so excited by the ideas set loose in the first meeting that "I did not notice that I had broken one of my teeth until the Conference was over." 
The core group consisted of key thinkers in biology, social science, and what 
we would now call computer science, although this group were only beginning to invent the concept of computers at the time. Their chief achievement was to articulate a language of control and design that worked for biology, social sciences, and computers. Much of the brilliance of these conferences came by the then unconventional approach of rigorously considering living things as machines and machines as living things. Von Neumann quantitatively compared the speed of brain neurons and the speed of vacuum tubes, boldly implying the two could be compared. Wiener reviewed the history of machine automata segueing into human anatomy. Rosenblueth, the doctor, saw homeostatic circuits in the body and in cells. In Steve Heims's history of this influential circle of minds, The Cybernetics Group, he says of the Macy Conferences: "Even such anthropocentric social scientists as Mead and Frank became proponents for the mechanical level of understanding, wherein life is described as an entropy-reducing device and humans characterized as servomechanisms, their minds as computers, and social conflicts by mathematical game theory." 
In an age when popular science fiction had just hatched, and was not the 
influential element it now is in modern science, the Macy Conference participants often pushed the metaphors they were playing with to extremes, much as science fiction writers do now. At one conference McCulloch said, "I don't particularly like people, never have. Man to my mind is about the nastiest, most destructive of all the animals. I don't see any reason, if he can evolve machines that can have more fun that he himself can, why they shouldn't take over, enslave us, quite happily. They might have a lot more fun, invent better games than we ever did." Humanists were horrified by such speculations, but under this nightmarish, dehumanized scenario some very important concepts were buried: that machines might evolve, that they might really be able to do practical intellectual chores better than we could, and that we share operating principles with very sophisticated machines. These are very much metaphors of the next millennium.
As Mead wrote later of the Macy Conferences, "Out of the deliberations of 
this (cybernetics) group came a whole series of fruitful developments of a very high order." Specifically, the ideas of feedback control, circular causality, homeostasis in machines, and political game theory were born there and gradually entered the mainstream until they became elemental, almost cliché, concepts today. 
The cybernetic group did not find answers as much as they prepared an 
agenda for questions. Decades later scientists studying chaos, complexity, artificial life, subsumption architecture, artificial evolution, simulations, ecosystems, and bionic machines would find a framework for their questions in cybernetics. A short-hand synopsis of Out of Control would be to say it is an update on the current state of cybernetic research.But therein lies a curious puzzle. If this book is really about cybernetics, why 
is the word "cybernetics" so absent from it? Where are the earlier practitioners of such cutting-edge science now? Why are the old gurus and their fine ideas not at the center of this natural extension of their work? What ever happened to cybernetics?
It was a mystery that perplexed me when I first started hanging out with the 
young generation of systems pioneers. The better-read were certainly aware of the early cybernetic work, but there was almost no one from a cybernetic background working with them. It was as if there was an entire lost generation, a hole in the transmission of knowledge.
There are three theories about why the cybernetic movement died:
●     Cybernetics was starved to death by the siphoning away of its funding 
to the hot-shot-but stillborn-field of artificial intelligence. It was the failure of AI to produce usefulness that did cybernetics in. AI was just one facet of cybernetics, but while it got most of the government and university money, the rest of cybernetics' vast agenda withered. The grad students fled to AI, so the other fields dried up. Then, AI itself stalled. 
●     Cybernetics was a victim of batch-mode computing. For all its great ideas, cybernetics was mostly talk. The kind of experiments required to test its notions demanded many cycles of a computer, at its full power, in a completely exploratory mode. These were all the wrong things to ask of the priesthood guarding the mainframe. Therefore, very little cybernetic theory ever made it to experiment. When cheap personal computers hit the world, universities were notoriously slow to adopt them. So while high school kids had Apple IIs at home, the universities were still using punch cards. Chris Langton started his first a-life experiments on an Apple II. Doyne Farmer and friends discovered chaos theory by making their own computer. Real-time command of a complete universal computer was what traditional cybernetics needed but never got.
●     Cybernetics was strangled by "putting the observer inside the box." In 1960, Heinz von Foerster made the brilliant suggestion that a refreshing view of social systems could be had by including the observer of the system as part of a larger metasystem. He framed his observation as Second Order Cybernetics, or the system of observing systems. The insight was useful in such fields as family therapy where the therapist had to include him- or herself in a theory of the family they were treating. But "putting the observer into the system" fell into an infinite regress when therapists video-taped patients and then sociologists taped therapists watching the tape of the patients and then taped themselves watching the therapists....By the 1980s the rolls of the American Society of Cybernetics were filled with therapists, sociologists, and political scientists primarily interested in 
the effects of observing systems.
All three reasons conspired so that by the late 1970s cybernetics had died of dry rot. Most of the work in cybernetics was at the level of the book you are now reading: armchair attempts to weave a coherent big picture together. Real researchers were bumping their heads in frustration in AI labs, or working in obscure institutes in Russia, where cybernetics did continue as a branch of mathematics. I don't believe a single formal textbook on cybernetics was ever written in English.

In the fabric of knowledge we call science, there was a rent here, a hole. 
It was filled by young enthusiasts not burdened by wise old men. This gap 
made me wonder about the space of science.
Scientific knowledge is a parallel distributed system. It has no center, no one 
in control. A million heads and dispersed books hold parts of it. It too is a web, a coevolutionary system of fact and theory interacting and influencing other facts and theories. But the study of science as a network of agents searching in parallel over a rugged landscape of mysteries is a field larger than any I've tackled here. To deal fairly with the mechanics of science alone would require a larger book than I've written so far. I can only hint at such a system in these closing pages.
Knowledge, truth, and information flow in networks and swarm systems. I 
have always been interested in the texture of scientific knowledge because it appears to be lumpy and uneven. Much of what we collectively know derives from a few small areas, yet between them lie vast deserts of ignorance. I can interpret that observation now as the effect of positive feedback and attractors. A little bit of knowledge illuminates much around it, and that new illumination feeds on itself, so one corner explodes. The reverse also holds true: ignorance breeds ignorance. Areas where nothing is known, everyone avoids, so nothing is discovered. The result is an uneven landscape of empty know-nothing interrupted by hills of self-organized knowledge. 
Of this culturally produced space, I am most fascinated by the deserts-by 
the holes. What can we know about what we don't know? The greatest promise looming in evolution theory is unraveling the mystery of why organisms don't change, because stasis is more common than change yet harder to explain. What can we know about no-change in a system of change? What do the holes of change tell us about the whole of change? And so, it is the holes in the space of wholes that I'd like to explore here.
This very book is full of holes as well as wholes. What I don't know far 
exceeds what I know, but unfortunately, it is far easier to write about what I know than about what I don't know. By the nature of ignorance, I am, of course, not aware of all the places and gaps where my own knowledge fails. Recognizing one's own ignorance is quite a trick. That goes for science, too. Mapping the holes of ignorance is perhaps science's next advance.Scientists today believe science is revolutionary. They explain how science 
works via a model of ongoing minirevolutions. According to this perspective, researchers build a theory to explain facts (for example, rainbows occur because light is a wave). The theory itself will suggest places to look for new facts (can you bend a wave?). It's the law of increasing returns again. As new facts are uncovered they are incorporated into the theory, buttressing its strength and reliability. Occasionally, scientists uncover new facts that aren't readily explained by the theory (light sometimes acts like a particle). These are called anomalies. Anomalies are set aside at first, while new facts that concur with the reigning theory continue to stream in. At some point, the accumulating anomalies prove too great, too troublesome, or too numerous to ignore. Inevitably then, some young turk proposes a revolutionary different model that explains the anomalies (such as, light is both wave and particle). The old is gone; the new quickly reigns.
In the terminology of science historian Thomas Kuhn, the reigning theory 
forms a self-reinforcing mindset called a paradigm that dictates what is fact and what is mere noise. From within the paradigm, anomalies are trivia, curiosities, illusions, or bad data. Research proposals endorsing the paradigm win grants, lab space, and degrees. Proposals operating outside the paradigm-those dabbling in distracting trivia-get nothing. The famous scientist who made his great revolutionary discovery while denied funds or credibility is so common it's become cliché; I've trotted out several of those cliché stories in this book. One example is the ignored work of scientists dabbling in ideas that contradict neodarwinian dogma. 
Real discovery in science, according to Kuhn in his seminal The Structure of 
Scientific Revolutions, only "commences with the awareness of anomaly." Progress is an acknowledgment of the opposition. A series of established paradigms are overthrown by downtrodden and oppressed anomalies (and their finders) as they rebel and usurp the throne by their countertruth. The new ideas reign, at least for a while, until they too become ossified and insensitive to the squawks of new anomalies, and are eventually overthrown themselves.
Kuhn's model of paradigm shift in science is so convincing that it has 
become a paradigm itself-the paradigms of paradigms. We now see paradigms and paradigm overthrows everywhere, inside of science and out. Paradigm shifts are our paradigm. The fact that things don't really work that way is, well, an anomaly.
Alan Lightman and Owen Gingerich, writing in a 1991 Science article, "When 
Do Anomalies Begin?," claim that contrary to the reigning Kuhnian model of science, "certain scientific anomalies are recognized only after they are given compelling explanations within a new conceptual framework. Before this recognition, the peculiar facts are taken as givens or are ignored in the old framework." In other words, the real anomalies that eventually overthrow a reigning paradigm are at first not even perceived as anomalies. They are invisible.
A few brief examples of "retrorecognition," based on Lightman's and 
Gingerich's article:
●     The fact that the shape of South America and Africa fit together like a lock 
and key did not bother any pre-1960s geologists. There was nothing troubling to them or their theories of continent formation in this observation, or in the observed ridges down the center of the oceans. Although the remarkable fit had been noticed since the Atlantic Ocean was first mapped, it was a fact that did not even need an explanation. Only later was the fit retrorecognized as something to explain.
●     Newton precisely measured the inertial mass of a great many objects 
(what it took to get them moving, as in getting a pendulum started) and their gravitational mass (how fast they fell to the Earth), to determine that the two forces were equal, if not equivalent, and could be canceled out when doing physics. For hundreds of years this relationship was not questioned. Einstein, however, was struck that "the law has not found any place in the foundations of our edifice of the physical universe." Unlike others, he was perplexed by this observation which he successfully explained in his revolutionary general theory of relativity.
●     For decades, the almost exact balance between the universe's kinetic and 
gravitational energies-a pair of forces that kept the expanding universe balanced between blowing up or collapsing-was noted in passing by astronomers. But it was never a "problem" until the revolutionary "inflationary universe" model came along in 1981 and made this fact a troubling paradox. The observation of the balance did not begin to be an anomaly until after the paradigm shift, when in retrospect, it was seen as a troublemaker. 
The common theme in each example is that anomalies begin as observed 
facts that don't require any explanation at all. They are not troublesome facts; they just are. Rather than the cause of a paradigm shift, anomalies are the result of the shift.
In a letter to Science, David P. Barash tells of his own experience with 
nonanomalies. He wrote a textbook of sociobiology in 1982, where he stated that "evolutionary biologists, beginning with Darwin, have been troubled by the fact that animals often do things that appear to benefit others, often at great cost to themselves." Sociobiology was launched by the 1964 publication of William Hamilton's inclusive fitness theory, which provided a workable, though controversial, way to interpret animal altruism. Barash writes, "However, stimulated by the Lightman-Gingerich thesis, I have reviewed numerous pre-1964 textbooks of animal behavior and evolutionary biology and have discovered that, in fact-and contrary to my own above-cited assertion-before Hamilton's insight, evolutionary biologists were not very much troubled by the occurrence of apparently altruistic behavior 
among animals (at least they did not devote much theoretical or empirical attention to the phenomenon)." He ends his letter by suggesting, half in jest, that biologists "teach a course in what we don't know about, say, animal behavior."

The final section in my book is a short course in what we, or at least I, 
don't know about complex adaptive systems and the nature of control. It's a 
list of questions, a catalogue of holes. A lot of the questions may seem silly, obvious, trivial, or hardly worth worrying about, even for nonscientists. Scientists in the pertinent fields may say the same: these questions are distractions, the ravings of a amateur science-groupie, the ill-informed musing of a techno-transcendentalist. No matter. I am inspired to follow this unorthodox short course by a wonderful paragraph written by Douglas Hofstadter in an forward to Pentti Kanerva's obscure technical monograph on sparse distributed computer memory. Hofstadter writes:
I begin with the nearly trivial observation that members of a familiar perceptual category automatically evoke the name of the category. Thus, when we see a staircase (say), no matter how big or small it is, no matter how twisted or straight, no matter how ornamented or plain, modern or old, dirty or clean, the label "staircase" spontaneously jumps to center stage without any conscious effort at all. Obviously, the same goes for telephones, mailboxes, milkshakes, butterflies, model airplanes, stretch pants, gossip magazines, women's shoes, musical instruments, beachballs, station wagons, grocery stores, and so on. This phenomenon, whereby an external physical stimulus indirectly activates the proper part of our memory, permeates human life and language so thoroughly that most people have a hard time working up any interest in it, let alone astonishment, yet it is probably the most key of all mental mechanisms.
To be astonished by a question no one else can get worked up about, or to be astonished by a matter nobody considers a problem, is perhaps a better paradigm for the progress of science.
This book is based on my astonishment that nature and machines work at 
all. I wrote it by trying to explain my amazement to the reader. When I came to something I didn't understand, I wrestled with it, researched, or read until I did, and then started writing again until I came to the next question I couldn't readily answer. Then I'd do the cycle again, round and round. Eventually I would come to a question that stopped me from writing further. Either no one had an answer, or they provided the stock response 
and would not see my perplexity at all. These halting questions never seemed weighty at first encounter-just a question that seems to lead to nowhere for now. But in fact they are protoanomalies. Like Hofstadter's unappreciated astonishment at our mind's ability to categorize objects before we recognize them, out of these quiet riddles will come future insight, and perhaps revolutionary understanding, and eventually recognition that we must explain them.
Readers may be perplexed themselves when they see that most of these 
questions appear to be the very ones I seemed to have answered in the preceding chapters! But really all I did was drive around these questions, surveying their girth, hill-climbing up them until I was stuck on a false summit. In my experience most good questions come while stuck on a partial answer somewhere else. This book has been an endeavor to find interesting questions. But on the way, some of the rather ordinary questions stopped me. They follow below.
●     I often use the word "emergent" in this book. As used by the practitioners 
of complexity, it means something like: "that organization which is generated out of parts acting in concert." But the meaning of emergent begins to disappear when scrutinized, leaving behind a vague impression that the word is, at bottom, meaningless. I tried substituting the word "happened" in every instance I used "emerged" and it seemed to work. Try it. Global order happens from local rules. What do we mean by emergent?
●     And what is "complexity" anyway? I looked forward to the two 1992 
science books identically titled Complexity, one by Mitch Waldrop and one by Roger Lewin, because I was hoping one or the other would provide me with a practical measurement of complexity. But both authors wrote books on the subject without hazarding a guess at a usable definition. How do we know one thing or process is more complex than another? Is a cucumber more complex that a Cadillac? Is a meadow more complex than a mammal brain? Is a zebra more complex than a national economy? I am aware of three or four mathematical definitions for complexity, none of them broadly useful in answering the type of questions I just asked. We are so ignorant of complexity that we haven't yet asked the right question about what it is.
●     If evolution tends to grow more complex, why? And if it really does not, 
then why does it appear to? Is complexity in fact more efficient than simplicity?
●     There seems to be a "requisite variety"-a minimum complexity or 
diversity of parts-for such processes as self-organization, evolution, learning, and life. How do we know for sure when enough variety is enough? We don't even have a good measure for diversity. We have intuitive feelings but we can't translate that into anything very precise. What is variety?●     The "edge of chaos" often sounds like "moderation in all things." Is it 
merely playing Goldilocks to define the values at which systems are 
maximally adaptable, as "just right for adaptation?" Is this yet another necessary tautology?
●     In computer science there is a famous conjecture called the 
Church/Turing hypothesis which undergirds much of the reasoning in artificial intelligence and artificial life. The hypothesis says: a universal computing machine can compute anything that another universal computing machine can compute, given unlimited time and an infinite tape. But my goodness! Unlimited time and space is the precise difference between the living and the dead. The dead have infinite time and space. The living live in finitude. So while, within a certain range, computational processes are independent of the hardware they run on (one machine can emulate anything another can), there are real limits to the fungibility of processes. Artificial life is based on the premise that life can be extracted from its carbon-based hardware and set to run on a different matrix somewhere else. The experiments so far have shown that to be true more than was expected. But where are the limits in real time and real space? 
●     What, if anything, cannot be simulated?
●     The quest for artificial intelligence and artificial life is wrapped up (some 
say bogged down) in the important riddle of whether a simulation of an extremely complex system is a fake or something real in its own right. Maybe it is hyperreal, or maybe the term hyperreality just ducks the question. No one doubts the ability of a model to imitate an original thing. The questions are: What sort of reality do we assign a simulation of a thing? What, if any, are the distinctions between a simulation and a reality?
●     How far can you compress a meadow into seeds? This was the question 
the prairie restorers inadvertently asked. Can you reduce the treasure of information contained in an entire ecosystem into several bushels of seeds, which, when watered, would reconstitute the awesome complexity of prairie life? Are there important natural systems which simply cannot be reduced and modeled accurately? Such a system would be its own smallest expression, its own model. Are there any artificial large systems that cannot be compressed or abstracted? 
●     I'd like to know more about stability. If we build a "stable" system, is 
there some way we can define that? What are the boundary conditions, the requirements, for stable complexity? When does change cease to be change?
●     Why do species ever go extinct? If all of nature is hourly working to 
adapt, never resting in its effort to outwit competitors and exploit its environment, why do certain classes of species fail? Perhaps some certain organisms are better adapted than others. But why would the universal mechanism of nature sometimes work and sometimes not for entire types of 
organisms, allowing particular groups to lag and others to advance? More precisely, why would the dynamics of adaptation work for some organisms but not others? Why does nature allow some biological forms to be pushed into forms that are inherently inefficient? There is a case of an oysterlike bivalve that evolved a more and more spiraled shell until, just before extinction, the valves could barely open. Why doesn't the organism return to the range of the workable? And why does extinction run in families and groups, as if bad genes may be responsible? How could nature produce a group of bad genes? Perhaps, extinctions are caused by something outside, like comets and asteroids. Paleontologist Dave Raup postulates that 75 percent of all extinction events were caused by asteroid impacts. If there were no asteroids would there be no extinctions? If there were no extinctions of species on Earth, what would life look like now? Why, for that matter, do complex systems of any sort fail or die? 
●     On the other hand, why, in this coevolutionary world, is anything at all 
stable?
●     Every figure I've heard for both natural and artificial self-sustaining 
systems puts the self-stabilizing mutation rate between 1 percent and 0.01 percent. Are mutation rates universal? 
●     What are the down sides of connecting everything to everything? 
●     In the space of all possible lifes, life on Earth is but a tiny sliver-one 
attempt at creativity. Is there a limit to how much life a given quantity of matter can hold? Why isn't there more variety of life on Earth? How come the universe is so small?
●     Are the laws of the universe evolvable? If the laws governing the universe 
arose from within the universe, might they be susceptible to the forces of self-adjustment? Perhaps the very foundational laws upholding all sensible laws are in flux. Are we playing in a game where all the rules are constantly being rewritten? 
●     Can evolution evolve its own teleological purpose? If organisms, which are 
but a federation of mindless agents, can originate goals, can evolution itself, equally blind and dumb but in a way a very slow organism, also evolve a goal?
●     And what about God? God gets no honor in the academic papers of 
artificial lifers, evolutionary theorists, cosmologists, or simulationists. But much to my surprise, in private conversations these same researchers routinely speak of God. As used by scientists, God is a coolly nonreligious technical concept, closer to god-a local creator. When talking of worlds, both real and modeled, God is an almost algebraically precise notation standing for whatever "X" operating outside a world that has created that world. "Okay, you're God..." says one computer scientist during a demo when he 
means that I'm now setting the rules for the world. God is a shorthand for the uncreated observer making things real. God thus becomes a scientific term, and a scientific concept. It doesn't have the philosophical subtleties of prime cause, or the theological finery of Creator; it is merely a handy way to talk about the necessary initial conditions to run a world. So what are the requirements for godhood. What makes a good god?

None of these questions is new. They have been asked before in different 
contexts by others. If the web of knowledge were completely wired then I 
could tag on the appropriate historical citations at this point, and pull out the historical context for all these musings.
Researchers dream of such a heavily connected network of data and ideas. 
Science today is at the other end of a connectivity limit; the nodes in the distributed network of science need to be much more connected before they reach maximum evolvability.
The first step toward a highly linked web of knowledge was made by U.S. Army medical librarians trying to unify the indexing of medical journals. In 1955, Eugene Garfield, a librarian on that project who was interested in machine indexing, developed a computer system to automatically track the bibliographic citations of every scientific paper published in medicine. Eventually he founded a commercial company in his garage in Philadelphia-the Institute of Science Information (ISI)-that would track on a computer every scientific paper published, period. Today ISI-a company with many employees and supercomputers-cross-links millions of scholarly papers with their bibliographic references. 
For instance, let's take one of the papers I refer to in my bibliography: 
Rodney Brooks's 1990 article "Elephants Don't Play Chess." I can go to the ISI system to find "Elephants Don't Play Chess" listed under its author and read off the list of all other published scientific papers, in addition to my Out of Control, that have cited "Elephants" in their bibliographies or footnotes. On the premise that other researchers and authors who find "Elephants" useful may also be useful to me, I have a way to backtrack the influence of ideas. (However, books are not at the moment indexed for citations, so in reality this example would only work if Out of Control were an article. But the principle holds.)
This citation index allows me to track the future dissemination of my own 
ideas. Again, assume Out of Control was indexed as a paper. Every year I could consult the ISI Citation Index and get a list of all those authors who cited my work in their work. This web would bring me to many people's ideas-many of them very germane since they quote me-that I might never find otherwise.
Citation indexing is currently employed to map the breaking "hot" areas of 
science. Clusters of a few extremely highly cited papers can indicate a rapidly moving area of research. An unintended corollary of this system is that government fund-givers use the Citation Index to assist them in determining whose research to fund. They count the total number of citations-adjusted for the "weight" or stature of the journal publishing the paper-of an individual scientist's work in order to indicate the importance of that scientist. But like any network, citation evaluation breeds the opportunity for a positive feedback loop: the more funding, the more papers produced, the more citations garnered, the more funding secured, and so on. And it engenders the identical reverse loop of no funding, no papers, no citations, no funding.
The Citation Index can also be thought of as a footnote tracking system. If you think of each bibliographic reference as a footnote in a text, then a citation index brings you to the footnote and then permits you to chase down the footnote to the footnote. A more elegant description of that system was coined "Hypertext" by Ted Nelson in 1974. In essence, hypertext is a large distributed document. A hypertext document is a vague network of live links between its words and ideas and sources. The document has no center, no end. You read hypertext by navigating through it, taking side tours to footnotes, and to footnotes to the footnotes, following parenthetical thoughts as long and complex as the "main" text. Any other document can be linked to and become part of another text. Computerized hypertext incorporates marginalia and commentaries to the text by other writers, updates, revisions, abstracts, digests, misinterpretations, and as in citation indexing, all bibliographic references to the work.
The extent of the distributed document is thus unknowable because it is 
without boundaries and often multiauthored. It's a swarm text. But a single author can compile a simple hypertext document which can be read in many different directions and along many paths. Thus, the reader of hypertext creates a different work of the author's web depending on how she goes through the material. Therefore in hypertext, as in other distributed creations, the creator must give up some control of his creation.
Hypertext documents of various depths have existed for ten years. In 1988, 
I was involved in developing one of the first commercial hypertext works-an electronic version of the Whole Earth Catalog, rendered in HyperCard on the Macintosh computer. Even in this relatively small network of texts (there were 10,000 microdocuments; and millions of ways to travel through them), I got a sense of this new space of interlinked ideas.
For one thing, it was easy to get lost. Without the centering hold of a 
narrative, everything in a hypertext network seems to have equal weight and appears to be the same wherever you go, as if the space were a suburban sprawl. The problem of locating items in a network is substantial. 
It harks back to the days of early writing when texts in a 14th-century scriptorium were difficult to locate since they lacked cataloguing, indexes, or tables of contents. The advantages which the hypertext model offers over the web of oral tradition is that the former can be indexed and catalogued. An index is an alternative way to read a printed text, but it is only one of many ways to read a hypertext. In a sufficiently large library of information without physical form-as future electronic libraries promise to be-the lack of simple but psychologically vital clues, such as knowing how much of the total you've read or roughly how many ways it can be read, is debilitating.
Hypertext creates it own possibility space. As Jay David Bolter writes in his 
outstanding, but little known book, Writing Spaces: 
In this late age of print, writers and readers still conceive of all texts, of text 
itself, as located in the space of a printed book. The conceptual space of a printed book is one in which writing is stable, monumental, and controlled exclusively by the author. It is the space defined by perfect printed volumes that exist in thousands of identical copies. The conceptual space of electronic writing, on the other hand, is characterized by fluidity and an interactive relationship between writer and reader.
Technology, particularly the technology of knowledge, shapes our thought. 
The possibility space created by each technology permits certain kinds of thinking and discourages others. A blackboard encourages repeated modification, erasure, casual thinking, spontaneity. A quill pen on writing paper demands care, attention to grammar, tidiness, controlled thinking. A printed page solicits rewritten drafts, proofing, introspection, editing. Hypertext, on the other hand, stimulates yet another way of thinking: telegraphic, modular, nonlinear, malleable, cooperative. As Brian Eno, the musician, wrote of Bolter's work, "[Bolter's thesis] is that the way we organize our writing space is the way we come to organize our thoughts, and in time becomes the way which we think the world itself must be organized."
The space of knowledge in ancient times was a dynamic oral tradition. By 
the grammar of rhetoric, knowledge was structured as poetry and dialogue-subject to interruption, questioning, and parenthetical diversions. The space of early writing was likewise flexible. Texts were ongoing affairs, amended by readers, revised by disciples; a forum for discussions. When scripts moved to the printed page, the ideas they represented became monumental and fixed. Gone was the role of the reader in forming the text. The unalterable progression of ideas across pages in a book gave the work an impressive authority-"authority" and "author" deriving from a common root. As Bolter notes, "When ancient, medieval, or even Renaissance texts are prepared for modern readers, it is not only the words that are translated: the text itself is translated into the space of the modern printed book."
A few authors in the printed past tried to explore expanded writing and thinking spaces, attempting to move away from the closed linearity of print 
and into the nonsequential experience of hypertext. James Joyce wrote Ulysses and Finnegan's Wake as a network of ideas colliding, cross-referencing, and shifting upon each reading. Borges wrote in a traditional linear fashion, but he wrote of writing spaces: books about books, texts with endlessly branching plots, strangely looping self-referential books, texts of infinite permutations, and the libraries of possibilities. Bolter writes: "Borges can imagine such a fiction, but he cannot produce it....Borges himself never had available to him an electronic space, in which the text can comprise a network of diverging, converging, and parallel times."

I live on computer networks. The network of networks-the Internet-links 
several millions of personal computers around the world. No one knows 
exactly how many millions are connected, or even how many intermediate nodes there are. The Internet Society made an educated guess in August 1993 that the Net was made up of 1.7 million host computers and 17 million users. No one controls the Net, no one is in charge. The U.S. government, which indirectly subsidizes the Net, woke up one day to find that a Net had spun itself, without much administration or oversight, among the terminals of the techno-elite. The Internet is, as its users are proud to boast, the largest functioning anarchy in the world. Every day hundreds of millions of messages are passed between its members, without the benefit of a central authority. I personally receive or send about 50 messages per day. In addition to the vast flow in individual letters, there exist between its wires that disembodied cyberspace where messages interact, a shared space of written public conversations. Every day authors all over the word add millions of words to an uncountable number of overlapping conversations. They daily build an immense distributed document, one that is under eternal construction, constant flux, and fleeting permanence. "Elements in the electronic writing space are not simply chaotic," Bolter wrote, "they are instead in a perpetual state of reorganization."
The result is far different from a printed book, or even a chat around a table. 
The text is a sane conversation with millions of participants. The type of thought encouraged by the Internet hyperspace tends toward nurturing the nondogmatic, the experimental idea, the quip, the global perspective, the interdisciplinary synthesis, and the uninhibited, often emotional, response. Many participants prefer the quality of writing on the Net to book writing because Net-writing is of a conversational peer-to-peer style, frank and communicative, rather than precise and overwritten.
A distributed dynamic text, such as the Net and a number of new books in 
hypertext, is an entirely new space of ideas, thought, and knowledge. Knowledge shaped by the age of print birthed the very idea of a canon, which in turn implied a core set of fundamental truths-fixed in ink and perfectly duplicated-from which knowledge progressed but never retreated. The job of every generation of readers was to find the canonical truth in texts.Distributed text, or hypertext, on the other hand supplies a new role for 
readers-every reader codetermines the meaning of a text. This relationship is the fundamental idea of postmodern literary criticism. For the postmodernists, there is no canon. They say hypertext allows "the reader to engage the author for control of the writing space." The truth of a work changes with each reading, no one of which is exhaustive or more valid then another. Meaning is multiple, a swarm of interpretations. In order to decipher a text it must be viewed as a network of idea-threads, some threads of which are owned by the author, some belonging to the reader and her historical context and others belonging to the greater context of the author's time. "The reader calls forth his or her own text out of the network, and each such text belongs to one reader and one particular act of reading," says Bolter.
This fragmentation of a work is called "deconstruction." Jacques Derrida, the 
father of deconstructionism, calls a text (and a text could be any complex thing) "a differential network, a fabric of traces referring endlessly to something other than itself, to other differential traces," or in Bolter's words "a texture of signs that point to other signs." This image of symbols referring to other symbols is, of course, the archetypal image of the infinite regress and the tangled recursive logic of a distributed swarm; the banner of the Net and the emblem of everything connected to everything.
The total summation we call knowledge or science is a web of ideas pointing 
to, and reciprocally educating each other. Hypertext and electronic writing accelerate that reciprocity. Networks rearrange the writing space of the printed book into a writing space many orders larger and many ways more complex than of ink on paper. The entire instrumentation of our lives can be seen as part of that "writing space." As data from weather sensors, demographic surveys, traffic recorders, cash registers, and all the millions of electronic information generators pour their "words" or representation into the Net, they enlarge the writing space. Their information becomes part of what we know, part of what we talk about, part of our meaning.
At the same time the very shape of this network space shapes us. It is no 
coincidence that the postmodernists arose in tandem as the space of networks formed. In the last half-century a uniform mass market-the result of the industrial thrust-has collapsed into a network of small niches-the result of the information tide. An aggregation of fragments is the only kind of whole we now have. The fragmentation of business markets, of social mores, of spiritual beliefs, of ethnicity, and of truth itself into tinier and tinier shards is the hallmark of this era. Our society is a working pandemonium of fragments. That's almost the definition of a distributed network. Bolter again: "Our culture is itself a vast writing space, a complex of symbolic structures....Just as our culture is moving from the printed book to the computer, it is also in the final stages of the transition from a hierarchical social order to what we might call a 'network culture.'"
There is no central keeper of knowledge in a network, only curators of particular views. People in a highly connected yet deeply fragmented society 
can no longer rely on a central canon for guidance. They are forced into the modern existential blackness of creating their own culture, beliefs, markets, and identity from a sticky mess of interdependent pieces. The industrial icon of a grand central or a hidden "I am" becomes hollow. Distributed, headless, emergent wholeness becomes the social ideal.
The ever insightful Bolter writes, "Critics accuse the computer of promoting 
homogeneity in our society, of producing uniformity through automation, but electronic reading and writing have just the opposite effect." Computers promote heterogeneity, individualization, and autonomy. 
No one has been more wrong about computerization than George Orwell in 
1984. So far, nearly everything about the actual possibility-space which computers have created indicates they are the end of authority and not its beginning.
Swarm-works have opened up not only a new writing space for us, but a 
new thinking space. If parallel supercomputers and online computer networks can do this, what kind of new thinking spaces will future technologies-such as bioengineering-offer us? One thing bioengineering could do for the space of our thinking is shift our time scale. We moderns think in a bubble of about ten years. Our history extends into the past five years and our future runs ahead five years, but no further. We don't have a structured way, a cultural tool, for thinking in terms of decades or centuries. Tools for thinking about genes and evolution might change this. Pharmaceuticals that increase access to our own minds would, of course, also remake our thinking space.
One last question that stumped me, and halted my writing: How large is the 
space of possible ways of thinking? How many, or how few, of all types of logic have we found so far in the Library of thinking and knowledge?
Thinking space may be vast. The number of ways to overcome a problem, or 
to explore a notion, or to prove a statement, or to create a new idea, may be as large as the number of ideas itself. Contrarily, thinking space may be as small and narrow as the Greek philosophers thought it was. My bet is that artificial intelligence, when it comes, will be intelligent but not very humanlike. It will be one of many nonhuman methods of thought that will probably fill the library of thinking space. This space will also hold types of thinking that we simply cannot understand at all. But still we will use them. Nonhuman cognitive methods will provide us wonderful results beyond and out of our control.
Or we may surprise ourselves. We may have a brain that, like a Kauffman 
machine, is able to generate all types of thinking and never-seen-before complexity from a small finite set of instructions. Perhaps the space of possible cognition is our space. We could then climb into whatever kind of logic we can make, evolve, or find. If we can travel anywhere in cognitive 
space, we would be capable of an open-ended universe of thoughts. 
I think we'll surprise ourselves.