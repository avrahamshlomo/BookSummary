Tom Ray created an experimental computer virus that reproduced rapidly in a confined world. It evolved into a soup of nearly 100 types of computer viruses, all battling it out for survival. Ray had brewed artificial evolution. Ray was overwhelmed by the complexity of ecological relations in the rainforest. By the time he finished his Ph.D., he felt that ecology lacked good theories to generalize the wealth of observations piling up from every patch of wilderness. It was stymied by extensive local knowledge. Tom Ray, a biologist, believed that ecology needed a science of complexity that addressed the riddles of form, history, development, and was supported by field data. Ray dreamed of making an electric-powered evolution machine that could demonstrate the historical principles of ecology. He wanted to study evolution itself rather than studying the products of evolution. To solve the problem of making an electronic evolution machine, Ray started with simple replicators and gave them a cozy habitat and plenty of energy and places to fill. He cooked up a soup of computer viruses to make his dream come true. Ray devised a virtual computer to contain his experiments and sealed them from the outside world. He called his world "Tierra" and seeded it with a single creature that he programmed by hand. The 80 creature reproduced by finding an empty RAM block 80 bytes big and then filling it with a copy of itself. In Tierra, a computer program designed by Tom Ray, creatures were created through random variation, death, and natural selection. Ray introduced two key features that modified the copying machine into an evolution machine: his program occasionally scrambled the digital bits during copying, and he assigned his creatures a priority tag for an executioner. The Reaper would kill off buggy programs that couldn't reproduce, but would pass over the very rare mutants that worked, that is, those that happened to form a bona fide alternative program. These legitimate variations could multiply and breed other variants. Ray found that a viable creature with only 45 very efficient bytes overran all other creatures. On close examination of 45's code, Ray was amazed to discover that it was a parasite. It contained only a part of the code it needed to survive. As long as there were enough 80 hosts around, the 45s thrived. But if there were too many 45s in the limited world, there wouldn't be enough 80s to supply copy resources. As the 80s waned, so did the 45s. Ray learned to run ecological experiments in Tierra using parasites. Tom Ray's experiment with Tierra, an artificial life program, led to the discovery of hyperparasites, social cheaters, and creatures that surpassed the programming skills of human software engineers. Ray discovered that smaller creatures had an advantage as they consumed fewer computer cycles, leading to a size-neutral world. The experiment also created sex, which was not programmed but discovered by the system. Ray found that even without programmed mutation, evolution pushed forward, and sex was a much more important source of variation in real natural life. Artificial life researcher, Tom Ray, created a digital ecosystem called Tierra, where computer programs evolved and reproduced. Ray observed that interrupted asexual reproduction, caused by the death of a host, resulted in wild new recombinations that fueled evolution. Tierra also displayed punctuated equilibrium, where long periods of stasis were punctuated by rapid bursts of change. Ray is now working on a Darwin Chip, which could be plugged into any computer and used to evolve lines of code or entire software programs. However, he believes that messy evolution should happen away from the end-user, happening offline in back rooms. The article discusses the use of evolutionary algorithms in computer programs to optimize solutions to complex problems. These algorithms mimic the process of natural selection and genetic recombination to produce better results over time. The article gives examples of how these algorithms are used, such as in the Evolver software for optimizing spreadsheets and in genetic algorithms developed by John Holland. Holland's algorithms rely on sexual recombination and mutation to produce better code, and they have been used in a variety of fields, including engineering and finance. The article also discusses the challenges of using evolutionary algorithms, such as the risk of getting stuck on local peaks instead of finding the global optimum. John Holland's genetic algorithms (GAs) mimic nature's evolutionary process to solve complex problems. GAs create populations of code that evolve and mutate simultaneously, with high-performing code mating to focus on the most promising areas of the problem landscape. Parallelism is key to GAs' success, allowing for the testing of multiple combinations at once to locate the global optima. Holland's work on GAs was largely ignored until the late 1980s, with only a few dozen articles published in science between 1972 and 1982. Despite this, GAs have become a powerful tool in computer science and problem-solving. The disregard for computational evolution can be traced back to the conflict it presented to the reigning dogma of computers: the von Neumann serial program. Von Neumann's serial architecture for computation, where one instruction at a time was executed, became the standard for every commercial computer thereafter. John Holland, who worked on Project Whirlwind and later joined the logical design team on IBM's Defense Calculator, believed that understanding evolutionary adaptation could help understand and maybe imitate conscious learning. Holland stumbled upon R.A. Fisher's The Genetical Theory of Natural Selection in the University of Michigan math library in 1953, which transformed population-thinking into a quantitative science. The article discusses the history of computational evolution, starting with R.A. Fisher's use of mathematics to study evolution. John Holland was inspired by Fisher's work and developed a vision of using computers to simulate evolution. Holland faced the challenge of designing a computer that could handle parallel processing, which he eventually achieved with the creation of the "Holland Machine." However, it wasn't until the mid-1980s that Danny Hillis developed the first massively parallel computer, the Connection Machine, which allowed for true parallelism in computational evolution. Hillis believes that evolution is the key to creating complex systems and has used the Connection Machine to simulate evolution with a population of simple software programs. Danny Hillis conducted an experiment using the Connection Machine, a supercomputer, to simulate evolution and coevolution. He created a program that sorted a long string of numbers into numerical order, and allowed thousands of sorters to mutate and reproduce, terminating the less fit. He then introduced a mutating sorting test, which acted as a parasite trying to disrupt the sorter, and found that the introduction of a parasite sped up the rate of evolution. The coevolution of sorters and tests led to the evolution of a sorting program previously unknown to computer scientists. This approach to artificial intelligence, known as "connectionism," involves vast webs interconnecting dumb neurons, which perform simultaneous calculations en masse, similar to genetic algorithms but with more sophisticated (smarter) accounting systems. These smartened up networks are called neural networks. The article discusses the potential of artificial evolution in creating intelligence and consciousness. While neural nets have limited success in generating intelligence, artificial evolutionists still pursue the dream of connectionism. However, the slow pace of evolution is a concern, but evolution in a computer allows for the recording of every creature's genome and demographic record. Tom Ray faces skepticism from colleagues that his synthesized evolution is identical to the evolution in nature. The article concludes by drawing a parallel between the artificial lightning generated by Benjamin Franklin and the potential of artificial evolution in shaping our digital society. The article discusses the potential of artificial evolution, which is seen as a natural technology that can be easily moved into computer code. Evolution can coordinate the complexity demanded by modern technology, and it is already being used in bioengineering. The article highlights the challenges of protein folding and drug design, which have traditionally relied on hit or miss or rational drug design approaches. Evolutionary systems offer a new approach that generates billions of random molecules that are tested against the lock, with one molecule containing a single site that matches one of the lock's sites being retained. The article discusses the potential of using evolutionary methods to develop new drugs and vaccines. By repeatedly mutating and testing molecules against a target, researchers can evolve new molecules that fit the target. This method can be used to create alternative versions of natural drugs that may have advantages such as being smaller, producing fewer side effects, or being more specific in their targets. The resulting molecules are indistinguishable from rationally designed drugs, but the difference is that while an evolved drug works, we have no idea of how or why it does so. The article also discusses the difficulty of incorporating the dual nature of heritable information and mortal bodies into an evolutionary system and how biochemist Gerald Joyce has recreated a probable earlier stage of life on Earth, "RNA world," in a test tube to develop a robust artificial evolutionary system. Gerald Joyce and his team at Scripps Institute are using RNA molecules to create an open-ended artificial evolution system that can produce a billion copies in one hour. Joyce sees the potential for this system to be used commercially to create useful chemicals and drugs through directed evolution. Directed evolution is a form of supervised learning where the breeder directs the choice of varieties. David Ackley, a researcher of neural nets and genetic algorithms at Bellcore, has also explored evolutionary systems and found that his world was able to evolve amazingly fit organisms. Successful individuals would live Methuselahian lifetimes. The article discusses the work of Christopher Langton and his experiments with artificial life. Langton explored the concept of genetic engineering and how it affects the survival of a species. He discovered that while handcrafted genes suited individuals, they lacked the robustness of organically grown genes, which suited the species to the max. Langton's experiments were aimed at expanding the space of what people recognize as computation. He was primarily interested in those procedures that underpin learning and weak learning as a way of maximizing computation. Langton's mission was to find out what can be learned using only death as a teacher. In the space of all possible computation and learning, natural selection holds a special position where information transfer is minimized. The article discusses the concept of natural selection and its role in learning and evolution. It highlights that natural selection is the elemental melting point of learning and that it plays out in many guises. The article also discusses the limitations of biological Lamarckian evolution due to its need to trace advantageous changes in the body back through embryonic development into genetic blueprints. However, the article notes that Lamarckian evolution can work in artificial computer worlds, as demonstrated by David Ackley and Michael Littman's implementation of a Lamarckian system on a parallel computer with 16,000 processors. The article concludes that with the new hardware of computers, a host of other adaptive systems can be conjured up, and entirely different search strategies set out to shape them. The article discusses different methods of evolution and learning, including Darwinian and Lamarckian evolution and the use of ant algorithms inspired by the collective behavior of ant colonies. The Bellcore scientists found that Lamarckian evolution, which allows for information acquired during an individual's lifetime to be incorporated into long-term learning, produced smarter solutions than Darwinian evolution. The Milan group's Ant Algorithms utilize the distributed parallel systems and communication methods of real ants to optimize paths on computationally rugged landscapes. These methods expand the "space of all possible types of computation" and offer new insights into the potential of evolutionary and learning systems. Ant algorithms are a type of distributed computation that mimic the behavior of ants in finding the shortest route between a large number of cities. Each virtual ant in the colony leaves a trail of pheromones, and the shorter the path between cities, the stronger the pheromone signal. By varying parameters, researchers can come up with different computational ant searches. Ant algorithms are a type of Lamarckian search, where learning in one ant's lifetime is indirectly incorporated into the whole colony's inheritance of information. The cleverness of the ants, both real and virtual, is that the amount of information invested into "broadcasting" is very small, done very locally, and is very weak. Parallel computers are the future of computing, but parallel software is a tangled web of horizontal, simultaneous causes, making it hard to manage. The complexity of programming parallel computers is beyond human capabilities, according to experts. They suggest that natural evolution may be the solution to this challenge, as it can optimize parallelism in a way that humans cannot. Danny Hillis proposes using swarm systems to evolve better software, with tiny parasitic programs that encourage faster convergence to an error-free, robust software navigation program. Evolution can also make software more flawless than humans can, as it can test them ruthlessly. As computer programs grow to billions of lines of code, maintaining their reliability and up-time will become the primary chore of the software itself. The article discusses the concept of "software biology" or "living computation," which refers to the idea that software must constantly adapt and evolve to survive in the ever-changing world of daily use. The author argues that artificial evolution may be necessary to keep software adaptable and flexible, but this comes at the cost of control. As software becomes more complex and evolves, traditional engineering principles such as precision and predictability become less important, and survival becomes the primary concern. The article also touches on the potential of nanotechnology and the convergence of biotechnology and nanotechnology, but warns of the dangers of allowing artificial evolution to run rampant. Overall, the article suggests that the future of software development will require a shift in thinking from precise engineering to adaptable, evolving systems. The article discusses the trade-off between control and flexibility in complex systems. It argues that while control provides predictability, it limits the ability of a system to respond to unexpected events. The author suggests that we should embrace the messiness of life and allow for out-of-control but responsive systems, even if it means sacrificing some level of correctness. The article also touches on the challenge of selecting out disadvantageous side effects in evolution, which is limited by our inability to define what we don't want. The author concludes that giving up control can lead to the evolution of new possibilities and richness in the world.