In 1959, a diverse group of scientists gathered in Chicago to discuss self-organizing systems, a topic that had never before been the focus of a large meeting. The conference covered three basic elements of self-organization: memory, differentiation, and subordination. The ideas presented by Dr. Joachim Weyl at the conference were remarkably prescient, and have since become the basis of the breaking science of adaptive, distributed systems. However, despite recent progress, many basic questions about self-organization still remain mysterious. The 1959 conference was part of a series of meetings organized by the Josiah Macy, Jr. Foundation, which brought together scientists from different fields to discuss interdisciplinary topics. The Macy Conferences brought together a group of interdisciplinary thinkers who pioneered cybernetics, the art and science of control. They articulated a language of control and design that worked for biology, social sciences, and computers, and considered living things as machines and machines as living things. The conferences produced important concepts such as feedback control, circular causality, homeostasis in machines, and political game theory. The cybernetic group did not find answers but prepared an agenda for questions that would later be explored by scientists studying chaos, complexity, artificial life, subsumption architecture, artificial evolution, simulations, ecosystems, and bionic machines. The absence of the word "cybernetics" in modern systems thinking is a mystery, and the author offers three theories about why the cybernetic movement died. Firstly, cybernetics lost funding to artificial intelligence (AI), which failed to produce usefulness. Secondly, cybernetics was mostly talk and never made it to experiment due to the limitations of batch-mode computing. Thirdly, cybernetics was strangled by "putting the observer inside the box," leading to an infinite regress. By the late 1970s, cybernetics had died of dry rot, and the gap was filled by young enthusiasts not burdened by wise old men. The author concludes that scientific knowledge is a parallel distributed system, a web of coevolutionary systems of fact and theory. The author explores the uneven landscape of scientific knowledge, which is lumpy and characterized by vast deserts of ignorance between small areas of knowledge. Positive feedback and attractors create hills of self-organized knowledge, while ignorance breeds ignorance. The author is fascinated by the deserts of knowledge and the holes in the space of wholes. The book is full of holes as well as wholes, and mapping the holes of ignorance is science's next advance. Scientists believe science is revolutionary and works via ongoing mini-revolutions, building a theory to explain facts, which suggests places to look for new facts. Anomalies are set aside until they prove too great, troublesome, or numerous to ignore, and a young turk proposes a revolutionary different model that explains the anomalies. The reigning theory forms a self-reinforcing mindset called a paradigm that dictates what is fact and what is mere noise. Real discovery in science only commences with the awareness of anomaly, and progress is an acknowledgment of the opposition. The Kuhnian model of science suggests that anomalies lead to paradigm shifts, but Alan Lightman and Owen Gingerich argue that some anomalies are not perceived as such until after a new conceptual framework is established. These "retrorecognized" anomalies are initially seen as nonanomalies and do not require explanation. Examples include the fit of South America and Africa, the equivalence of inertial and gravitational mass, and the balance of kinetic and gravitational energies in the universe. Anomalies are the result, not the cause, of paradigm shifts. The inclusive fitness theory in sociobiology is an example of a workable explanation that made animal altruism no longer an anomaly. The author of the book "Complex Adaptive Systems: An Introduction to Computational Models of Social Life" suggests that there is still much that we do not understand about complex adaptive systems and the nature of control. He presents a list of questions that he has encountered while researching the topic, which may seem trivial or obvious to some but could lead to future insights and revolutionary understanding. The author explains that he wrote the book by trying to explain his amazement at how nature and machines work and wrestling with questions that he did not understand until he found answers or encountered new questions. The author believes that asking questions that no one else is interested in or that seem to lead nowhere could be a better paradigm for scientific progress. The concept of "emergent" as used in complexity theory is vague and difficult to define, leading to questions about the meaning of complexity itself. There is no practical measurement of complexity, and it is unclear why evolution tends towards complexity or whether complexity is more efficient than simplicity. The idea of "requisite variety" for self-organization, evolution, learning, and life is also difficult to measure. The limits of simulation and artificial life raise questions about the distinction between a simulation and reality, and whether a simulation can be considered real in its own right. Finally, the ability to compress complex natural systems into models or simulations is uncertain, leading to the possibility that some systems cannot be accurately modeled or reduced. The text poses several questions about the stability and evolution of complex systems, both natural and artificial. The author questions why some species go extinct while others thrive, and whether the laws of the universe are evolvable. They also explore the concept of "God" as a technical term used by scientists to refer to the uncreated observer making things real. The author acknowledges that these questions are not new, but are still relevant and require further exploration. The web of knowledge, a highly-connected network of data and ideas, is a dream for researchers. The Institute of Science Information (ISI) was founded by Eugene Garfield in 1955 to track bibliographic citations of scientific papers, and today it cross-links millions of scholarly papers with their bibliographic references. The citation index allows researchers to backtrack the influence of ideas and track the future dissemination of their own ideas. It is also used to map the breaking "hot" areas of science and assist government fund-givers in determining whose research to fund. However, citation evaluation can lead to a positive feedback loop of funding, papers, and citations, or the reverse. The Citation Index can also be thought of as a footnote tracking system or hypertext, a large distributed document with live links between words, ideas, and sources. Hypertext is a form of computerized text that allows for marginalia, commentaries, updates, revisions, and bibliographic references. The reader of hypertext creates a different work of the author's web depending on how they traverse the material, thus giving up some control to the creator. Hypertext creates its own possibility space, characterized by fluidity and an interactive relationship between writer and reader. The space of knowledge has evolved from a dynamic oral tradition to a fixed printed page, and now to hypertext, which stimulates telegraphic, modular, nonlinear, malleable, and cooperative thinking. However, the lack of simple but psychologically vital clues, such as knowing how much of the total you've read or roughly how many ways it can be read, is debilitating in large libraries of information without physical form. The article explores the differences between printed books and the internet, specifically hypertext, as spaces for ideas, knowledge, and thought. Printed books are linear and authoritative, while hypertext is nonsequential, constantly changing, and allows for a peer-to-peer conversational style of writing. The internet, as the largest functioning anarchy in the world, offers a new space for ideas and knowledge that is constantly in flux and where every reader codetermines the meaning of a text. The article argues that hypertext supplies a new role for readers, where the truth of a work changes with each reading, and there is no canon. The meaning of a text is multiple and fragmented, belonging to both the author and the reader's historical context. This fragmentation is called "deconstruction," and the image of symbols referring to other symbols is the emblem of everything connected to everything. The total summation of knowledge is a web of ideas that accelerate reciprocity through hypertext and electronic writing. Networks rearrange the writing space of the printed book into a writing space many orders larger and more complex than ink on paper. The shape of this network space shapes us, and our society is a working pandemonium of fragments. There is no central keeper of knowledge in a network, only curators of particular views. The computer promotes heterogeneity, individualization, and autonomy, and swarm-works have opened up a new thinking space. The author explores the concept of thinking space and how future technologies, such as bioengineering, could shift our time scale and expand our ability to think beyond the current ten-year bubble. The space of possible ways of thinking may be vast, with the number of ways to overcome a problem or create new ideas being as large as the number of ideas itself. Artificial intelligence may be one of many nonhuman methods of thought that will fill the library of thinking space, along with types of thinking that we cannot even understand. However, the author suggests that we may have a brain capable of generating all types of thinking and complexity, allowing us to travel anywhere in cognitive space and achieve an open-ended universe of thoughts.