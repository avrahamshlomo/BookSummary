The article discusses the evolution of computer animation and the potential for computer-generated creatures to have their own behavior and will. The author describes the limitations of traditional animation and how computer graphics engineers are trying to imbue physics into simulated worlds to increase realism. By adding physical rules, such as gravity and mass, to the environment, computer-generated creatures can become more lifelike. The Jurassic Park dinosaurs are cited as an example of lifelike computer-generated creatures, while the talking cat in Disney's Hocus Pocus is mentioned as a virtual character with behavior. The article concludes by suggesting that as artificial environments become more complex, they may become fertile ground for synthetic life. The article discusses the use of computer simulations in creating realistic animations in movies. Animators use physics laws to create virtual objects that behave like their real-world counterparts. To achieve this, everything in creation will have to be reduced to equations, including trees, buildings, and weather. The article also mentions the use of CAD programs and automatic fabrication technology to create physical prototypes of objects. The mathematical model of botanical growth is also discussed, which can be used to create 3-D virtual flowers. The article concludes that in the future, every object in the world, manufactured or not, will be modeled inside a computer. The article discusses the use of computer programs to simulate growth and create realistic images of plants and animals. Prusinkiewicz and his team have created algorithms that can simulate the growth of plants and seashells, which have practical applications in horticulture and landscape design. The article also explores the potential for computer programs to simulate physical forces and create virtual worlds that obey the laws of physics. The author then visits Disney's animation studio, where they are cautiously exploring ways to automate animation while preserving the artistry of the craft. The article discusses Disney's transition to computer-generated animation in the early 1990s, starting with The Great Mouse Detective and culminating in the paperless animation of the ballroom scene in Beauty and the Beast. The article also highlights the use of computer-generated backgrounds and characters in subsequent films such as The Little Mermaid and Aladdin. The article then shifts focus to Pixar, where ex-Disney animator John Lassiter is creating award-winning short computer animations featuring handcrafted animation using a cursor to modify computer-rendered 3-D objects. The article notes that there is a communication problem between hand animators and computer animators and that computer animation reverses the animation process. The article discusses the role of animators in creating believable characters and the limitations of computer-generated characters. Animators act as surrogate actors, using mirrors to draw exaggerated facial expressions and bringing characters to life through their own movements. While computer-generated effects have advanced, creating a convincing human form and facial expressions remains a challenge. However, with increasing computer power, it is predicted that within five years, a character created by releasing synthetic behavior into a synthetic body could star in a film. The article concludes by discussing the ongoing quest for control of human facial expressions and behavior in computer-generated characters. Colossal is a special effects studio that created famous animated commercials and shows, including Liquid TV for MTV. They have a studio with large computer monitors and heavy-duty graphic workstations. The studio created Moxy, the first completely computer-animated character, who can dance and lip-sync automatically. The next step for animators is to imbue characters like Moxy with elementary moves, but this is difficult given the complexity of human movement, which has about 200 moving joints and countless possible positions and pathways. The way we move is unique to each individual and can be identified even from a distance. Creating artificial movement is challenging as some sequences of movements are more natural than others, and timing is crucial. Researchers have studied animal behavior to create legged vehicles and extract generic locomotion patterns. MIT's Media Lab developed autonomous animats that could walk across uneven terrain without human intervention. Lemonhead, a cartoony figure, inspired the creation of a library of behaviors and gestures that could be combined in the right sequence for sensible action. Two characters, John and Mary, were created to act out a goodbye scene in a simple room viewed from an oblique angle above the ceiling. The article discusses the development of autonomous virtual characters in computer animations. The process involves creating a bottom-up behavioral engine that allows characters to move and act autonomously, without being dictated by the programmer's every movement. The challenge is to create characters that can navigate, express emotion, and react in a way that is realistic and believable. To achieve this, animators and roboticists are drawing inspiration from the science of ethology, which studies animal behavior and provides a framework for understanding the logical underpinnings of character. Ultimately, the goal is to create virtual actors that can be directed like real actors, with general instructions that allow them to figure out how to coordinate their limbs and interact with their environment on their own. Ethology, the study of animal behavior, has provided a decentralized behavioral framework that has been adopted by cartoonists, engineers, and computer scientists. At the core of this architecture is the concept of decentralized coordination of independent action centers, which work like building blocks to produce behavior. Some modules consist of reflexes that invoke simple functions, while others are management modules that arbitrate between conflicting drives. The behavior of animals is determined by a web of independent "behavioral agents" that cross-activate each other, forming a gross pattern. This decentralized, distributed control also governs robots and digital creatures, and any personality can be programmed by arranging a circuit of sub-behaviors. Autonomous animated characters in films behave according to the same organizational rules as real animals, with their behavior being synthetic but real. The article discusses the potential for artificially generated behavior in the form of characters with programmed emotions and personalities. The challenge is to find a way to allow such characters to behave autonomously while still being useful to humans. The Oz project at Carnegie Mellon University explores this challenge by creating a virtual world with automatons and human-directed characters that allows humans to participate in the story without changing the plot. The project focuses on three frontiers of control research: organizing a narrative to allow deviations, constructing an environment that generates surprise events, and creating creatures with autonomy but not too much. The goal is to create a world where personality pet-like creatures, or "woggles," can exist and interact with humans. The article concludes by suggesting that this shift in control is shifting the goal of authors from creating a story to creating a world with personalities. The article discusses the development of virtual creatures in the field of artificial life research. The author describes woggles, which are small creatures that exhibit different behaviors based on their color, and a virtual reality prototype that allows users to interact with jellyfish and fish in a three-dimensional underwater environment. Another researcher, Pattie Maes, has developed a system called ALIVE that allows users to interact with autonomous goal-seeking animats called hamsters via a computer screen and video camera. Maes aims to teach her creatures to learn from their environment without much human supervision. The article emphasizes the importance of creating interesting and autonomous virtual characters for virtual reality to be engaging. The article describes the evolution of Mickey Mouse from a two-dimensional cartoon character to a 3D virtual actor capable of learning and developing his own personality. The process involved deconstructing Mickey Mouse and reanimating him using computer graphics. The animators then added a Maes learning module to Mickey's code, allowing him to mature as an actor and respond to the emotions and actions of other characters. However, over time, Mickey began to develop his own ideas and personality, sometimes becoming hostile or obstinate. While he became a great actor, he was difficult to work with, causing frustration for directors and programmers alike. The article discusses the issue of control when it comes to animated characters and machines that get smarter. The author argues that as more lifelike behaviors are implanted into these creations, they become less under the control of their creators. However, the author also notes that these creations will remain indirectly under our influence and guidance but free of our domination. The author suggests that the word "co-control" may be more appropriate to describe this type of relationship between creator and creation. The article also mentions the use of autopilots in flying planes as an example of co-control. The article discusses the concept of control in relation to automation and artificial intelligence. It highlights the limitations of human pilots in passive monitoring and the shift towards having computers monitor pilots instead. This has led to a constant tussle between the two, with pilots feeling like they are fighting for control. The article also explores the idea of co-control and partnership between humans and machines, using examples from the animation industry. The author argues that the future of control lies in sharing control and destiny with creations, rather than surrendering control to machines.